<mediawiki xmlns="http://www.mediawiki.org/xml/export-0.10/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.mediawiki.org/xml/export-0.10/ http://www.mediawiki.org/xml/export-0.10.xsd" version="0.10" xml:lang="en">
  <siteinfo>
    <sitename>Wikipedia</sitename>
    <dbname>enwiki</dbname>
    <base>https://en.wikipedia.org/wiki/Main_Page</base>
    <generator>MediaWiki 1.27.0-wmf.7</generator>
    <case>first-letter</case>
    <namespaces>
      <namespace key="-2" case="first-letter">Media</namespace>
      <namespace key="-1" case="first-letter">Special</namespace>
      <namespace key="0" case="first-letter" />
      <namespace key="1" case="first-letter">Talk</namespace>
      <namespace key="2" case="first-letter">User</namespace>
      <namespace key="3" case="first-letter">User talk</namespace>
      <namespace key="4" case="first-letter">Wikipedia</namespace>
      <namespace key="5" case="first-letter">Wikipedia talk</namespace>
      <namespace key="6" case="first-letter">File</namespace>
      <namespace key="7" case="first-letter">File talk</namespace>
      <namespace key="8" case="first-letter">MediaWiki</namespace>
      <namespace key="9" case="first-letter">MediaWiki talk</namespace>
      <namespace key="10" case="first-letter">Template</namespace>
      <namespace key="11" case="first-letter">Template talk</namespace>
      <namespace key="12" case="first-letter">Help</namespace>
      <namespace key="13" case="first-letter">Help talk</namespace>
      <namespace key="14" case="first-letter">Category</namespace>
      <namespace key="15" case="first-letter">Category talk</namespace>
      <namespace key="100" case="first-letter">Portal</namespace>
      <namespace key="101" case="first-letter">Portal talk</namespace>
      <namespace key="108" case="first-letter">Book</namespace>
      <namespace key="109" case="first-letter">Book talk</namespace>
      <namespace key="118" case="first-letter">Draft</namespace>
      <namespace key="119" case="first-letter">Draft talk</namespace>
      <namespace key="446" case="first-letter">Education Program</namespace>
      <namespace key="447" case="first-letter">Education Program talk</namespace>
      <namespace key="710" case="first-letter">TimedText</namespace>
      <namespace key="711" case="first-letter">TimedText talk</namespace>
      <namespace key="828" case="first-letter">Module</namespace>
      <namespace key="829" case="first-letter">Module talk</namespace>
      <namespace key="2300" case="first-letter">Gadget</namespace>
      <namespace key="2301" case="first-letter">Gadget talk</namespace>
      <namespace key="2302" case="case-sensitive">Gadget definition</namespace>
      <namespace key="2303" case="case-sensitive">Gadget definition talk</namespace>
      <namespace key="2600" case="first-letter">Topic</namespace>
    </namespaces>
  </siteinfo>
  <page>
    <title>Mean Directional Accuracy (MDA)</title>
    <ns>0</ns>
    <id>48591546</id>
    <revision>
      <id>693234127</id>
      <parentid>691497558</parentid>
      <timestamp>2015-12-01T06:52:40Z</timestamp>
      <contributor>
        <ip>2601:646:8F00:655D:6D58:7E37:CD65:4C82</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve">'''Mean Directional Accuracy''' ('''MDA'''), also known as '''Mean Direction Accuracy''', is a measure of prediction accuracy of a forecasting method in [[statistics]]. It compares the forecast direction (upward or downward) to the actual realized direction. It is defined by the following formula

:&lt;math&gt; \frac{1}{N}\sum_t \mathbf{1}_{sign(A_t - A_{t-1}) == sign(F_t - F_{t-1})} &lt;/math&gt;
where ''A''&lt;sub&gt;''t''&lt;/sub&gt; is the actual value at time ''t'' and ''F''&lt;sub&gt;''t''&lt;/sub&gt; is the forecast value at time ''t''. Variable ''N'' represents number of forecasting points. The function &lt;math&gt;sign(\cdot)&lt;/math&gt; is [[sign function]] and &lt;math&gt;\mathbf{1}&lt;/math&gt; is the [[indicator function]].

In simple words, MDA provides the probability that the under study forecasting method can detect the correct direction of the time series. MDA is a popular metric for forecasting performance in [[economics]] and [[finance]].&lt;ref&gt;Pesaran, M. H., &amp; Timmermann, A. (2004). How costly is it to ignore breaks when forecasting the direction of a time series?. International Journal of Forecasting, 20(3), 411-425 [https://www.repository.cam.ac.uk/bitstream/handle/1810/338/wp0306.pdf?sequence=1]&lt;/ref&gt;&lt;ref&gt;Schnader, M. H., &amp; Stekler, H. O. (1990). Evaluating predictions of change. Journal of Business, 99-107&lt;/ref&gt;

MDA is used in [[economics]] applications where the economists is often interested only in directional movement of variable of interest. As an example in [[macroeconomics]], a monetary authority who likes to know the direction of the inflation, to raises interest rates or decrease the rates if inflation is predicted to rise or drop respectively.&lt;ref&gt;Greer, M. (2003). Directional accuracy tests of long-term interest rate forecasts. International Journal of Forecasting, 19(2), 291-298.&lt;/ref&gt; Another example can be found in financial planning where the user wants to know if the demand has increasing direction or decreasing trend.&lt;ref&gt;Blaskowitz, O., &amp; Herwartz, H. (2011). On economic evaluation of directional forecasts. International Journal of Forecasting, 27(4), 1058-1065. [http://edoc.hu-berlin.de/series/sfb-649-papers/2009-52/PDF/52.pdf]&lt;/ref&gt;

==Comparison to Other Forecasting Metrics==
Many techniques such as [[Mean absolute percentage error]] or [[Median absolute deviation]] evaluate  forecasting and provided information about
the accuracy and value of the forecasts. While accuracy, as measured by quantitative errors, is
important, it may be more crucial to accurately forecast the direction of change.  Directional accuracy is similar to a binary evaluation. The metric only consider the upward or downward direction in the time series and is independent of quantitive value of increase or decrease. For example, Will prices rise or fall? How much it will increase or decrease can be detected by other forecasting metrics.&lt;ref&gt;Sinclair, T. M., Stekler, H. O., &amp; Kitzinger, L. (2010). Directional forecasts of GDP and inflation: a joint evaluation with an application to Federal Reserve predictions. Applied Economics, 42(18), 2289-2297.&lt;/ref&gt;

==References==
{{reflist}}

{{uncategorized|date=November 2015}}</text>
      <sha1>0vm88c3tiidckupcmjbtoptk6hkj3qs</sha1>
    </revision>
  </page>
  <page>
    <title>Draft:DETIboot</title>
    <ns>118</ns>
    <id>48591877</id>
    <revision>
      <id>691404568</id>
      <parentid>691404317</parentid>
      <timestamp>2015-11-19T16:39:18Z</timestamp>
      <contributor>
        <username>Robert McClenon</username>
        <id>318807</id>
      </contributor>
      <comment>Declining submission: nn - Submission does not meet general notability guidelines (be more specific if possible) ([[WP:AFCH|AFCH]] 0.9)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve">{{AFC submission|d|nn|u=Avzuquete|ns=118|decliner=Robert McClenon|declinets=20151119163918|ts=20151119095324}} &lt;!-- Do not remove this line! --&gt;

{{AFC comment|1=It appears that all of the references are to papers by the developers.  Please provide multiple ''independent'' [[WP:RS|reliable sources]] as to notability of the product. [[User:Robert McClenon|Robert McClenon]] ([[User talk:Robert McClenon|talk]]) 16:39, 19 November 2015 (UTC)}}

----

'''DETIboot''' is an over-the-air, secure [[Wi-Fi]]-based solution for [[booting|bootstrapping]] an homogeneous [[Linux]] image in countless target computers. DETIboot was developed by researchers of [https://www.ua.pt/deti DETI](Department of Electronics, Telecommunications and Informatics) and [http://wiki.ieeta.pt IEETA] (Institute of Electronics and Informatics Engineering of Aveiro) at [[University of Aveiro]].

DETIboot is a [[scalability|scalable]] solution developed to distribute and [[booting|boot]] a single, temporary [[Linux]] [[operating system]] image in an arbitrarily large number of target computers&lt;ref&gt;{{cite thesis
|degree = M.Sc.
|author = João Cardoso
|title = DETIboot: distribuição e arranque de sistemas Linux com redes WiFi
|publisher = University of Aveiro, Portugal
|year = 2013
}}&lt;/ref&gt;&lt;ref&gt;{{cite conference
|author = Carlos Faneca, José M. N. Vieira, André Zúquete, João Cardoso
|title = DETIboot: A fast, wireless system to install operating systems on students laptops
|booktitle = 2nd International Conference on Advances in Computing, Electronics and Communication (ACEC 2014)
|date = October 2014 
}}&lt;/ref&gt;. The [[booting]] time on each computer depends only on the specific work conditions of that computer and on its [[hardware]] performance. In other words, there is no interference of any kind between the computers [[booting]] the same [[operating system]], which enables the booting procedure to scale naturally to an arbitrarily large set of computers.

For dealing with an arbitrarily set of target computers, it was decided to use wireless [[Broadcasting (networking)|broadcast]] in DETIboot to distribute a single [[operating system]] image to all nearby, target computers. The [[Broadcasting (networking)|broadcast]] is performed using a novel protocol directly implemented on top of [[Wi-Fi]] ([[link-layer]]) frames. Target computers use a new, [[booting|second-stage boot loader]], which can be deployed in a [[USB flash drive]] or installed in the computers' internal [[disk storage|disk]], to handle this new protocol and the downloading and [[booting]] an [[operating system]] image. The wireless communications are performed in [[wireless ad hoc network|ad hoc]] mode and no [[internet protocol|IP]] configuration is required.

The protocol used to distribute the operating system to target computers is a '''one-to-many [[file transfer]] protocol'''. A source endlessly transmits file blocks to all nearby receivers, and the source does not need to know how many receivers are in place (there is no enrollment) and how they are handling the reception (there is no [[feedback]]). To tackle receivers' frame losses (and, consequently, file block losses), we used [[Fountain code|Fountain codes]], namely [[Luby transform code|LT codes]] with a random coding. These rateless [[code|codes]] enable similar target computers with similar reception quality (i.e., equal packet [[loss ratio]]) to [[booting|boot]] the [[operating system]] in a similar time span, regardless of the instant they initiated their [[download]] of the [[operating system]] image.

For protecting target computers from accepting file blocks from unintended sources, the [[file transfer]] protocol was enhanced to incorporate [[message authentication|source authentication]]&lt;ref&gt;{{cite conference
|author = Simão Reis, André Zúquete, Carlos Faneca, José M. N. Vieira
|title = Authenticated File Broadcast Protocol
|booktitle = IFIC SEC 2015 conference
|location = Hamburg, Germany
|date = May 2015 
|url = http://link.springer.com/chapter/10.1007%2F978-3-319-18467-8_16
}}&lt;/ref&gt;. The [[authentication]] is performed with extra messages ('''authenticators''') interleaved at unpredictable places within the original block transmission flow. These authenticators enable a set of subsequent blocks to be authenticated by the receivers.

== Broadcast source system == 

The DETIboot source of the [[Linux]] image [[Broadcasting (networking)|broadcast]] to all target computers can be any ordinary computer equipped with a [[Wi-Fi]] interface where the [[file transfer]] protocol server could run and set up an [[wireless ad hoc network|ad hoc network]] for [[Broadcasting (networking)|broadcasting]] to all interested receivers. But small, low-cost, low-power systems can also be used to implement DETIboot [[Broadcasting (networking)|broadcasting]] sources&lt;ref&gt;{{cite conference
|author = Carlos Faneca, José M. N. Vieira, André Zúquete
|title = Fast image file distribution with Fountain Codes via a Wi-Fi Ad-Hoc network, using low power processors
|booktitle = 16th International Telecommunications Network Strategy and Planning Symposium (NETWORKS 2014)
|location = Funchal, Madeira, Portugal
|date = September 2014
|url = http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&amp;arnumber=6959232&amp;openedRefinements%3D*%26pageNumber%3D7%26rowsPerPage%3D100%26queryText%3D%28wi-fi%29
}}&lt;/ref&gt;. One of the systems we have successfully tested was an [[Odroid]] using an external USB [[Wi-Fi]] interface. 

== Second-stage boot loader ==

The DETIboot [[booting|second-stage boot loader]] is a minimum system capable of running the required [[file transfer]] protocol client, for downloading the final [[operating system]], and some mechanism to [[booting|boot]] it. For that, it needs to manage the computers' [[Wi-Fi]] interface (or interfaces) and to manage a minimum [[user interface]] (display and keyboard) for dealing with security-related decisions.

Several solutions can be considered to implement such [[booting|boot loader]]. The one we conceived and implemented is a minimum [[Linux]] system, installed in a [[booting|bootable]] [[USB flash drive]]. After its [[booting|bootstrap]], runs the [[file transfer]] protocol client to load another [[Linux]] image from a Wi-Fi source, stores it in [[random access memory|RAM memory]] and performs its live [[booting]] with the '''[[kexec]]''' [[system call]].

== Comparison with similar solutions ==

The DETIboot [[booting|second-stage boot loader]] can be primarily compared with [[Preboot Execution Environment|PXE]], since both allow computers to load a boot image from a remote network location. However, there are several fundamental differences between them:
* DETIboot uses [[link-layer]], [[Wi-Fi]] frames, whilst [[Preboot Execution Environment|PXE]] uses [[network-layer]], [[internet protocol|IP]] [[datagram|datagrams]];
* DETIboot uses [[wireless ad hoc network|ad hoc wireless networks]], whilst [[Preboot Execution Environment|PXE]] uses cabled networks;
* DETIboot uses (radio) [[Broadcasting (networking)|broadcast]], being able to serve an arbitrarily large set of target computers simultaneously, and the network traffic is constant regardless of the number of target computers ([[Big O notation|&lt;math&gt;O(1)&lt;/math&gt;]]). [[Preboot Execution Environment|PXE]] uses [[unicast]], thus network traffic grows linearly with the number of target computers ([[Big O notation|&lt;math&gt;O(n)&lt;/math&gt;]]);
* DETIboot has [[message authentication|source authentication]], whilst [[Preboot Execution Environment|PXE]] does not have any security mechanisms at all.

== Exploitation scenarios ==

DETIboot is very attractive to perform secure, live setups of [[operating system|operating systems]] to be used temporarily in many target computers, such as personal [[laptop|laptops]]. For instance, it can be used for giving classes or hands-on tutorials with specially prepared [[operating system]] images, using the attendants' [[laptop|laptops]]. The fact that the system's architecture is based on a standalone source station broadcasting through [[Wi-Fi]] makes it very convenient to deploy the system anywhere, since it does not require any kind of  infrastructural support.

== Variants ==

There are variants of DETIboot with some extra features. Namely, we developed a variant including an enrollment step and feedback, which can be used in scenarios requiring different levels of [[quality of service]] for different target computers&lt;ref&gt;{{cite conference
|author = Carlos Faneca, José M. N. Vieira, André Zúquete, Julio Cano, André Moreira, Luís Almeida
|title = Towards Dynamic Adaptation in Broadcasting with Hybrid Rateless Codes
|booktitle = 7th Workshop on Adaptive and Reconfigurable Embedded Systems
|location = Seattle, USA
|date = April 2015
|url = http://dl.acm.org/citation.cfm?id=2815491
}}&lt;/ref&gt;&lt;ref&gt;{{cite conference
|author = Carlos Faneca, André Zúquete, José M. N. Vieira, André Moreira, Luís Almeida, Julio Cano
|title = Orchestrating Feedback for Hybrid Rateless Codes
|booktitle = European Wireless 2015
|location = Budapest, Hungary
|date = May 2015 
|url = http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7147690&amp;punumber%3D7147658%26filter%3DAND%28p_IS_Number%3A7147659%29%26pageNumber%3D2
}}&lt;/ref&gt;. In that variant, target computers can state their needs (missing file blocks) on feedback messages, and the source can favor them by forcing the addition of those missing blocks to the codes it broadcasts.

== References ==
&lt;!--- See http://en.wikipedia.org/wiki/Wikipedia:Footnotes on how to create references using&lt;ref&gt;&lt;/ref&gt; tags, these references will then appear here automatically --&gt;
{{Reflist}}

== External links ==
* [https://technet.microsoft.com/pt-pt/library/cc161973.aspx How to Deploy an Operating System Image via Multicast]
* [https://technet.microsoft.com/en-us/library/hh397406.aspx Planning a Multicast Strategy in Configuration Manager]
* {{cite conference
|author = John W. Byers, Michael Luby Michael Mitzenmacher, Ashutosh Rege
|title = A digital fountain approach to reliable distribution of bulk data
|booktitle = Proc. of the ACM SIGCOMM '98 Conf. on Applications, technologies, architectures, and protocols for computer communication
|date = August 1998
|location = Vancouver, British Columbia, Canada
|url = http://www.cc.gatech.edu/classes/AY2007/cs7260_spring/papers/df.pdf
}}
* {{cite journal
|author = J. W. Byers, M. Luby, M. Mitzenmacher
|journal = IEEE Journal on Selected Areas in Communications
|title = A digital fountain approach to asynchronous reliable multicast
|year = 2002
|volume = 20
|number = 8
|url = http://ieeexplore.ieee.org/Xplore/defdeny.jsp?url=http%3A%2F%2Fieeexplore.ieee.org%2Fstamp%2Fstamp.jsp%3Ftp%3D%26arnumber%3D1038582%26userType%3Dinst&amp;denyReason=-134&amp;arnumber=1038582&amp;productsMatched=null&amp;userType=inst
}}
* {{cite journal
|title = Kadeploy3: Efficient and Scalable Operating System Provisioning
|author = Emmanuel Jeanvoine, Luc Sarzyniec, Lucas Nussbaum 
|journal = USENIX ;login:
|volume = 38
|number = 1
|date = February 2013
|url = https://www.usenix.org/publications/login/february-2013-volume-38-number-1/kadeploy3-efficient-and-scalable-operating
}}
* {{cite web
|author = Philippe Augerat, Wilfrid Billot, Simon Derr, Cyrille Martin
|title = A scalable file distribution and operating system installation toolkit for clusters
|url = http://ka-tools.sourceforge.net/publications/file-distribution.pdf
|year = 2001
}}
* {{cite conference
|author = Kuen-Min Lee, Wei-Guang Teng, Jin-Neng Wu, Kuo-Ming Huang, Yao-Hsing Ko, Ting-Wei Hou
|title = Multicast Deployment of Cloud Operating Systems
|booktitle = IEEE 12th Int. Conf. on Computer and Information Technology (CIT)
|location = Chengdu, Sichuan, China
|date = October 2012
|url = http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6391893
}}
* {{cite conference
|author = H. Chen, R. Maunder, L. Hanzo
|title = Fountain-Code Aided File Transfer in 802.11 WLANs
|booktitle = IEEE 70th Vehicular Technology Conf. Fall (VTC 2009-Fall)
|location = Anchorage, Alaska, USA
|date = September 2009
|url = http://core.ac.uk/download/pdf/1509983.pdf
}}
* {{cite conference
|author = Pasquale Cataldi, Andrea Tomatis, Gianluca Grilli, Mario Gerla
|title = A Novel Data Dissemination Method for Vehicular Networks with Rateless Codes
|booktitle = Proc. of the IEEE Wireless Communications \&amp; Networking Conf. (WCNC'09)
|location = Budapest, Hungary
|date = April 2009
|url = http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4917521&amp;tag=1
}}

&lt;!--- Categories ---&gt;

[[:Category:Articles created via the Article Wizard]]</text>
      <sha1>lr67fbfwdk2mbf1dgjoit089juoz21w</sha1>
    </revision>
  </page>
  <page>
    <title>Lagrange stability</title>
    <ns>0</ns>
    <id>48606512</id>
    <revision>
      <id>693136131</id>
      <parentid>692550673</parentid>
      <timestamp>2015-11-30T17:32:47Z</timestamp>
      <contributor>
        <username>LilHelpa</username>
        <id>8024439</id>
      </contributor>
      <minor />
      <comment>typo</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve">{{unreferenced|date=November 2015}}

'''Lagrange stability''' is a concept in the [[stability theory]] of [[dynamical systems]], named after [[Joseph-Louis Lagrange]].

Consider a real continuous [[Dynamical system (definition)|dynamical system]] &lt;math&gt;(T,M,\Phi)&lt;/math&gt;, where &lt;math&gt;T&lt;/math&gt; is &lt;math&gt;\mathbb{R}&lt;/math&gt;. For any point in the state space, &lt;math&gt;x \in M &lt;/math&gt;, the '''motion''' &lt;math&gt;\Phi(t,x)&lt;/math&gt; is said to be ''positively Lagrange stable'' if the [[Orbit (dynamics)|positive semi-orbit]] &lt;math&gt;\gamma_x^+&lt;/math&gt; is [[compact set|comapct]]. If the [[Orbit (dynamics)|negative semi-orbit]] &lt;math&gt;\gamma_x^-&lt;/math&gt; is  [[compact set|comapct]], then the motion is said to be ''negatively Lagrange stable''. The motion through &lt;math&gt;x&lt;/math&gt; is said to be ''Lagrange stable'' if it is both positively and negatively Lagrange stable. If the state space &lt;math&gt;M&lt;/math&gt; is the [[Euclidean space]] &lt;math&gt;\mathbb{R}^n&lt;/math&gt;, then the above definitions are equivalent to &lt;math&gt;\gamma_x^+, \gamma_x^-&lt;/math&gt; and &lt;math&gt;\gamma_x&lt;/math&gt; being [[bounded set|bounded]], respectively.

A '''dynamical system''' is said to be positively-/negatively-/Lagrange stable if ''for each''  &lt;math&gt;x \in M &lt;/math&gt;, the motion &lt;math&gt;\Phi(t,x)&lt;/math&gt; is positively-/negativey-/Lagrange stable, respectively.

==Further reading==
* Elias P. Gyftopoulos, ''Lagrange Stability and Liapunov's Direct Method''. Proc. of Symposium on Reactor Kinetics and Control, 1963. ([http://elias-gyftopoulos-memorial-collection.unibs.it/EPGyftopoulos-papers/p16-Gyftopoulos-ProcSympReactKinContr-1963.pdf PDF])
* {{cite book |last1=Bhatia |first1=Nam Parshad |last2=Szegő |first2=Giorgio P.  |title=Stability theory of dynamical systems |publisher=Springer |year=2002|isbn=978-3-540-42748-3 }}

[[Category:Lagrangian mechanics]]
[[Category:Stability theory]]
[[Category:Dynamical systems]]

{{mathematics-stub}}</text>
      <sha1>tvwpd8602yl9vbjeiney7c0hcmcnzwj</sha1>
    </revision>
  </page>
  <page>
    <title>Circumstellar disc</title>
    <ns>0</ns>
    <id>48609118</id>
    <revision>
      <id>693221416</id>
      <parentid>691604738</parentid>
      <timestamp>2015-12-01T04:25:50Z</timestamp>
      <contributor>
        <username>Me, Myself, and I are Here</username>
        <id>17619453</id>
      </contributor>
      <minor />
      <comment>/* Disc Evolution */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve">[[File:A Stars Spiral.ogv|thumb|350px|thumbtime=50|The star SAO 206462 has an unusual circumstellar disc]]

A '''circumstellar disc''' (or '''circumstellar disk''') is a [[torus]], pancake or ring-shaped accumulation of [[matter]] composed of [[gas]], [[dust]], [[planetesimal]]s, [[asteroid]]s or collision fragments in [[orbit]] around a [[star]]. Around the youngest stars, they are the reservoirs of material out of which planets may form. Around mature stars, they indicate that [[planetesimal]] formation has taken place and around [[white dwarf]]s, they indicate that planetary material survived the whole of stellar evolution. Such a disc can manifest itself in various ways.

==Young star==
[[File:Circumstellar Disks HD 141943 and HD 191089.jpg|thumb|Circumstellar discs HD 141943 and HD 191089.&lt;ref&gt;{{cite news|title=Circumstellar Disks HD 141943 and HD 191089|url=http://www.spacetelescope.org/images/opo1416a/|accessdate=29 April 2014|newspaper=ESA/Hubble images}}&lt;/ref&gt; ]]

{{main|Protoplanetary disk}}
According to the currently accepted model of [[star]] formation, sometimes referred to as the [[nebular hypothesis]], a star is formed by the gravitational collapse of a pocket of matter within a [[giant molecular cloud]]. The infalling material possesses some amount of [[angular momentum]], which results in the formation of a gaseous [[protoplanetary disc]] around the young, rotating star. The former is a rotating circumstellar disc of dense gas and dust that continues to feed the central star. It may contain a few percent of the mass of the central star, mainly in the form of gas which is itself mainly [[hydrogen]]. The [[accretion disc]] phase lasts a few to 10 million years. Accretion rates are typically 10&lt;sup&gt;−7&lt;/sup&gt; to 10&lt;sup&gt;−9&lt;/sup&gt; solar masses per year but can vary.

The disc gradually cools in what is known as the [[T Tauri star]] stage. Within this disc, the formation of small dust grains made of rocks and ices can occur, and these can coagulate into [[planetesimal]]s. If the disc is sufficiently massive, the runaway accretions begin, resulting in the appearance of planetary embryos. The formation of planetary systems is thought to be a natural result of star formation. A sun-like star usually takes around 100 million years to form.

==Circumstellar discs around the Solar System==
* [[Asteroid belt]] is a reservoir of small bodies in our Solar System located between the orbit of Mars and Jupiter. It is a source of interplanetary dust.
* Edgeworth-[[Kuiper belt]]
* [[Scattered disc]]
* Öpik–[[Oort cloud]] / Hills cloud, only the inner Oort cloud has a toroid-like shape. The outer Oort cloud is more spherical in shape.

==Binary system==
* Circumprimary disc is where a disc orbits the primary (i.e. more massive) star of the binary star system&lt;ref&gt;[http://adsabs.harvard.edu/abs/2001ApJ...561L.199B Discovery of a New Companion and Evidence of a Circumprimary Disk: Adaptive Optics Imaging of the Young Multiple System VW Chamaeleon], Brandeker, Alexis et al. 2001&lt;/ref&gt;
* Circumsecondary disc is one around the secondary (i.e. less massive) star of the binary star system
* [[Circumbinary planet|Circumbinary]] disc is where a disc orbits both the primary and the secondary of the binary system

==Dust==
* [[Debris discs]] consist of planetesimals along with fine dust and small amounts of gas generated through their collisions and evaporation. The original gas and small dust particles have been dispersed or accumulated into planets.&lt;ref&gt;{{cite book
 | first=Hubert | last=Klahr
 |author2=Brandner, Wolfgang | year=2006
 | title=Planet Formation | pages=25
 | publisher=Cambridge University Press
 | isbn=0-521-86015-6 }}&lt;/ref&gt;
* [[Zodiacal cloud]] or [[interplanetary dust]] is the material in the Solar System created by collisions of asteroids and evaporation of comet seen to observers on Earth as a band of scattered light along the ecliptic before sunrise or after sunset.
* [[Exozodiacal dust]] is dust around another star than the Sun in a location analogous to that of the Zodiacal Light in our own Solar System.

==Disc evolution==
Circumstellar discs are not equilibrium objects, but instead are constantly evolving. The evolution of the surface density &lt;math&gt;\Sigma&lt;/math&gt; of the disc, which is the amount of mass per unit area so after the volume density at a particular location in the disc has been integrated over the vertical structure, is given by:
&lt;math&gt;
\frac{\partial \Sigma}{\partial t} = \frac{3}{r} \frac{\partial}{\partial r} \left[ r^{1/2} \frac{\partial}{\partial r} \nu \Sigma r^{1/2} \right] 
&lt;/math&gt;
where &lt;math&gt;r&lt;/math&gt; is the radial location in the disc and &lt;math&gt;\nu&lt;/math&gt; is the viscosity at location &lt;math&gt;r&lt;/math&gt;.&lt;ref name=&quot;Armitage2011&quot;&gt;{{cite journal|title=Dynamics of Protoplanetary Disks | first=Philip | last=Armitage| year=2011| journal=Annual Review of Astronomy and Astrophysics| doi=10.1146/annurev-astro-081710-102521}}&lt;/ref&gt; This equation assumes axisymmetric symmetry in the disc, but is compatible with any vertical disc structure.

Viscosity in the disc, whether molecular, turbulent or other, transports angular momentum outwards in the disc and most of the mass inwards, eventually accreting onto the central object.&lt;ref name=&quot;Armitage2011&quot;/&gt; The mass accretion onto the star &lt;math&gt;\dot{M}&lt;/math&gt; in terms of the disc viscosity &lt;math&gt;\nu&lt;/math&gt; is expressed:
&lt;math&gt;
\dot{M} = 3 \pi \nu \Sigma \left[ 1 - \sqrt{\frac{r_\text{in}}{r}} \right]^{-1}
&lt;/math&gt;
where &lt;math&gt;r_\text{in}&lt;/math&gt; is the inner radius.

== See also ==
* [[Accretion disc]]
* [[Formation and evolution of the Solar System]]
* [[Extrasolar planet]]

==References==
{{Reflist}}

== External links ==
* {{cite web
 | last = McCabe | first = Caer | date =May 30, 2007
 | url =http://www.circumstellardisks.org/
 | title =Catalog of Resolved Circumstellar Disks
 | publisher = NASA JPL | accessdate =2007-07-17 }}
*[http://astro.berkeley.edu/~kalas/disksite/pages/gallery.html Image Gallery of Dust disks] (from [[Paul Kalas]], &quot;[http://astro.berkeley.edu/~kalas/disksite/index.html Circumstellar Disk Learning Site])&quot;

[[Category:Circumstellar disks| ]]
[[Category:Nebulae]]
[[Category:Articles containing video clips]]

[[ca:Disc protoplanetari]]
[[de:Protoplanetare Scheibe]]
[[fr:Disque protoplanétaire]]
[[ko:원시 행성계 원반]]
[[it:Disco protoplanetario]]
[[lt:Proplanetinis diskas]]
[[hu:Protoplanetáris korong]]
[[nl:Protoplanetaire schijf]]
[[ja:原始惑星系円盤]]
[[pl:Dysk protoplanetarny]]
[[sk:Protoplanetárny disk]]
[[fi:Protoplanetaarinen kiekko]]</text>
      <sha1>r1wneijhin752l09px1uwqj633buuzw</sha1>
    </revision>
  </page>
  <page>
    <title>BF-algebra</title>
    <ns>0</ns>
    <id>48615302</id>
    <revision>
      <id>692308295</id>
      <parentid>692093433</parentid>
      <timestamp>2015-11-24T20:42:01Z</timestamp>
      <contributor>
        <username>Michael Hardy</username>
        <id>4626</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve">{{Multiple issues|
{{refimprove|date=November 2015}}
{{no footnotes|date=November 2015}}
}}

A '''BF-algebra''' is a non-empty [[subset]] &lt;math&gt;X&lt;/math&gt; with a [[constant (mathematics)|constant]] &lt;math&gt;0&lt;/math&gt; and a [[binary operation]] &lt;math&gt;*&lt;/math&gt; satisfying the following:
# &lt;math&gt;x*x=0&lt;/math&gt;
# &lt;math&gt;x*0=x&lt;/math&gt;
# &lt;math&gt;0*(x*y)=y*x&lt;/math&gt;

== Example ==
Let &lt;math&gt;Z&lt;/math&gt; be the [[Set_(mathematics)|set]] of [[integer]]s and '&lt;math&gt;-&lt;/math&gt;' be the binary operation '[[subtraction]]'. Then the [[algebra]]ic structure &lt;math&gt;(Z,-)&lt;/math&gt; obeys the following [[Property (philosophy)|properties]]:
# &lt;math&gt;x-x=0&lt;/math&gt; 
# &lt;math&gt;x-0=x&lt;/math&gt;
# &lt;math&gt;0-(x-y)=y-x&lt;/math&gt;

==References==

*{{citation|mr=2357811 
|last=Walendziak|first= Andrzej
|title=On BF-algebras
|journal=Math. Slovaca |volume=57 |year=2007|issue= 2|pages= 119–128|doi=10.2478/s12175-007-0003-x}} 
[[Category:Algebraic structures]]</text>
      <sha1>4cm84lu0v3vowcxp7pmcm0c9tyjd6h5</sha1>
    </revision>
  </page>
  <page>
    <title>Range rate</title>
    <ns>0</ns>
    <id>48619258</id>
    <revision>
      <id>692786774</id>
      <parentid>692607860</parentid>
      <timestamp>2015-11-28T08:50:58Z</timestamp>
      <contributor>
        <username>Yobot</username>
        <id>7328338</id>
      </contributor>
      <minor />
      <comment>Removed invisible unicode characters + other fixes, replaced: →, added [[CAT:O|orphan]], [[CAT:UNCAT|uncategorised]] tags using [[Project:AWB|AWB]] (11749)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve">{{Multiple issues|{{unreferenced|date=November 2015}}{{expert-subject|date=November 2015|reason=Needs verification of notability and mathematical claims, as well as LaTeX/math typesetting if worth keeping.}}{{original research|date=November 2015}}
{{Orphan|date=November 2015}}
}}

Given a differentiable vector &lt;math&gt;\mathbf{r}&lt;/math&gt; in &lt;math&gt;\mathbb{R}^3&lt;/math&gt; defining the position of a target relative to an observer.

Let

{{NumBlk|:|&lt;math&gt; \mathbf{v} = \frac{d\mathbf{r}}{dt}&lt;/math&gt;|{{EquationRef|1}}}}

The magnitude of the position vector &lt;math&gt;\mathbf{r}&lt;/math&gt; is defined as

{{NumBlk|:|&lt;math&gt;||\mathbf{r}|| = \langle \mathbf{r},\mathbf{r} \rangle^{1/2}&lt;/math&gt;|{{EquationRef|2}}}}

The quantity range rate is the time [[derivative]] of the magnitude ([[Norm (mathematics)|norm]]) of &lt;math&gt;\mathbf{r}&lt;/math&gt;, expressed as

{{NumBlk|:|&lt;math&gt;\frac{d||\mathbf{r}||}{dt}&lt;/math&gt;|{{EquationRef|3}}}}

Substituting ({{EquationNote|2}}) into ({{EquationNote|3}})

: &lt;math&gt;\frac{d||\mathbf{r}||}{dt} = \frac{d \langle \mathbf{r},\mathbf{r} \rangle^{1/2} }{dt}&lt;/math&gt;

Evaluating the derivative of the right-hand-side

: &lt;math&gt;\frac{d||\mathbf{r}||}{dt} = \frac{1}{2}  \frac{d \langle \mathbf{r},\mathbf{r} \rangle}{dt}  \frac{1}{\langle \mathbf{r},\mathbf{r} \rangle^{1/2}}&lt;/math&gt;

: &lt;math&gt;\frac{d||\mathbf{r}||}{dt} = \frac{1}{2}  \frac{\langle \frac{d\mathbf{r}}{dt},\mathbf{r} \rangle + \langle \mathbf{r},\frac{d\mathbf{r}}{dt} \rangle}{\langle \mathbf{r},\mathbf{r} \rangle^{1/2}}&lt;/math&gt;

using ({{EquationNote|1}}) the expression becomes

: &lt;math&gt;\frac{d||\mathbf{r}||}{dt} = \frac{1}{2}  \frac{\langle \mathbf{v},\mathbf{r} \rangle+\langle \mathbf{r},\mathbf{v} \rangle}{\langle \mathbf{r},\mathbf{r} \rangle^{1/2}}&lt;/math&gt;

Since

: &lt;math&gt;\langle \mathbf{v},\mathbf{r} \rangle = \langle \mathbf{r},\mathbf{v} \rangle&lt;/math&gt;

then the range rate is simply defined as

: &lt;math&gt;\frac{d||\mathbf{r}||}{dt} = \frac{\langle \mathbf{r},\mathbf{v} \rangle}{\langle \mathbf{r},\mathbf{r} \rangle^{1/2}} = \langle \hat{\mathbf{r}},\mathbf{v} \rangle&lt;/math&gt;

the projection of the observer to target velocity vector onto the &lt;math&gt;\mathbf{r}&lt;/math&gt; unit vector.

==See also==
* [[inner product]]
* [[orbit determination]]
* [[Lp space]]

{{Uncategorized|date=November 2015}}</text>
      <sha1>736lo9ajj222qu5v65jgvnru5awnqp5</sha1>
    </revision>
  </page>
  <page>
    <title>Wikipedia:Articles for deletion/David Rosenberg</title>
    <ns>4</ns>
    <id>48629196</id>
    <revision>
      <id>693432396</id>
      <parentid>693432200</parentid>
      <timestamp>2015-12-02T15:03:04Z</timestamp>
      <contributor>
        <username>Necrothesp</username>
        <id>64853</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve">===[[David Rosenberg]]===
{{REMOVE THIS TEMPLATE WHEN CLOSING THIS AfD|B}}

:{{la|David Rosenberg}} – (&lt;includeonly&gt;[[Wikipedia:Articles for deletion/David Rosenberg|View AfD]]&lt;/includeonly&gt;&lt;noinclude&gt;[[Wikipedia:Articles for deletion/Log/2015 December 1#{{anchorencode:David Rosenberg}}|View log]]&lt;/noinclude&gt;{{int:dot-separator}} &lt;span class=&quot;plainlinks&quot;&gt;[https://tools.wmflabs.org/jackbot/snottywong/cgi-bin/votecounter.cgi?page=Wikipedia:Articles_for_deletion/David_Rosenberg Stats]&lt;/span&gt;)
:({{Find sources AFD|David Rosenberg}})
Despite wading through the long list of references, I can't find anything that conveys notability. Passing mentions, tangential references but nothing  that says this person is notable. Most merely name check him as &quot; commissaire&quot;. This article has been PRODed twice for similar concerns . The sheer bulk of references and the super-abundance of wiki-links and name-checking doesn't make up for the lack of real substance. Fails [[WP:GNG]] &lt;span style=&quot;background-color:lightblue&quot;&gt;'''''&amp;nbsp;[[User:Velella|Velella]]&amp;nbsp;'''''&lt;/span&gt;&lt;span style=&quot;background-color:lightblue&quot;&gt;&amp;nbsp;&lt;sup&gt;''[[User talk:Velella|Velella]] Talk ''&lt;/sup&gt;&amp;nbsp;&lt;/span&gt; 11:03, 23 November 2015 (UTC)

* '''keep'''  I just reviewed the references, and only kept the relevant ones, pages which mention David Rosenberg's implication in the project. He is an author and art curator so there will be no post on TMZ about him to show his &quot;star status&quot;, but he is mentioned by every institution he worked with. Please note that he was rewarded by the [http://www.culturecommunication.gouv.fr/Ministere/Services-rattaches-a-la-ministre/Section-des-distinctions-honorifiques/Arretes-de-Nominations-dans-l-ordre-des-Arts-et-des-Lettres/Nomination-dans-l-ordre-des-Arts-et-des-Lettres-juillet-2014 French Minister of Culture and Communications] (Ordre des Arts et des Lettres) and is listed by  [http://data.bnf.fr/14446577/david_rosenberg/#allmanifs Bibliothèque Nationale de France] (cf. references). Also, please check the notability of Palais de Tokyo, Maison Rouge etc, you will see that to work for these, you must be &quot;notable&quot;. For your information, in French, art curator is &quot;commissaire d'exposition&quot;, that's why he is mentioned as commissaire in every reference site. Besides, for almost all his publications the ISBN is mentioned, I can't see what kind of reference could be linked to that to prove his notability.  &lt;small&gt;&lt;span class=&quot;autosigned&quot;&gt;—&amp;nbsp;Preceding [[Wikipedia:Signatures|unsigned]] comment added by [[User:Anh.tamy|Anh.tamy]] ([[User talk:Anh.tamy|talk]] • [[Special:Contributions/Anh.tamy|contribs]]) 12:22, 23 November 2015 (UTC)&lt;/span&gt;&lt;/small&gt;&lt;!-- Template:Unsigned --&gt; &lt;!--Autosigned by SineBot--&gt;&lt;small class=&quot;afdnewuser-notice&quot;&gt;{{user|Anh.tamy||}} has only contributed to the article(s) under discussion for deletion and AFD. —&amp;nbsp;[[User:Jkudlick|Jkudlick]]&amp;nbsp;&lt;sup&gt;[[User_talk:Jkudlick|t]]&lt;/sup&gt;&lt;small&gt;[[Special:Contributions/Jkudlick|c]]&lt;/small&gt;&lt;sub&gt;[[User:Jkudlick/sandbox|s]]&lt;/sub&gt; 09:29, 1 December 2015 (UTC)&lt;/small&gt;
'''Delete''' Except passing mentions on exhibitions he curated, a couple of infotainment books he compiled with others and one short interview with [[Le Parisien]], I've found nothing. No substantial coverage, only a collection of all the google hits with his name. Oh, and hundreds of people receive the [[Ordre des Arts et des Lettres]] annually. [[User:AddMore der Zweite|AddMore der Zweite]] ([[User talk:AddMore der Zweite|talk]]) 22:06, 24 November 2015 (UTC)

::Just so you know, rewards Ordre des Arts et des Lettres are delivered by the French Ministry of Culture, it is indeed granted to about 200 contributors of arts and literature each year. The French population in 2014 was about 66 million people, which means that only 0,000303% got the reward. So sorry if I do think that the reward proves the notability of the person. 
::You're also talking about &quot;a couple of infotainment books he compiled&quot;: I can count 30 in the list (not including collective works). I'm not sure if you're an art expert, but I can assure you those books are not even close to &quot;infotainment&quot;, they are art books about notable artists. 
:: And besides in Le Parisien, he was also interviewed by Le Figaro which dedicated the front page and 4 pages inside of its Culture magazine (from an october 2015 issue). I couldn't find it online, if you could help me find it, would be useful. 
::And there is also an [http://bfmbusiness.bfmtv.com/mediaplayer/video/la-tendance-shopping-le-studio-des-acacias-propose-l-exposition-new-american-art-2010-666503.html interview on BFM] (BFM is a kind of a French CNN). The interview is about an exhibition he curated as part of the FIAC program. You can google &quot;FIAC&quot;, it's one of world's most important art fair dedicated to contemporary art. 
::To sum up, in my opinion the notability of one's works prove one's notability. [[User:Anh.tamy|Anh.tamy]] ([[User talk:Anh.tamy|talk]]) 23:27, 24 November 2015 (UTC)

*'''Strong Delete''' - as it fails [[WP:Notability]].--[[User:DaeafcMnnC|DaeafcMnnC]] ([[User talk:DaeafcMnnC|talk]]) 09:30, 28 November 2015 (UTC)
&lt;div class=&quot;xfd_relist&quot; style=&quot;border-top: 1px solid #AAA; border-bottom: 1px solid #AAA; padding: 0px 25px;&quot;&gt;&lt;span style=&quot;color: #FF6600;&quot;&gt;'''{{resize|91%|[[Wikipedia:Deletion process#Relisting discussions|Relisted]] to generate a more thorough discussion and clearer consensus.}}'''&lt;/span&gt;&lt;br /&gt;
&lt;small&gt;Please add new comments below this notice. Thanks, – '''[[User:Juliancolton|&lt;span style=&quot;font-family:Script MT Bold;color:#36648B&quot;&gt;Juliancolton&lt;/span&gt;]]'''&amp;nbsp;&amp;#124;&amp;nbsp;[[User_talk:Juliancolton|&lt;sup&gt;&lt;span style=&quot;font-family:Verdana;color:gray;text-shadow:gray .2em .18em .12em&quot;&gt;''Talk''&lt;/span&gt;&lt;/sup&gt;]] 23:13, 1 December 2015 (UTC)&lt;/small&gt;&lt;!-- from Template:Relist --&gt;[[Category:Relisted AfD debates|David Rosenberg]]&lt;/div&gt;&lt;!-- Please add new comments below this line --&gt;
::'''(provisional) KEEP''': I looked over this article and spent most of my time scratching my head at all the names of people (I didn't count, but it was in the hundreds upon hundreds). The article is confusing. Assuming all the references are legit (I don't speak French and wouldn't know a reliable French source from a hole in the ground), if the author can trim the fat and organize it better, I'd say keep. If the author (or someone else) doesn't fix it by the end of this AfD, then my vote is '''Delete'''. [[User:Sallicio|'''It's me...Sallicio!''']][[User talk:Sallicio|&lt;sup&gt;&lt;math&gt;\color{Red} \oplus&lt;/math&gt;&lt;/sup&gt;]] 23:23, 1 December 2015 (UTC)

::'''Keep''' - this person's notability is not to be questioned. Indeed, the article contains a lot of names, but some contribute to the person's notability- I'm no art expert but I guess Picasso or Jeff Koons are famous artists. The article needs to be improved by wikifying its presentation, I do agree that the neverending lists of names are confusing, but that's not the subject of the AfD.[[User:Waka waka1509|Waka waka1509]] ([[User talk:Waka waka1509|talk]]) 11:15, 2 December 2015 (UTC)
*'''Comment'''. Note that Chevalier of the Ordre des Arts et des Lettres is approximately equivalent to a British MBE, which is not considered to qualify a recipient for inherent notability under [[WP:ANYBIO]]. -- [[User:Necrothesp|Necrothesp]] ([[User talk:Necrothesp|talk]]) 15:00, 2 December 2015 (UTC)
:&lt;small class=&quot;delsort-notice&quot;&gt;Note:  This debate has been included in the [[Wikipedia:WikiProject Deletion sorting/France|list of France-related deletion discussions]]. [[User:Necrothesp|Necrothesp]] ([[User talk:Necrothesp|talk]]) 15:02, 2 December 2015 (UTC)&lt;/small&gt;
:&lt;small class=&quot;delsort-notice&quot;&gt;Note:  This debate has been included in the [[Wikipedia:WikiProject Deletion sorting/Museums and libraries|list of Museums and libraries-related deletion discussions]]. [[User:Necrothesp|Necrothesp]] ([[User talk:Necrothesp|talk]]) 15:02, 2 December 2015 (UTC)&lt;/small&gt;
:&lt;small class=&quot;delsort-notice&quot;&gt;Note:  This debate has been included in the [[Wikipedia:WikiProject Deletion sorting/Visual arts|list of Visual arts-related deletion discussions]]. [[User:Necrothesp|Necrothesp]] ([[User talk:Necrothesp|talk]]) 15:02, 2 December 2015 (UTC)&lt;/small&gt;</text>
      <sha1>dphsjuhayhi5f8p58mxzokb5jhervd5</sha1>
    </revision>
  </page>
  <page>
    <title>Recursive wave</title>
    <ns>0</ns>
    <id>48635032</id>
    <revision>
      <id>693304039</id>
      <parentid>692971685</parentid>
      <timestamp>2015-12-01T18:31:51Z</timestamp>
      <contributor>
        <username>Iridescent</username>
        <id>937705</id>
      </contributor>
      <minor />
      <comment>[[WP:AWB/T|Typo fixing]], [[WP:AWB/T|typo(s) fixed]]: similiar → similar, an unit → a unit using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve">[[File:Recwave1.gif|thumbnail|A recursive wave of depth 1, 2 and 3.]]

A '''recursive wave''' is a self-similar curve in three dimensional space that is constructed by iteratively adding a helix around the previous curve.

== Construction ==

A recursive wave of depth &lt;math&gt;n&lt;/math&gt; can be constructed as following:

{{Equation box 1 |indent =: |title= |equation = &lt;math&gt;\psi_0(x) = x(ai + bi + ck)&lt;/math&gt; |cellpadding= 6 |border |border colour = #0073CF |background colour=#F5FFFA}}

{{Equation box 1 |indent =: |title= |equation = &lt;math&gt;\psi_{n}(x) = \psi_{n-1}(x) + R(A(n)g_n(x),\ \psi_{n-1}'(x),\ f(n)x + \alpha(n))&lt;/math&gt; |cellpadding= 6 |border |border colour = #0073CF |background colour=#F5FFFA}}

Where  

{{Equation box 1 |indent =: |title= |equation = &lt;math&gt;g_n(x) = |\vec{w} \times |\psi_{n-1}'(x)||&lt;/math&gt; |cellpadding= 6 |border |border colour = #0073CF |background colour=#F5FFFA}}

And 

{{Equation box 1 |indent =: |title= |equation = &lt;math&gt;R(\vec{A}, \vec{B}, \theta) = e^{\vec{B}\theta /2}\vec{A}e^{-\vec{B}\theta /2}&lt;/math&gt; |cellpadding= 6 |border |border colour = #0073CF |background colour=#F5FFFA}}

==Clarification==

Each wave at non-zero depth &lt;math&gt;n&lt;/math&gt; is described by an amplitude &lt;math&gt;A(n)&lt;/math&gt;, frequency &lt;math&gt;f(n)&lt;/math&gt; and phase offset &lt;math&gt;\alpha(n)&lt;/math&gt;.

&lt;math&gt;g_n(x)&lt;/math&gt; represents a unit vector that is perpendicular to the previous curve at &lt;math&gt;x&lt;/math&gt;. An arbitrary vector &lt;math&gt;\vec{w}&lt;/math&gt; is chosen to be the fixed &quot;rag&quot; vector. 

&lt;math&gt;R&lt;/math&gt; is a function that rotates a vector &lt;math&gt;\vec{A}&lt;/math&gt; around an axis defined by a vector &lt;math&gt;\vec{B}&lt;/math&gt; by &lt;math&gt;\theta&lt;/math&gt; degrees. In this case it is expressed with quaternions.

{{uncat|date=November 2015}}</text>
      <sha1>dchy6hl2flhbmcgt3jtjscekdji19ck</sha1>
    </revision>
  </page>
  <page>
    <title>Wikipedia:Articles for deletion/Lakme (2016 film)</title>
    <ns>4</ns>
    <id>48640767</id>
    <revision>
      <id>693344097</id>
      <parentid>693332826</parentid>
      <timestamp>2015-12-01T23:39:17Z</timestamp>
      <contributor>
        <username>Sallicio</username>
        <id>5859517</id>
      </contributor>
      <comment>delete</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve">===[[Lakme (2016 film)]]===
{{REMOVE THIS TEMPLATE WHEN CLOSING THIS AfD|M}}

:{{la|Lakme (2016 film)}} – (&lt;includeonly&gt;[[Wikipedia:Articles for deletion/Lakme (2016 film)|View AfD]]&lt;/includeonly&gt;&lt;noinclude&gt;[[Wikipedia:Articles for deletion/Log/2015 December 1#{{anchorencode:Lakme (2016 film)}}|View log]]&lt;/noinclude&gt;{{int:dot-separator}} &lt;span class=&quot;plainlinks&quot;&gt;[https://tools.wmflabs.org/jackbot/snottywong/cgi-bin/votecounter.cgi?page=Wikipedia:Articles_for_deletion/Lakme_(2016_film) Stats]&lt;/span&gt;)
:({{Find sources AFD|Lakme (2016 film)}})
Fails my [[WP:BEFORE]] checks at Google News and Google Books (See [https://www.google.com/?gws_rd=ssl#q=Lakme+%22Rayya+Labib%22&amp;tbm=nws this search]). Fails [[WP:GNG]] as there doesn't appear to be anything significant about the subject via normal Google search. Nothing at [http://www.bollywoodhungama.com/index/search/q/lakme?q=lakme Bollywood Hungama], for instance. No indication via sourced prose that principal photography has commenced, so it fails [[WP:NFF]] for now. I did find [http://www.deewaneindia.com/lakme-movie/ this reference] at deewaneindia.com, but it doesn't do much to help establish notability, and note that [https://en.wikipedia.org/w/index.php?title=Lakme_(2016_film)&amp;action=history the article creator] calls himself Deewane India. [[User:Cyphoidbomb|Cyphoidbomb]] ([[User talk:Cyphoidbomb|talk]]) 17:09, 24 November 2015 (UTC)
:&lt;small class=&quot;delsort-notice&quot;&gt;Note:  This debate has been included in the [[Wikipedia:WikiProject Deletion sorting/Film|list of Film-related deletion discussions]]. &lt;span style=&quot;border:2px solid #090E0E;padding:0px;&quot;&gt;&lt;font style=&quot;color:#FFFFFF;background:#000000;&quot;&gt;[[User:Musa Raza|Musa]]&lt;/font&gt;[[User talk:Musa Raza|&lt;font style=&quot;color:#000000;background:#00B6B3;&quot;&gt;&amp;nbsp;Talk&amp;nbsp;&lt;/font&gt;]] &lt;/span&gt; 23:40, 24 November 2015 (UTC)&lt;/small&gt;
:&lt;small class=&quot;delsort-notice&quot;&gt;Note:  This debate has been included in the [[Wikipedia:WikiProject Deletion sorting/India|list of India-related deletion discussions]]. &lt;span style=&quot;border:2px solid #090E0E;padding:0px;&quot;&gt;&lt;font style=&quot;color:#FFFFFF;background:#000000;&quot;&gt;[[User:Musa Raza|Musa]]&lt;/font&gt;[[User talk:Musa Raza|&lt;font style=&quot;color:#000000;background:#00B6B3;&quot;&gt;&amp;nbsp;Talk&amp;nbsp;&lt;/font&gt;]] &lt;/span&gt; 23:40, 24 November 2015 (UTC)&lt;/small&gt;

:type:({{Find sources AFD|Lakme (film)}})
:director:({{Find sources AFD|Sanjay Khandelwal}})
:lead:({{Find sources AFD| Rayya Labib}})
:writer:({{Find sources AFD|Devender Kishor}})
:music:({{Find sources AFD|Jitu Bhowmik}})
*I'd opine a '''temporary deletion''' until we have '''A)''' confirmation of filming and '''B)''' more sources toward production.  Not that WP:BEFORE wasn't followed, but a flaw inherent with Google is that it does not does not crawl nor index Indian newspaper articles properly. Understanding and addressing this flaw, {{u|Titodutta}} created Indian-specific search engines at [[WP:INDAFD]]. For Indian topics, it is best used when Google fails us.... '''and''' it gives us [https://4d1fac7452666a9f1a1e7b6fa5af791fa725173b.googledrive.com/host/0B3ke7sJYbO1gVkRjUVh6bmtqeVU/IndianEnglishNewspaperSearch.html#gsc.tab=0&amp;gsc.q=%22Sanjay%20Khandelwal%22&amp;gsc.sort= &quot;Sanjay Khandelwal&quot;] [https://4d1fac7452666a9f1a1e7b6fa5af791fa725173b.googledrive.com/host/0B3ke7sJYbO1gVkRjUVh6bmtqeVU/IndianEnglishNewspaperSearch.html#gsc.tab=0&amp;gsc.q=%22Rayya%20Labib%22&amp;gsc.sort= &quot;Rayya Labib&quot;] [https://4d1fac7452666a9f1a1e7b6fa5af791fa725173b.googledrive.com/host/0B3ke7sJYbO1gVkRjUVh6bmtqeVU/IndianEnglishNewspaperSearch.html#gsc.tab=0&amp;gsc.q=%22Devender%20Kishor%22&amp;gsc.sort= &quot;Devender Kishor&quot;] [https://4d1fac7452666a9f1a1e7b6fa5af791fa725173b.googledrive.com/host/0B3ke7sJYbO1gVkRjUVh6bmtqeVU/IndianEnglishNewspaperSearch.html#gsc.tab=0&amp;gsc.q=%22Jitu%20Bhowmik%22&amp;gsc.sort= &quot;Jitu Bhowmik&quot;] [https://4d1fac7452666a9f1a1e7b6fa5af791fa725173b.googledrive.com/host/0B3ke7sJYbO1gVkRjUVh6bmtqeVU/IndianEnglishNewspaperSearch.html#gsc.tab=0&amp;gsc.q=%22Lakme%22&amp;gsc.sort= &quot;Lakme&quot;].   So we have sourcability, but not enough to meet [[WP:NFF|WP:NFF (paragraph 3)]]. '''[[User:MichaelQSchmidt|&lt;font color=&quot;blue&quot;&gt;Schmidt, &lt;/font&gt;]]''' ''[[User talk:MichaelQSchmidt|&lt;sup&gt;&lt;small&gt;Michael Q.&lt;/small&gt;&lt;/sup&gt;]]'' 04:41, 25 November 2015 (UTC)
&lt;div class=&quot;xfd_relist&quot; style=&quot;border-top: 1px solid #AAA; border-bottom: 1px solid #AAA; padding: 0px 25px;&quot;&gt;&lt;span style=&quot;color: #FF6600;&quot;&gt;'''{{resize|91%|[[Wikipedia:Deletion process#Relisting discussions|Relisted]] to generate a more thorough discussion and clearer consensus.}}'''&lt;/span&gt;&lt;br /&gt;
&lt;small&gt;Please add new comments below this notice. Thanks, [[User:Clpo13|clpo13]]&lt;sub&gt;([[User_talk:Clpo13|talk]])&lt;/sub&gt; 22:04, 1 December 2015 (UTC)&lt;/small&gt;&lt;!-- from Template:Relist --&gt;[[Category:Relisted AfD debates|Lakme (2016 film)]]&lt;/div&gt;&lt;!-- Please add new comments below this line --&gt;
'''DELETE''': I'm all for saving articles; however, if the author cannot find reliable sources before the end of this AfD, it should just be deleted. Sending it to the &quot;[[Stet|stet]] [[Docket (court)|docket]]&quot; does nothing. It will just sit there until the power stops running to the Wikipedia server. Either delete it or fix it. [[User:Sallicio|'''It's me...Sallicio!''']][[User talk:Sallicio|&lt;sup&gt;&lt;math&gt;\color{Red} \oplus&lt;/math&gt;&lt;/sup&gt;]] 23:39, 1 December 2015 (UTC)</text>
      <sha1>7klxq4069017brtjhpueoejowk47e9p</sha1>
    </revision>
  </page>
  <page>
    <title>Banach game</title>
    <ns>0</ns>
    <id>48645657</id>
    <revision>
      <id>692566014</id>
      <parentid>692565987</parentid>
      <timestamp>2015-11-26T16:54:37Z</timestamp>
      <contributor>
        <username>Michael Hardy</username>
        <id>4626</id>
      </contributor>
      <comment>\cdots</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve">In mathematics, the '''Banach game''' is a [[topological game]] introduced by [[Stefan Banach]] in 1935 in the second addendum to problem 43 of the [[Scottish book]] as a variation of the [[Banach–Mazur game]].&lt;ref name=&quot;The Scottish Book&quot;&gt;{{cite book|last1=Mauldin|first1=R. Daniel|title=The Scottish Book: Mathematics from the Scottish Cafe|date=April 1981|publisher=Birkhäuser|isbn=978-3-7643-3045-3|page=113|edition=1|url=http://users.auth.gr/siskakis/The%20Scottish%20book.pdf}}&lt;/ref&gt;

Given a subset &lt;math&gt;X&lt;/math&gt; of real numbers, two players alternatively write down arbitrary (not necessarily in &lt;math&gt;X&lt;/math&gt;) positive real numbers &lt;math&gt;x_0, x_1, x_2,\ldots&lt;/math&gt; such that &lt;math&gt;x_0 &gt; x_1 &gt; x_2 &gt;\cdots&lt;/math&gt; Player one wins if and only if &lt;math&gt;\sum^\infty_{i=0} x_i&lt;/math&gt; exists and is in &lt;math&gt;X&lt;/math&gt;.&lt;ref name=&quot;Telgarsky 1987&quot;&gt;{{cite journal|last1=Telgársky|first1=Rastislav|title=Topological Games: On the 50th Anniversary of the Banach–Mazur Game|journal=Rocky Mountain Journal of Mathematics|date=Spring 1987|volume=17|issue=2|pages=227–276|url=http://www.telgarsky.com/1987-RMJM-Telgarsky-Topological-Games.pdf}} at 242.&lt;/ref&gt;

One observation about the game is that if &lt;math&gt;X&lt;/math&gt; is a [[countable set]], then either of the players can cause the final sum to avoid the set.{{sfn|Mauldin|1981|p=116}} Thus in this situation the second player can win.

== References ==
{{reflist}}

== Further reading ==
* {{cite journal|last1=Moran|first1=Gadi|title=Existence of nondetermined sets for some two person games over reals|journal=Israel Journal of Mathematics|date=September 1971|volume=9|issue=3|pages=316–329|doi=10.1007/BF02771682|url=http://link.springer.com/article/10.1007%2FBF02771682}}

[[Category:Topological games]]</text>
      <sha1>0bt4v8wl609x7ebgxyytotkruhuufgf</sha1>
    </revision>
  </page>
  <page>
    <title>Draft:Communicability</title>
    <ns>118</ns>
    <id>48647888</id>
    <revision>
      <id>692397147</id>
      <parentid>692395002</parentid>
      <timestamp>2015-11-25T11:28:43Z</timestamp>
      <contributor>
        <username>Pablovm1990</username>
        <id>26890036</id>
      </contributor>
      <comment>Redifinion introduction</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve">{{AFC submission|t||ts=20151125102450|u=Pablovm1990|ns=118}} &lt;!--- Important, do not remove this line before article has been created. ---&gt;
Communicability, in [[Graph (mathematics)|network]] theory, defines the information between a pair of nodes, not only taking in account the shortest paths, but also other walks involving these nodes. First introduced by Estrada &amp; Hatano (2008)&lt;ref&gt;{{Cite journal|title = Communicability in complex networks|url = http://link.aps.org/doi/10.1103/PhysRevE.77.036111|journal = Physical Review E|date = 2008-03-11|pages = 036111|volume = 77|issue = 3|doi = 10.1103/PhysRevE.77.036111|first = Ernesto|last = Estrada|first2 = Naomichi|last2 = Hatano}}&lt;/ref&gt;, it's a measure of how a given &quot;particle&quot; could travel from one node to another, and reach again the first one. According to the article mentioned, most of the real-world networks display the largest communicability between the most connected 􏰂popular􏰃 nodes.

But this concept is not that important without the inclusion of two measures: communicability distance and angle. These two, will give us measures of the importance of a given node, and it'll expand the idea of [[betweenness centrality]], accounting other walks, rather than only the shortest paths.

== Definition ==
Given a graph &lt;math&gt;G = (V,E)&lt;/math&gt;, where &lt;math&gt;V&lt;/math&gt; represent the vertices and &lt;math&gt;E&lt;/math&gt; represents the edges. Let &lt;math&gt;A(G)&lt;/math&gt; be the adjacency matrix of &lt;math&gt;G&lt;/math&gt;, the communicability is defined by:

:&lt;math&gt;G_{pq} = \sum_{k=0}^{\infty} \frac{(A^k)_{pq}}{k!} = (e^A)_{pq}&lt;/math&gt;

where &lt;math&gt;p&lt;/math&gt; and &lt;math&gt;q&lt;/math&gt; are the start and finishing nodes, respectively. 

== Communicability measures ==

Extending communicability, Estrada add another measures, related to this idea. 

=== Communicability angle ===

=== Communicability distance ===
(like [[Betweenness centrality|betweenness centrality]])

indicates a measure of importance of that node, related to its [[Centrality|centrality]]. Thus, the disconnection (or deletion) of a node with high communicability, will lead to a disruption of the network, in terms of an increased [[Diameter (graph theory)|diameter of the network]], and how the information flows through the network itself. On the contrary, with a low communicability we'll found terminal nodes and non-important topologically ones.

==References==
{{reflist}}</text>
      <sha1>1laffzuefaogyq2wsncnjg9c5s6uycr</sha1>
    </revision>
  </page>
  <page>
    <title>Draft:Graph Sparsification</title>
    <ns>118</ns>
    <id>48651084</id>
    <revision>
      <id>692448518</id>
      <parentid>692427498</parentid>
      <timestamp>2015-11-25T19:44:43Z</timestamp>
      <contributor>
        <username>Graeme Bartlett</username>
        <id>38427</id>
      </contributor>
      <comment>remove db-afc tag</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve">{{AFC submission|t||ts=20150520013715|u=AshishBora1|ns=118}} &lt;!--- Important, do not remove this line before article has been created. ---&gt;

Given a graph, sparsification is the problem of constructing another graph with fewer number of edges or nodes such that certain properties of the original graph are approximately preserved. This can be used to dramatically speed up graph algorithms while incurring only a small error in the final answer

Two types of sparsification have been studied:

1. Spectrum-preserving Sparsification : Let &lt;math&gt;L_G&lt;/math&gt; be the Laplacian of the graph &lt;math&gt;G&lt;/math&gt;. Then a graph H is said is an &lt;math&gt;\epsilon&lt;/math&gt; approximation of &lt;math&gt; G &lt;/math&gt; if &lt;math&gt;(1-\epsilon) x^t L_H x \leq x^t L_G x \leq (1-\epsilon) x^t L_H x&lt;/math&gt; for all &lt;math&gt;x&lt;/math&gt;. The aim in spectrum preserving sparsification is to produce a graph &lt;math&gt;H&lt;/math&gt; which is an &lt;math&gt;\epsilon&lt;/math&gt; approximation of &lt;math&gt;G&lt;/math&gt;, but has fewer number of edges.

2. Cut-preserving Sparsification : A cut is a partition of vertices of a graph into two disjoint sets. Value of the cut is the sum of all the edges that cross between these two sets. Cut preserving sparsification aims to find a new graph with lesser number of edges, but all graph where cut values are preserved approximately.





==References==
{{reflist}}
&lt;!--- After listing your sources please cite them using inline citations and place them after the information they cite. Please see http://en.wikipedia.org/wiki/Wikipedia:REFB for instructions on how to add citations. ---&gt;

* Spielman, Daniel A., and Nikhil Srivastava. &quot;Graph sparsification by effective resistances.&quot; SIAM Journal on Computing 40.6 (2011): 1913-1926.
* Karger, David R. &quot;Using Randomized Sparsification to Approximate Minimum Cuts.&quot; SODA. Vol. 94. 1994.</text>
      <sha1>5zg18k82mujbl5wfo6s0fiyb2xvdlu2</sha1>
    </revision>
  </page>
  <page>
    <title>Wikipedia:Reference desk/Archives/Mathematics/2015 November 15</title>
    <ns>4</ns>
    <id>48653296</id>
    <revision>
      <id>692485915</id>
      <timestamp>2015-11-26T01:17:44Z</timestamp>
      <contributor>
        <username>Scsbot</username>
        <id>1590000</id>
      </contributor>
      <comment>edited by robot:
archiving November 15</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve">&lt;noinclude&gt;&lt;!-- From archive header--&gt;
{{#ifeq:{{PAGENAME}}|Special:Undelete| |{{#if:|&lt;div style=&quot;display:none;&quot;&gt;}} {{#ifeq:{{NAMESPACE}}|Wikipedia|{{#switch:{{NAMESPACE}}|= |&lt;div style=&quot;display:none;&quot;&gt;}}|{{error:not substituted|Archive header}}&lt;div style=&quot;display:none;&quot;&gt;}}}} {{#if:|&lt;/div&gt;&lt;/div&gt;}}{| width = &quot;100%&quot;
|-
! colspan=&quot;3&quot; align=&quot;center&quot; | [[Wikipedia:Reference desk/Mathematics|Mathematics desk]]
|-
! width=&quot;20%&quot; align=&quot;left&quot;  | &amp;lt; [[Wikipedia:Reference desk/Archives/Mathematics/2015 November 14|November 14]]
! width=&quot;25%&quot; align=&quot;center&quot;|&amp;lt;&amp;lt; [[Wikipedia:Reference desk/Archives/Mathematics/October 2015|Oct]] | [[Wikipedia:Reference desk/Archives/Mathematics/November 2015|November]] | [[Wikipedia:Reference desk/Archives/Mathematics/December 2015|Dec]] &amp;gt;&amp;gt;
! width=&quot;20%&quot; align=&quot;right&quot; |{{#ifexist:Wikipedia:Reference desk/Archives/Mathematics/2015 November 16|[[Wikipedia:Reference desk/Archives/Mathematics/2015 November 16|November 16]]|[[Wikipedia:Reference desk/Mathematics|Current desk]]}} &amp;gt;
|}
{| align=center width=95% style=&quot;background: #FFFFFF; border: 1px solid #003EBA;&quot; cellpadding=&quot;8&quot; cellspacing=&quot;0&quot;
|-
! style=&quot;background: #5D7CBA; text-align: center; font-family:Arial; color:#FFFFFF;&quot; | '''Welcome to the Wikipedia Mathematics Reference Desk Archives'''
|-
| The page you are currently viewing is {{#ifexist:Wikipedia:Reference desk/Archives/Mathematics/2015 November 25|an archive page|a [[Wikipedia:Transclusion|transcluded]] archive page}}. While you can leave answers for any questions shown below, please ask new questions on one of the [[Wikipedia:Reference desk|current reference desk]] pages.
|}


__TOC__
&lt;/noinclude&gt;
= November 15 =

== piecewise polynomial least squares ==

(I tried math.stackexchange and got no response for a week.  Boo hoo.)

I have in mind a project involving a least-squares fit using piecewise polynomials; at a finite number of known arguments ''x&lt;sub&gt;j&lt;/sub&gt;'', the ''k&lt;sub&gt;j&lt;/sub&gt;''th derivative is discontinuous.

How many basis functions are needed?  My guess is: ''x&lt;sup&gt;n&lt;/sup&gt;'' for 0≤''n''&amp;lt;min(''k''), and then, for each ''j,n'' such that ''k&lt;sub&gt;j&lt;/sub&gt;'' ≤ ''n'' ≤ the maximum degree, a pair of functions which are zero on one side and (''x''-''x&lt;sub&gt;j&lt;/sub&gt;'')&lt;sup&gt;''n''&lt;/sup&gt; on the other.  Is that right?

In general, I welcome any pointers that might reduce the number of wheels I'll reinvent. —[[User:Tamfang|Tamfang]] ([[User talk:Tamfang|talk]]) 07:00, 15 November 2015 (UTC)

:Just trying to understand the problem here.  You are trying to create a spline composed of multiple polynomial arcs, right ?  The adjacent arc endpoints must have point continuity, of course, but how about tangent &amp; curvature continuity, etc. ?  Since you are using least squares method, I assume you don't need an exact fit.  So, how many points would each arc run through ?  (Just offhand, this method sounds like it would generate an extremely &quot;lumpy&quot; spline.)  I assume you already know how the number of constraints relates to the degree of the polynomial ? [[User:StuRat|StuRat]] ([[User talk:StuRat|talk]]) 07:12, 15 November 2015 (UTC)

:: Sure, let's say I'm trying to create a spline composed of multiple polynomial arcs, and the degree of continuity is ''k&lt;sub&gt;j&lt;/sub&gt;''-1.  Maybe I '''like''' it &lt;s&gt;[[Cheese Shop sketch|runny]]&lt;/s&gt; lumpy; if it's lumpier than I like, I'll increase ''k&lt;sub&gt;j&lt;/sub&gt;''.  Rather than discrete points, my input is piecewise continuous, so the algo involves integrals rather than sums.  Number of constraints, in the sense I think you mean, is not meaningful here. —[[User:Tamfang|Tamfang]] ([[User talk:Tamfang|talk]]) 08:51, 15 November 2015 (UTC)

:If the function is on the domain &lt;math&gt;[x_0,x_m]&lt;/math&gt;, and the polynomials are of degree at most ''d'', and for &lt;math&gt;1\le j&lt;m&lt;/math&gt; the derivatives at &lt;math&gt;x_j&lt;/math&gt; are expected to be continuous up to &lt;math&gt;k_j-1&lt;/math&gt; (&lt;math&gt;k_j&lt;/math&gt; constraints), then I'm pretty sure the number of degrees of freedom is &lt;math&gt;m(d+1)-\sum_{j=1}^{m-1}k_j&lt;/math&gt;. -- [[User:Meni Rosenfeld|Meni Rosenfeld]] ([[User Talk:Meni Rosenfeld|talk]]) 09:53, 15 November 2015 (UTC)
:And I think the following basis functions will work (probably the same as what you wrote, but I think is clearer): Letting &lt;math&gt;k_0=0&lt;/math&gt;, for each &lt;math&gt;0 \le j &lt; m&lt;/math&gt; and &lt;math&gt;k_j \le n \le d&lt;/math&gt;, the function which is 0 for &lt;math&gt;x \le x_j&lt;/math&gt; and &lt;math&gt;(x-x_j)^n&lt;/math&gt; for &lt;math&gt;x&gt;x_j&lt;/math&gt;. This also means their number can be rewritten as &lt;math&gt;\sum_{j=0}^{m-1}(d+1-k_j)&lt;/math&gt;. -- [[User:Meni Rosenfeld|Meni Rosenfeld]] ([[User Talk:Meni Rosenfeld|talk]]) 10:06, 15 November 2015 (UTC)

:: Hm ... thanks, yes, I think that does work; the ''k''&lt;sub&gt;0&lt;/sub&gt;=0 is a good gimmick (removing some special cases from the description). You've saved me some redundancy; I was thinking that for each discontinuity I'd need ''pairs'' of functions: zero on the left and (''x''-''x&lt;sub&gt;j&lt;/sub&gt;'')&lt;sup&gt;''n''&lt;/sup&gt; on the right, (''x''-''x&lt;sub&gt;j&lt;/sub&gt;'')&lt;sup&gt;''n''&lt;/sup&gt; on the left and zero on the right. —[[User:Tamfang|Tamfang]] ([[User talk:Tamfang|talk]]) 03:51, 24 November 2015 (UTC)

:This is above my head (I don't know why I even look at this notice board!) but does [[Savitzky–Golay filter]] help? [[User:Thincat|Thincat]] ([[User talk:Thincat|talk]]) 09:00, 20 November 2015 (UTC)

:: That's interesting, but no. —[[User:Tamfang|Tamfang]] ([[User talk:Tamfang|talk]]) 03:03, 22 November 2015 (UTC)</text>
      <sha1>2fwbj6r6d261pdg3n6fc6e33h8epfzd</sha1>
    </revision>
  </page>
  <page>
    <title>Wikipedia:Reference desk/Archives/Mathematics/2015 November 19</title>
    <ns>4</ns>
    <id>48653303</id>
    <revision>
      <id>692485988</id>
      <timestamp>2015-11-26T01:18:25Z</timestamp>
      <contributor>
        <username>Scsbot</username>
        <id>1590000</id>
      </contributor>
      <comment>edited by robot:
archiving November 19</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve">&lt;noinclude&gt;&lt;!-- From archive header--&gt;
{{#ifeq:{{PAGENAME}}|Special:Undelete| |{{#if:|&lt;div style=&quot;display:none;&quot;&gt;}} {{#ifeq:{{NAMESPACE}}|Wikipedia|{{#switch:{{NAMESPACE}}|= |&lt;div style=&quot;display:none;&quot;&gt;}}|{{error:not substituted|Archive header}}&lt;div style=&quot;display:none;&quot;&gt;}}}} {{#if:|&lt;/div&gt;&lt;/div&gt;}}{| width = &quot;100%&quot;
|-
! colspan=&quot;3&quot; align=&quot;center&quot; | [[Wikipedia:Reference desk/Mathematics|Mathematics desk]]
|-
! width=&quot;20%&quot; align=&quot;left&quot;  | &amp;lt; [[Wikipedia:Reference desk/Archives/Mathematics/2015 November 18|November 18]]
! width=&quot;25%&quot; align=&quot;center&quot;|&amp;lt;&amp;lt; [[Wikipedia:Reference desk/Archives/Mathematics/October 2015|Oct]] | [[Wikipedia:Reference desk/Archives/Mathematics/November 2015|November]] | [[Wikipedia:Reference desk/Archives/Mathematics/December 2015|Dec]] &amp;gt;&amp;gt;
! width=&quot;20%&quot; align=&quot;right&quot; |{{#ifexist:Wikipedia:Reference desk/Archives/Mathematics/2015 November 20|[[Wikipedia:Reference desk/Archives/Mathematics/2015 November 20|November 20]]|[[Wikipedia:Reference desk/Mathematics|Current desk]]}} &amp;gt;
|}
{| align=center width=95% style=&quot;background: #FFFFFF; border: 1px solid #003EBA;&quot; cellpadding=&quot;8&quot; cellspacing=&quot;0&quot;
|-
! style=&quot;background: #5D7CBA; text-align: center; font-family:Arial; color:#FFFFFF;&quot; | '''Welcome to the Wikipedia Mathematics Reference Desk Archives'''
|-
| The page you are currently viewing is {{#ifexist:Wikipedia:Reference desk/Archives/Mathematics/2015 November 29|an archive page|a [[Wikipedia:Transclusion|transcluded]] archive page}}. While you can leave answers for any questions shown below, please ask new questions on one of the [[Wikipedia:Reference desk|current reference desk]] pages.
|}


__TOC__
&lt;/noinclude&gt;
= November 19 =

== Finite endomorphism ring ==

Is there an infinite abelian group with a finite endomorphism ring? [[User:GeoffreyT2000|GeoffreyT2000]] ([[User talk:GeoffreyT2000|talk]]) 01:17, 19 November 2015 (UTC)

:I don't think so.  If an abelian group has a finite endomorphism ring, then it is necessarily a torsion abelian group.  (Otherwise multiplication by an integer gives an obvious injection from the set of integers into the endomorphism ring.)  In fact, for the same reason, the elements must have bounded order.  By the [[Prüfer theorems|first Prüfer theorem]], a torsion abelian group of this kind is isomorphic to a direct sum of cyclic groups.  Because the order of these cyclic groups is bounded, infinitely many direct factors are repeated by the pigeonhole principle, and so in that case the endomorphism ring is uncountably infinite.  &lt;small&gt;&lt;span style=&quot;display:inline-block;vertical-align:-.3em;line-height:.8em;text-align:right;text-shadow:black 1pt 1pt 1pt&quot;&gt;[[User:Slawekb|&lt;big&gt;S&lt;/big&gt;ławomir]]&lt;br/&gt;&lt;font color=&quot;red&quot;&gt;[[User talk:Slawekb|Biały]]&lt;/font&gt;&lt;/span&gt;&lt;/small&gt; 12:07, 19 November 2015 (UTC)

==Probability distributions, frequency of events==
Hi, I'd like to create an IID stochastic process &lt;math&gt;O_n \in \{0,1\}&lt;/math&gt;, where 1 indicates that an event has occurred, and 0 that it has not. I had been using &lt;math&gt;O_n \thicksim B(p) &lt;/math&gt;, where B(p) is the [[Bernoulli distribution]] with parameter p. This works ok, and I can vary the mean frequency of events, but the variance is p(1-p), and so the variance increases as I increase frequency - I am primarily interested in &lt;math&gt; p \in (0,0.5)&lt;/math&gt;. What distribution should I use that would allow me to manipulate the frequency and variance independently? Or at least have a fixed variance for all frequencies? I need at the end to have a string of length N like 010...001, so counting events in an interval is not helpful. To be clear, I'd like to have the time between events (expected value of 1/p) have the same variance for all frequencies p. I can think of a few ways to generate strings with the necessary properties programmatically, but it would be far better if I could do it with a simple distribution. Any ideas? I feel like there must be something simple I'm forgetting about. Thanks, [[User:SemanticMantis|SemanticMantis]] ([[User talk:SemanticMantis|talk]]) 16:14, 19 November 2015 (UTC)
:That's impossible. The ''only'' distribution with support &lt;math&gt;\{0,1\}&lt;/math&gt; is Bernoulli. To get what you described (which may or may not be what you want), you'll have to make your events dependent (contrary to the assumption of IID).
:And the best way to do that is probably to start with the distribution you want for the time between successful events, with a given mean and variance (plenty of choice there - a good choice is the maximum entropy distribution), and simply running that and deriving the process (with 0's to fill in the gaps between successive 1's). But again, this will mean that there will be dependence between the events at given times (the dependence will be stronger for nearby events). -- [[User:Meni Rosenfeld|Meni Rosenfeld]] ([[User Talk:Meni Rosenfeld|talk]]) 18:53, 19 November 2015 (UTC)
::{{ping|Meni Rosenfeld}} D'oh! Thanks, that makes sense, I forgot to leave out option &quot;c) Is this impossible?&quot; -- of course I see now that what I asked for is indeed impossible. To clarify, you're suggesting that I could instead create an IID process S_n for the spaces, then create O=1...1...1, where the number of zeros in the ... is S_n. Then O_n is not IID, but I could create S_n such that the mean and variance are independent. But can I do that with maximal entropy? E.g. I thought the [[geometric distribution]] had maximal entropy on {(0),1,2,...}, and that won't let me pick mean and variance independently. If I want to demand independent mean and variance, what are my options for support on \mathbb{N}? I think maybe I can re-parameterize the [[Beta-binomial distribution]] by mean and variance like you can do with the [[Beta distribution]], but that's only quasi-independent, because once you pick the mean it bounds the allowable variances. I think I may well stick with my Bernoulli set up for now, but I'm interested in the time-dependent case as possible future refinement. [[User:SemanticMantis|SemanticMantis]] ([[User talk:SemanticMantis|talk]]) 19:54, 19 November 2015 (UTC)
:::I guess I could pick the mean \mu and then let S_n = DiscreteUniform(\mu-k, \mu+k). Variance could then at least be arbitrarily large or small. [[User:SemanticMantis|SemanticMantis]] ([[User talk:SemanticMantis|talk]]) 20:01, 19 November 2015 (UTC)
::::Yes, that is the process I proposed.
::::I meant, &quot;maximum entropy for given mean and variance supported on positive integers&quot; (geometric is max. ent. for given mean, without variance specification). This kind of distribution has the same form as the normal distribution, but the scale &amp; shift parameters will not be exactly the mean and s.d. (since the restriction to positive integers changes the mean and variance for a given formula). You'd have to do a bit of work to find the correct parameters.
::::Alternatively, a combination of binomial distribution (for low variance) and negative binomial (for high variance) can work, and it's easier to find the parameters, but they don't cover the entire possibilities of mean &amp; variance. -- [[User:Meni Rosenfeld|Meni Rosenfeld]] ([[User Talk:Meni Rosenfeld|talk]]) 20:18, 19 November 2015 (UTC)
::::Note also that a uniform distribution will be fairly restrictive - you must keep it positive, so it limits how wide an interval you can take, and hence you can't have high variance. Negative binomial doesn't have this problem. -- [[User:Meni Rosenfeld|Meni Rosenfeld]] ([[User Talk:Meni Rosenfeld|talk]]) 20:31, 19 November 2015 (UTC)
:::::Thanks again Meni, very helpful. [[User:SemanticMantis|SemanticMantis]] ([[User talk:SemanticMantis|talk]]) 22:31, 19 November 2015 (UTC)</text>
      <sha1>97kr39a252pfly5lxvkirirpmnuak4g</sha1>
    </revision>
  </page>
  <page>
    <title>Ground heat exchanger</title>
    <ns>0</ns>
    <id>48653964</id>
    <revision>
      <id>692577459</id>
      <parentid>692514342</parentid>
      <timestamp>2015-11-26T18:36:18Z</timestamp>
      <contributor>
        <username>CAPTAIN RAJU</username>
        <id>25523690</id>
      </contributor>
      <minor />
      <comment>clean up, added [[CAT:UNCAT|uncategorised]] tag using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve">{{orphan|date=November 2015}}

{{merge to|Geothermal heat pump|date=November 2015}}
Ground heat exchangers (GHEs), which are also called geothermal heat exchangers, have emerged as a promising and globally accepted way of exploiting shallow geothermal energy, for example ground-coupled heat pumps, ground heat storage. A GHE is essentially a pipe (e.g., U-, W-, or helical-shaped) in a vertical borehole or a foundation pile of a building, in which a circulating heat-carrying fluid absorbs (or discharges) heat from (or to) the ground.&lt;ref name=&quot;Review&quot;&gt;Li M, Lai ACK. Review of analytical models for heat transfer by vertical ground heat exchangers (GHEs): A perspective of time and space scales, Applied Energy 20015; 151: 178-191.&lt;/ref&gt;&lt;ref name=&quot;H&quot;&gt;Hellstrom G. Ground heat storage – thermal analysis of duct storage systems I. Theory. Lund: University of Lund; 1991.&lt;/ref&gt;
GHEs can have various configurations. This article discusses two kinds of closed loop GHEs, i.e., borehole and foundation pile GHEs. The borehole type is the most common. It consists of one or two U-shaped pipes that are inserted into a vertical borehole and connected to a heat pump or a heating system to form a closed loop. A U-shaped channel usually comprises two small-diameter high-density polyethylene (HDPE) tubes thermally fused to form a U-shaped bend at the bottom.&lt;ref&gt;ASHRAE. ASHRAE handbook: HVAC applications. Atlanta: ASHRAE, Inc; 2011.&lt;/ref&gt; The space between the wall of the borehole and the U-shaped tubes is usually grouted completely with grouting material or, in some cases, partially filled with groundwater.&lt;ref&gt;Kavanaugh SK, Rafferty K. Ground-source heat pumps: Design of geothermal systems for commercial and institutional buildings. Atlanta, GA: American Society of Heating, Refrigerating and Air-Conditioning Engineers, Inc.; 1997.&lt;/ref&gt; The depth of the hole (generally from 30 m to 200 m) depends strongly on local geological conditions and available drilling equipment.
In a foundation pile GHE (or energy pile), the heat transfer tubes are inside the steel frame of a foundation pile. There are various possible shapes. Foundation piles are usually much shallower than boreholes and have a greater radius. Since energy piles generally require less land area, this technology is evoking increasing interest in the ground-source heat pumps community.

== Analysis of heat transfer by GHEs ==
A huge challenge in predicting the thermal response of a GHE is the diversity of the time and space scales involved.Four space scales and eight time scales are involved in the heat transfer of GHEs. The first space scale having practical importance is the diameter of the borehole (~ 0.1 m) and the associated time is on the order of 1 hr, during which the effect of the heat capacity of the backfilling material is significant. The second important space dimension is the half distance between two adjacent boreholes, which is on the order of several meters. The corresponding time is on the order of a month, during which the thermal interaction between adjacent boreholes is important. The largest space scale can be tens of meters or more, such as the half length of a borehole and the horizontal scale of a GHE cluster. The time scale involved is as long as the lifetime of a GHE (decades).&lt;ref&gt;Li M, Li P, Chan V, Lai ACK. Full-scale temperature response function (G-function) for heat transfer by borehole ground heat exchangers (GHEs) from sub-hour to decades. Appl Energy 2014; 136: 197-205.&lt;/ref&gt;

The short-term hourly temperature response of the ground is vital for analyzing the energy of ground-source heat pump systems and for their optimum control and operation. By contrast, the long-term response determines the overall feasibility of a system from the standpoint of life cycle. Addressing the complete spectrum of time scales require vast computational resources.

=== Formalization of the heat transfer problem ===
The main questions that engineers may ask in the early stages of designing a GHE are (a) what the heat transfer rate of a GHE as a function of time is, given a particular temperature difference between the circulating fluid and the ground, and (b) what the temperature difference as a function of time is, given a required heat exchange rate. In the language of heat transfer, the two questions can probably be expressed as
&lt;math&gt;q_l = [T_f(t) - T_0]/R(t)&lt;/math&gt;

where ''T''&lt;sub&gt;f&lt;/sub&gt; is the average temperature of the circulating fluid, ''T''&lt;sub&gt;0&lt;/sub&gt; is the effective, undisturbed temperature of the ground, ''q&lt;sub&gt;l&lt;/sub&gt;'' is the heat transfer rate of the GHE per unit time per unit length (W/m), and ''R'' is the total thermal resistance (m'''&lt;sup&gt;.&lt;/sup&gt;'''K/W).''R''(''t'') is often an unknown variable that needs to be determined by heat transfer analysis. Despite ''R''(''t'') being a function of time, analytical models exclusively decompose it into a time-independent part and a time-dependent part to simplify the analysis.

Various models for the time-independent and time-dependent R can be found in the references.&lt;ref name=&quot;Review&quot;/&gt;&lt;ref name=&quot;H&quot;/&gt;

==References==
{{Reflist}}

{{Uncategorized|date=November 2015}}</text>
      <sha1>mnl9175ycc2p1oa3eqcwnwxvsu7h08j</sha1>
    </revision>
  </page>
  <page>
    <title>Littlewood's 4/3 inequality</title>
    <ns>0</ns>
    <id>48657998</id>
    <revision>
      <id>692873205</id>
      <parentid>692871827</parentid>
      <timestamp>2015-11-28T23:01:36Z</timestamp>
      <contributor>
        <username>Michael Hardy</username>
        <id>4626</id>
      </contributor>
      <comment>/* See also */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve">In [[mathematical analysis]], '''Littlewood's 4/3 inequality''', named after [[John Edensor Littlewood]],&lt;ref&gt;{{cite journal|last1=Littlewood|first1=J. E.|title=On bounded bilinear forms in an infinite number of variables|journal=The Quarterly Journal of Mathematics|date=1930|issue=1|pages=164–174|doi=10.1093/qmath/os-1.1.164}}&lt;/ref&gt; is an inequality that holds for every complex-valued [[bilinear form]] defined on [[c0 space|''c''&lt;sub&gt;0&lt;/sub&gt;]], the [[Banach space]] of real sequences that converge to zero.

Precisely, let ''B'':''c''&lt;sub&gt;0&lt;/sub&gt; × ''c''&lt;sub&gt;0&lt;/sub&gt; → ℂ be a bilinear form. Then the following holds:

:&lt;math&gt;\left( \sum_{i,j=1}^\infty |B(e_i,e_j)|^{4/3} \right)^{3/4} \le \sqrt{2} \| B \|,&lt;/math&gt;

where 
:&lt;math&gt;\| B \| = \sup \{|B(x_1,x_2)|: \|x_i\|_\infty \le 1 \}.&lt;/math&gt;

==Generalizations==
===Bohnenblust–Hille inequality===
Bohnenblust–Hille inequality&lt;ref&gt;{{cite journal|last1=Bohnenblust|first1=H. F.|last2=Hille|first2=Einar|title=On the Absolute Convergence of Dirichlet Series|journal=The Annals of Mathematics|date=1931|volume=32|issue=3|pages=600–622|doi=10.2307/1968255}}&lt;/ref&gt; is a [[multilinear]] extension of Littlewood's inequality that states that for all ''m''-linear mapping ''M'':''c''&lt;sub&gt;0&lt;/sub&gt; × ... × ''c''&lt;sub&gt;0&lt;/sub&gt; → ℂ the following holds:

:&lt;math&gt;\left( \sum_{i_1,\ldots,i_m=1}^\infty |M(e_{i_1},\ldots,e_{i_m})|^{2m/(m+1)} \right)^{(m+1)/(2m)} \le 2^{(m-1)/2} \| M \|,&lt;/math&gt;

==See also==
 * [[Grothendieck inequality]]

==References==
{{Reflist}}


[[Category:Theorems in analysis]]
[[Category:Inequalities]]

{{Mathanalysis-stub}}</text>
      <sha1>1iolpei8ys7qxvf68hubo69znytsjzk</sha1>
    </revision>
  </page>
  <page>
    <title>Wikipedia:Reference desk/Archives/Mathematics/2015 November 20</title>
    <ns>4</ns>
    <id>48668166</id>
    <revision>
      <id>692741176</id>
      <timestamp>2015-11-28T00:06:34Z</timestamp>
      <contributor>
        <username>Scsbot</username>
        <id>1590000</id>
      </contributor>
      <comment>edited by robot:
archiving November 20</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve">&lt;noinclude&gt;&lt;!-- From archive header--&gt;
{{#ifeq:{{PAGENAME}}|Special:Undelete| |{{#if:|&lt;div style=&quot;display:none;&quot;&gt;}} {{#ifeq:{{NAMESPACE}}|Wikipedia|{{#switch:{{NAMESPACE}}|= |&lt;div style=&quot;display:none;&quot;&gt;}}|{{error:not substituted|Archive header}}&lt;div style=&quot;display:none;&quot;&gt;}}}} {{#if:|&lt;/div&gt;&lt;/div&gt;}}{| width = &quot;100%&quot;
|-
! colspan=&quot;3&quot; align=&quot;center&quot; | [[Wikipedia:Reference desk/Mathematics|Mathematics desk]]
|-
! width=&quot;20%&quot; align=&quot;left&quot;  | &amp;lt; [[Wikipedia:Reference desk/Archives/Mathematics/2015 November 19|November 19]]
! width=&quot;25%&quot; align=&quot;center&quot;|&amp;lt;&amp;lt; [[Wikipedia:Reference desk/Archives/Mathematics/October 2015|Oct]] | [[Wikipedia:Reference desk/Archives/Mathematics/November 2015|November]] | [[Wikipedia:Reference desk/Archives/Mathematics/December 2015|Dec]] &amp;gt;&amp;gt;
! width=&quot;20%&quot; align=&quot;right&quot; |{{#ifexist:Wikipedia:Reference desk/Archives/Mathematics/2015 November 21|[[Wikipedia:Reference desk/Archives/Mathematics/2015 November 21|November 21]]|[[Wikipedia:Reference desk/Mathematics|Current desk]]}} &amp;gt;
|}
{| align=center width=95% style=&quot;background: #FFFFFF; border: 1px solid #003EBA;&quot; cellpadding=&quot;8&quot; cellspacing=&quot;0&quot;
|-
! style=&quot;background: #5D7CBA; text-align: center; font-family:Arial; color:#FFFFFF;&quot; | '''Welcome to the Wikipedia Mathematics Reference Desk Archives'''
|-
| The page you are currently viewing is {{#ifexist:Wikipedia:Reference desk/Archives/Mathematics/2015 November 30|an archive page|a [[Wikipedia:Transclusion|transcluded]] archive page}}. While you can leave answers for any questions shown below, please ask new questions on one of the [[Wikipedia:Reference desk|current reference desk]] pages.
|}


__TOC__
&lt;/noinclude&gt;
= November 20 =

== Calculus formula on back of jacket ==

I saw a guy on a train wearing a fairly plain black jacket but there was a mathematical formula embroided on the back. 

:&lt;math&gt;{\delta V \over \delta t} + {1 \over 2} \sigma^2 S^2 {\delta^2 V \over \delta S^2}+ rS {\delta V \over \delta S} - rV = 0 &lt;/math&gt;

Does anyone recognize what this is and why would someone have it on their jacket? I'm guessing it's some math &quot;in joke&quot;. [[User:Vespine|Vespine]] ([[User talk:Vespine|talk]]) 03:36, 20 November 2015 (UTC)

:It's the [[Black-Scholes equation]].  As for what the intended message of putting it on a jacket was, your guess is as good as mine.--[[User:Antendren|Antendren]] ([[User talk:Antendren|talk]]) 08:29, 20 November 2015 (UTC)

::This was the equation that all the financial experts were using until it was realised recently that there was a flaw in the assumptions.  Sorry I can't remember the subtle details.  [[User:Dbfirs|''&lt;font face=&quot;verdana&quot;&gt;&lt;font color=&quot;blue&quot;&gt;D&lt;/font&gt;&lt;font color=&quot;#00ccff&quot;&gt;b&lt;/font&gt;&lt;font color=&quot;#44ffcc&quot;&gt;f&lt;/font&gt;&lt;font color=&quot;66ff66&quot;&gt;i&lt;/font&gt;&lt;font color=&quot;44ee44&quot;&gt;r&lt;/font&gt;&lt;font color=&quot;44aa44&quot;&gt;s&lt;/font&gt;&lt;/font&gt;'']] 23:10, 21 November 2015 (UTC)
::::That's as much of an answer as I was expecting, thank you! 
{{resolved}}
:The standard Black-Scholes equation computes the theoretical price of a European call option, which is a simple type of financial derivative. The jacket formula you saw is the generalized form of the equation, which is a partial differential equation that estimates the theoretical price of any &quot;derived&quot; asset in terms of the change in its &quot;strike&quot; asset. It's not flawed per se, but it assumes the asset return is normally distributed, which is not always the case in practice. My guess is the person you saw is a [[Quantitative_analyst|quant]]. [[User:OldTimeNESter|OldTimeNESter]] ([[User talk:OldTimeNESter|talk]]) 21:31, 23 November 2015 (UTC)</text>
      <sha1>d8me0z97an1rw59h2hu5zdxg5ua2hvr</sha1>
    </revision>
  </page>
  <page>
    <title>Wikipedia:Reference desk/Archives/Mathematics/2015 November 21</title>
    <ns>4</ns>
    <id>48668170</id>
    <revision>
      <id>692741241</id>
      <timestamp>2015-11-28T00:07:16Z</timestamp>
      <contributor>
        <username>Scsbot</username>
        <id>1590000</id>
      </contributor>
      <comment>edited by robot:
archiving November 21</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve">&lt;noinclude&gt;&lt;!-- From archive header--&gt;
{{#ifeq:{{PAGENAME}}|Special:Undelete| |{{#if:|&lt;div style=&quot;display:none;&quot;&gt;}} {{#ifeq:{{NAMESPACE}}|Wikipedia|{{#switch:{{NAMESPACE}}|= |&lt;div style=&quot;display:none;&quot;&gt;}}|{{error:not substituted|Archive header}}&lt;div style=&quot;display:none;&quot;&gt;}}}} {{#if:|&lt;/div&gt;&lt;/div&gt;}}{| width = &quot;100%&quot;
|-
! colspan=&quot;3&quot; align=&quot;center&quot; | [[Wikipedia:Reference desk/Mathematics|Mathematics desk]]
|-
! width=&quot;20%&quot; align=&quot;left&quot;  | &amp;lt; [[Wikipedia:Reference desk/Archives/Mathematics/2015 November 20|November 20]]
! width=&quot;25%&quot; align=&quot;center&quot;|&amp;lt;&amp;lt; [[Wikipedia:Reference desk/Archives/Mathematics/October 2015|Oct]] | [[Wikipedia:Reference desk/Archives/Mathematics/November 2015|November]] | [[Wikipedia:Reference desk/Archives/Mathematics/December 2015|Dec]] &amp;gt;&amp;gt;
! width=&quot;20%&quot; align=&quot;right&quot; |{{#ifexist:Wikipedia:Reference desk/Archives/Mathematics/2015 November 22|[[Wikipedia:Reference desk/Archives/Mathematics/2015 November 22|November 22]]|[[Wikipedia:Reference desk/Mathematics|Current desk]]}} &amp;gt;
|}
{| align=center width=95% style=&quot;background: #FFFFFF; border: 1px solid #003EBA;&quot; cellpadding=&quot;8&quot; cellspacing=&quot;0&quot;
|-
! style=&quot;background: #5D7CBA; text-align: center; font-family:Arial; color:#FFFFFF;&quot; | '''Welcome to the Wikipedia Mathematics Reference Desk Archives'''
|-
| The page you are currently viewing is {{#ifexist:Wikipedia:Reference desk/Archives/Mathematics/2015 December 1|an archive page|a [[Wikipedia:Transclusion|transcluded]] archive page}}. While you can leave answers for any questions shown below, please ask new questions on one of the [[Wikipedia:Reference desk|current reference desk]] pages.
|}


__TOC__
&lt;/noinclude&gt;
= November 21 =

==Infinite Integral Sum==
Hello! I'm a first year university student, and while practicing integration I made up a question &lt;math&gt; \int x^2 e^{x^2} dx &lt;/math&gt; and tried to solve it using integration by parts. I got nowhere after a short while and, after looking the integral up, it turns out there is no elementary integral for the problem. However, integration by parts gives me this infinite series:

:&lt;math&gt; \frac{1}{3}x^3e^{x^2}-\frac{2}{3}\left(\frac{1}{5}\right)x^5e^{x^2}+\frac{2}{3}\left(\frac{2}{5}\right)\left(\frac{1}{7}\right)x^7e^{x^2}-\frac{2}{3}\left(\frac{2}{5}\right)\left(\frac{2}{7}\right)\left(\frac{1}{9}\right)x^9e^{x^2}+\frac{2}{3}\left(\frac{2}{5}\right)\left(\frac{2}{7}\right)\left(\frac{2}{9}\right)\left(\frac{1}{11}\right)x^{11}e^{x^2} \pm \cdots &lt;/math&gt;

I was just wondering, is there a way to write this expression as an infinite sum? I can kind of see a pattern but I don't have enough experience with infinite sums to generate one (if it even exists!). Thanks for your help. [[Special:Contributions/70.54.112.243|70.54.112.243]] ([[User talk:70.54.112.243|talk]]) 04:15, 21 November 2015 (UTC)
:You just wrote down an infinite sum, so it is not clear to me what you want. You may put the first term outside parentheses to get the expression
::&lt;math&gt; \frac{1}{3}x^3 e^{x^2} \sum_{i=0}^\infty\prod_{k=1}^i \frac{-2x^2}{2k+3}&lt;/math&gt;
:Try also [[http://www.wolframalpha.com/input/?i=integrate+%28x%5E2%29%28e%5Ex%5E2%29dx]]
:[[User:Bo Jacoby|Bo Jacoby]] ([[User talk:Bo Jacoby|talk]]) 05:59, 21 November 2015 (UTC).
::This can also written without the product notation as &lt;math&gt;e^{x^2}\sum_{i=0}^{\infty}\frac{(-1)^i4^{i+1}(i+2)!}{(2i+4)!}x^{2i+3}&lt;/math&gt;. -- [[User:Meni Rosenfeld|Meni Rosenfeld]] ([[User Talk:Meni Rosenfeld|talk]]) 22:49, 21 November 2015 (UTC)
:I'm sure you are familiar with [[Capital-sigma notation]], but if you haven't seen [[Capital Pi notation]] before, you may wish to check out that article.  (And while reading on the subject, you should familiarize yourself with [[Empty product]].)  Finaly, the erfi in the indefinite integral on the Wolfram Alpha page Bo Jacoby linked above is the [[imaginary error function]]. -- [[User talk:Thinking of England|ToE]] 14:18, 21 November 2015 (UTC)
::In [[J (programming language)|J]] the function looks like this:
::::f=.13 : '3%~(y^3)*(^y^2)*+/*/\1,(-2*y^2)%3+2*1+i.10'
::and a test run looks like this:
::::f&amp;&gt;0 0.5 1 1.5 2 2.5
:::0 0.0485128 0.627815 5.0843 46.7366 1398.4
::[[User:Bo Jacoby|Bo Jacoby]] ([[User talk:Bo Jacoby|talk]]) 18:46, 22 November 2015 (UTC).
The [[Maclaurin series]] of this is straightforward to derive: &lt;math&gt;\int x^2e^{x^2} dx = \int x^2\sum_{i=0}^\infty \frac{(x^2)^i}{i!} dx = \int \sum_{i=0}^\infty \frac{x^{2i+2}}{i!} dx = \sum_{i = 0}^\infty \frac{x^{2i+3}}{i!(2i+3)} + C&lt;/math&gt;. Using the doubling formulas for the [[gamma function]], this can be easily shown to be equivalent to the series you derived. --[[User:Jasper Deng|Jasper Deng]] [[User talk:Jasper Deng|(talk)]] 23:33, 23 November 2015 (UTC)</text>
      <sha1>8sfvs7syp97zdibyo7yvso4rpfiyq1l</sha1>
    </revision>
  </page>
  <page>
    <title>Draft:Social Earnings Ratio</title>
    <ns>118</ns>
    <id>48672101</id>
    <revision>
      <id>693370580</id>
      <parentid>693369863</parentid>
      <timestamp>2015-12-02T03:22:40Z</timestamp>
      <contributor>
        <username>Social Earnings Ratio</username>
        <id>26914706</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve">{{AFC submission|d|essay|u=Social Earnings Ratio|ns=118|decliner=Onel5969|declinets=20151201131326|ts=20151129203703}} &lt;!-- Do not remove this line! --&gt;

{{AFC comment|1=What is the relationship between the subject of this article and the username of its creator? [[User:Robert McClenon|Robert McClenon]] ([[User talk:Robert McClenon|talk]]) 22:53, 29 November 2015 (UTC)}}

----

&lt;!-- EDIT BELOW THIS LINE --&gt;
The '''Social Earnings Ratio''' ('''s/e''') is a single number metric of non-financial value. It is the corollary to the [[Price earnings ratio|Price Earnings Ratio]] (p/e) which is the single number metric of financial value. It is an Open Source Creative Commons 4.0 development initiated in 2011.&lt;ref&gt;{{Cite web|title = Social Earnings Ratio|url = http://find.jorum.ac.uk/resources/18904|website = Jorum|accessdate = 2015-11-28}}&lt;/ref&gt;, and identified by the Vatican press as &quot;''the most rapidly adopted metric in the world''&quot;&lt;ref&gt;{{Cite web|title = Olinga Ta'eed: On Link Between Profit Motive and Reduction of Poverty|url = http://www.zenit.org/en/articles/olinga-ta-eed-on-link-between-profit-motive-and-reduction-of-poverty|website = ZENIT - The World Seen From Rome|accessdate = 2015-11-28|language = en-US}}&lt;/ref&gt;.  

The standards body that curates S/E globally is the not-for-profit [http://www.cceg.org.uk Centre for Citizenship, Enterprise and Governance], and the licensing arm is [http://www.seratio.com Seratio]. 

= Theory =
S/E Ratio is a financial metric that converts sentiment into financial value, and purports to be the currency of intangible values&lt;ref&gt;{{Cite journal|url = http://www.csrconference.eu/|title = The Paradox of Intangible Values|last = Hrabětová|first = Jaroslava|date = 11 September 2015|journal = CSR: University Builds Country, Prague|doi = |pmid = |access-date = |last2 = Dohnalová|first2 = Marie|publisher = |issue = ISBN 97880906064033|isbn = |last3 = Ta'eed|first3 = Olinga}}&lt;/ref&gt;.  Its basis is:

&lt;math display=&quot;block&quot;&gt;Total Value = Financial Value + NonFinancial Value = p/e + s/e&lt;/math&gt;[[File:Horizontal (TM).png|thumb|Social Earnings Ratio: Micro to Meso to Macro levels|677x677px]]The S/E utilises financial value data, claimed outcomes and independent verification within a [http://www.brandanomics.com Citizenship Framework]. S/E provides digital articulation of value across micro, meso and macro levels.  The same algorithm is used which allows simple calculations through to sophisticated correlations to be applied to the movement of value like any comparative currency flow. 

The work was developed using a wiki-university collaborative approach involving some 90+ universities globally. The academic framework examines the influences between the value formed at citizen, family, community, team, organisation, regional, national, and global levels.  A principle rule is that S/E is never applied to negative actions. For example, violence is not measured but treated as the absence of peace; slavery is treated as a lack of freedom. 

Due to its universal approach, and positive paradigm, it was labelled by Vatican press as &quot;The God Metric&quot;&lt;ref&gt;{{Cite web|url = http://www.zenit.org/en/articles/the-vatican-has-long-promoted-intangible-values-can-they-be-measured|title = The Vatican Has Long Promoted Intangible Values; Can They Be Measured?|date = 5 January 2015|accessdate = 28 November 2015|website = The World Seen From Rome|publisher = ZENIT|last = Lubov|first = Deborah}}&lt;/ref&gt;
[[File:Round Citizenship Map.png|thumb|Citizenship Framework|362x362px]]

=== Analysis Tools ===
S/E works like a digital non-financial currency, which lends itself to articulate value through deep academic analysis using tools normally reserved for financial value. Through 4 years of research Interdependencies have been discovered and used regularly to map out solution pathways for intangible value:
* Time dependency -value changes over time 
* [https://webbrain.com/u/18uh Direction] - value changes depending on which stakeholder viewpoint is assessed
* Distance - the 1/r&lt;sup&gt;2&lt;/sup&gt;&lt;ref&gt;{{Cite journal|title = Distance Matters: Physical Space and Social Impact|url = http://psp.sagepub.com/content/21/8/795|journal = Personality and Social Psychology Bulletin|date = 1995-08-01|issn = 0146-1672|pages = 795-805|volume = 21|issue = 8|doi = 10.1177/0146167295218002|language = en|first = Bibb|last = Latané|first2 = James H.|last2 = Liu|first3 = Andrzej|last3 = Nowak|first4 = Michael|last4 = Bonevento|first5 = Long|last5 = Zheng}}&lt;/ref&gt; rule - value dilutes over increasing distance
* Multiplier effect - enhancing, diminishing, neutral processes
* Rules regarding creation and total value loss
* Tracking Movement - transfer between stakeholders

== Prodigy Metrics ==
Since 2011 further sibling sentiments have been metricated, each using the S/E algorithm as the base. Quality standards for the adaptation of the S/E and the release of Prodigy Metrics are fully controlled by the [http://www.cceg.org.uk Centre for Citizenship, Enterprise and Governance] who maintain a rigorous approval system.   
{| class=&quot;wikitable&quot;
!
!FOCUS
!PRODIGY METRIC
!APPLICATION
|-
|1
|Institution
|Organisational
|UK Social Value Act 2012, Corporate CSR, NGO efficiency
|-
|2
|Citizen
|[http://www.serat.io Personal Value]
|Recruitment, Social Media, Retail, Therapy
|-
|3
|Slavery
|Freedom
|UK Modern Slaver Act 2015
|-
|4
|Health
|Wellbeing
|Health, Corporate HR
|-
|5
|Regional
|Hyperlocality
|Regional Government, Corporate CSR
|-
|6
|Collaboration
|Team
|Clipper Round the World Yacht Race 2015
|-
|7
|Ethics
|Leadership
|Mission Performance
|-
|8
|Enablement
|Independence
|NHS integrated social and health care
|-
|9
|Inspiration
|Art
|Arts Council
|-
|10
|Impact
|Investment
|Sustainable investment
|-
|11
|[[Intellectual capital|Intellectual]] 
|Capital
|Universities, Corporate UK
|-
|12
|Happiness
|City
|Local Government, Politics 
|-
|13
|Violence
|Peace
|Domestic Violence
|-
|14
|Animal
|Welfare
|Pet industry
|-
|15
|Tax
|Avoidance
|Corporate evasion
|-
|16
|Pay 
|Equalities
|Gender 
|-
|17
|Love
|Relationship
|Online dating
|-
|18
|Retail
|Consumer
|b2c
|}

== Critique and Comparison ==
[[File:Loomba Chart.PNG|thumb|355x355px|Social Impact Analysis by Metric (Lord Loomba) 2014]]
It is not possible to compare Social Earnings Ratio to other metrics, nor often others to each other. Claims that the S/E licencing arm, [http://www.seratio.com Seratio], is the &quot;world leader of measurement of value&quot;&lt;ref&gt;{{Cite web|title = Seratio Brochure|url = https://drive.google.com/a/cceg.org.uk/folderview?id=0B0ZjMfC4YqmvckUweVRIdEtSNkE&amp;usp=sharing#|website = drive.google.com|accessdate = 2015-12-02}}&lt;/ref&gt; can be neither substantiated or disproved as there does not appear to be a #2 in &quot;measuring the movement of non-financial and intangible value&quot;. If one compares only to other social impact analysis tools then [[Social return on investment|Social Return on Investment]] (SROI) claim to be the most popular by brand name is highly likely to be correct; a Google search substantiates this at 156,000&lt;ref&gt;{{Cite web|title = Google|url = https://www.google.co.uk/?gfe_rd=cr&amp;ei=KDpeVrLpE5Hj8wednozQCA#q=%2522social+return+on+investment%2522|website = www.google.co.uk|accessdate = 2015-12-02}}&lt;/ref&gt; pages compared to 571&lt;ref&gt;{{Cite web|title = Google|url = https://www.google.co.uk/?gfe_rd=cr&amp;ei=KDpeVrLpE5Hj8wednozQCA#q=%2522social+earnings+ratio%2522|website = www.google.co.uk|accessdate = 2015-12-02}}&lt;/ref&gt; for S/E. This may, however,be an artefact of SROI's historic release in 2002 compared to S/E in 2011, which according to the [http://www.socialvalueportal.com Social Value Portal] is the latest social impact metric. Equally HACT claims to &quot;have created the largest bank of methodologically consistent and robust social values ever produced&quot;&lt;ref&gt;{{Cite web|title = Social Value Bank {{!}} HACT|url = http://www.hact.org.uk/social-value-bank|website = www.hact.org.uk|accessdate = 2015-12-02}}&lt;/ref&gt; by which they are referring to a database of financial proxy data which can be used in conjunction to SROI, although in August 2015 they have criticised SROI comprehensively in the &quot;Seven Principle Problems of SROI&quot;&lt;ref&gt;{{Cite web|title = Simetrica {{!}} Social Impact Metrics|url = http://www.simetrica.co.uk/#!The-Seven-Principle-Problems-of-SROI/c1jf1/3|website = Simetrica {{!}} Social Impact Metrics|accessdate = 2015-12-02}}&lt;/ref&gt;. Broadening the comparison, in 2014 Lord Loomba presented a report to the House of Lords, &quot;Social Impact Analysis&quot;&lt;ref&gt;Loomba, Lord, Social Impact Analysis: a report for the Loomba Foundation, House of Lords (June 2014)&lt;/ref&gt; , which included metrics from [[Business in the Community]] and [[Benefit corporation|Benefit Corporation]], as well as SROI and S/E. The steep rise in S/E take-up in 2013 represents the automation of the metric and not necessarily any indication of quality. A 'big-data' approach has been provided by [http://www.csrhub.com/ CSR-Hub] who have harvested 417 metrics; in May 2014 they announced an MoU with CCEG to explore S/E&lt;ref&gt;{{Cite web|title = Cross Atlantic Big Data CSR Partnership|url = http://www.csrhub.com/blog/2014/05/cross-atlantic-big-data-csr-partnership.html|website = CSRHub|accessdate = 2015-12-02}}&lt;/ref&gt;.                                        

S/E has undergone considerable analysis and critique in recent years. Most recent comparing [[Integrated reporting|IIRC]] (International Integrated Reporting Council) to S/E&lt;ref&gt;{{Cite journal|url = |title = Core Values for 21st Century Business Success|last = Helps|first = Charlie|date = December 2015|journal = Corporate Report|doi = |pmid = |access-date = |publisher = Juta, South Africa|editor-last = King|editor-first = Mervyn}}&lt;/ref&gt;, concluding &quot;''Together, the &lt;IR&gt; Framework and the S/E Ratio provide a powerful set of metrics and analysis for both founders and funders to evaluate value as well as strategic risk''.&quot; Earlier in the year '''''The Guardian''''' wrote:                                                                                 &lt;blockquote&gt;''&quot;Since 2013, there has certainly been an increase in the number of online measurement tools. But the challenge of calculating social impact goes back to the early 1990s. One measure, called LM3, allows organisations to calculate the local economic impact, while another, called SROI, calculates social return on investment. But both are very complex and are based on lots of assumptions that the people who do the evaluation will have to identify, agree and sign off.&quot; Perhaps we need a simpler tool such as the social earnings ratio, which doesn’t require lots of assumptions and is based on a one-size-fits-all approach.&quot;''&lt;ref&gt;{{Cite web|title = Is measuring social value the key to better public sector commissioning? {{!}} Dan Ebanks|url = http://www.theguardian.com/society/2015/feb/17/measuring-social-value-public-sector-contracts|website = the Guardian|accessdate = 2015-12-02|first = Dan|last = Ebanks}}&lt;/ref&gt;&lt;/blockquote&gt;The focus on the broad context of applications of S/E have drawn sometimes heated debate amongst the international academic and social innovation community in Italy&lt;ref&gt;{{Cite web|title = Svezia: Social Services Days Conference, un modello per le politiche sociali Eurocomunicazione|url = http://www.eurocomunicazione.com/2015/11/svezia-social-services-days-conference-un-modello-per-le-politiche-sociali-europee/|website = www.eurocomunicazione.com|accessdate = 2015-12-02}}&lt;/ref&gt;, Romania&lt;ref&gt;{{Cite web|title = 10 secunde pentru impact social|url = http://www.csrreport.ro/stiri-csr-romania/10-secunde-pentru-impact-social.html|website = The CSR Report|accessdate = 2015-12-02}}&lt;/ref&gt;, Russia&lt;ref&gt;{{Cite web|title = Ватикан давно продвигает нематериальные ценности - Газета Панорама|url = http://pan.md/drugie/Vatikan-davno-prodvigaet-nematerialinie-tsennosti|website = pan.md|accessdate = 2015-12-02}}&lt;/ref&gt; ... through to the United Nations with Nobel Prize laureate Mohammad Yunus&lt;ref&gt;{{Cite web|title = News {{!}} Yunus Centre for Social Business and Health|url = http://www.gcu.ac.uk/yunuscentre/newsevents/news/article.php?id=119808|website = www.gcu.ac.uk|accessdate = 2015-12-02}}&lt;/ref&gt;. These discussions surrounding the concept of a 'universal metric' draw both praise and scepticism. 

== Controversy ==
[[File:NOVEMBER cover-final (4).pdf|thumb|Social Value &amp; Intangibles Review]]
The S/E makes use of Big Data, Social Media and [[Sentiment analysis|Sentiment Analysis]] to automate the algorithm on a [[Software as a service|SaaS]] platform. Results are reported within [http://www.si2000.org 10 seconds].

For an organisation whose financials are regularly available via [[XBRL]] this can be performed without intervention. For the measurement of individuals [http://www.serat.io Personal Value] takes 60 seconds to input and start reporting. In the main criticism of this approach surrounds:
* '''How can S/E capture an accurate reflection of intangible value in such time frames when surrounded by inherent complexity?''' 
* '''It is not morally right to put a financial value on feelings/thoughts/sentiment?''' 
* '''Is Personal Value an instrument for a [[Minority Report (film)|Minority Report]] style scenario - if you can measure value then ultimately can you control it?'''
The debate is still forming as S/E begins to take prominence in the market.  Some S/E exponents embrace the disruptive metric tag arguing it as instrument for change, others debate the moral and potential future implications.  There are a number of platforms discussing and organically developing S/E including a forum&lt;ref&gt;{{Cite web|title = Sector Marketplace|url = http://www.sectormarketplace.com|website = Sector Marketplace|accessdate = 2015-11-29}}&lt;/ref&gt;, a micro-blog&lt;ref&gt;{{Cite web|title = Intangibles Future|url = http://theseratio.tumblr.com|website = theseratio.tumblr.com|accessdate = 2015-11-29}}&lt;/ref&gt;, and an international journal in 8 languages, the ''[http://issuu.com/seratio/docs/social_value_and_intangibles_review_929adaa177aea2 Social Value &amp; Intangibles Review]''&lt;ref&gt;{{Cite web|title = Social Value and Intangibles Review - November 2015|url = http://issuu.com/seratio/docs/social_value_and_intangibles_review_929adaa177aea2|website = Issuu|accessdate = 2015-11-29}}&lt;/ref&gt;.    

== Intellectual Property ==
In 2015 the UK Intellectual Property Office accepted the terms &quot;Social Earnings Ratio&quot;, &quot;S/E Ratio&quot; and &quot;Seratio&quot; as having &quot;acquired a distinctive character as a result of the use made of it&quot;&lt;ref&gt;{{Cite web|title = Intellectual Property Office - search results|url = https://www.ipo.gov.uk/tmownerid/search?domain=1&amp;id=390722&amp;app=0&amp;mark=UK00003092365|date = 2012-08-06|accessdate = 2015-11-29|language = English|first = Concept House|last = Intellectual Property Office}}&lt;/ref&gt; . Full Registration rights have been granted.   This is an important milestone for the Social Earnings Ratio which will allow it to achieve parity to the [[Price–earnings ratio|Price Earnings Ratio]]. .

= Key Applications =
S/E Ratio can be applied across all organisational, institutional, and personal levels, as well as projects, processes, products in private, public, third (NGO, voluntary, civil society), and community sectors.  This wide spread approach is a key factor in its adoption in certain high profile applications which have gained it traction. 

== Corporate Social Responsibility  ==
[[File:E Harvest.PNG|thumb|306x306px|S/E Organisational Benchmark (2014)]]
There is a propensity of non-financial value data covered by over 1000 different metrics and measures (1153 recorded in a 250 page report released in September 2014)&lt;ref&gt;{{Cite web|title = Social Value in Public Procurement|url = https://drive.google.com/a/cceg.org.uk/folderview?id=0B51i5nTXo9qtV1ROTDk5QWR5cUk&amp;usp=sharing#|accessdate = 2015-11-29|publisher = Centre for Citizenship, Enterprise and Governance|editor-first = Olinga|editor-last = Taeed|date = September 2014|pages = 250}}&lt;/ref&gt;. There is remarkable lack of consistency between them. Due to the speed of S/E and it's ability to translate empirical data from other metrics to an S/E score, it has harvested a significant database of organisational values reported. Ther EU SEiSMiC Social Value Group&lt;ref&gt;{{Cite web|title = EU SEiSMiC Social Value - Infogram, charts &amp; infographics|url = https://infogr.am/eu_seismic_social_value|website = infogr.am|accessdate = 2015-11-29}}&lt;/ref&gt; have charted growth and their 2015-16 forecasts for measured asset value (Euro € trillion) by S/E. Figures for 2015 and 2016 include other value measurements outside organisational value, the key differentiator presumably is Personal Value. 

&lt;graph&gt;{
	&quot;width&quot;: 400,
	&quot;height&quot;: 200,
	&quot;data&quot;: [
		{
			&quot;name&quot;: &quot;table&quot;,
			&quot;values&quot;: [
				{
					&quot;x&quot;: 2011,
					&quot;y&quot;: 0
				},
				{
					&quot;x&quot;: 2012,
					&quot;y&quot;: 400
				},
				{
					&quot;x&quot;: 2013,
					&quot;y&quot;: 1800
				},
				{
					&quot;x&quot;: 2014,
					&quot;y&quot;: 4000
				},
				{
					&quot;x&quot;: 2015,
					&quot;y&quot;: 6600
				},
				{
					&quot;x&quot;: 2016,
					&quot;y&quot;: 9700
				}
			]
		}
	],
	&quot;scales&quot;: [
		{
			&quot;name&quot;: &quot;x&quot;,
			&quot;type&quot;: &quot;ordinal&quot;,
			&quot;range&quot;: &quot;width&quot;,
			&quot;zero&quot;: false,
			&quot;domain&quot;: {
				&quot;data&quot;: &quot;table&quot;,
				&quot;field&quot;: &quot;data.x&quot;
			}
		},
		{
			&quot;name&quot;: &quot;y&quot;,
			&quot;type&quot;: &quot;linear&quot;,
			&quot;range&quot;: &quot;height&quot;,
			&quot;nice&quot;: true,
			&quot;domain&quot;: {
				&quot;data&quot;: &quot;table&quot;,
				&quot;field&quot;: &quot;data.y&quot;
			}
		}
	],
	&quot;axes&quot;: [
		{
			&quot;type&quot;: &quot;x&quot;,
			&quot;scale&quot;: &quot;x&quot;
		},
		{
			&quot;type&quot;: &quot;y&quot;,
			&quot;scale&quot;: &quot;y&quot;
		}
	],
	&quot;marks&quot;: [
		{
			&quot;type&quot;: &quot;rect&quot;,
			&quot;from&quot;: {
				&quot;data&quot;: &quot;table&quot;
			},
			&quot;properties&quot;: {
				&quot;enter&quot;: {
					&quot;x&quot;: {
						&quot;scale&quot;: &quot;x&quot;,
						&quot;field&quot;: &quot;data.x&quot;
					},
					&quot;y&quot;: {
						&quot;scale&quot;: &quot;y&quot;,
						&quot;field&quot;: &quot;data.y&quot;
					},
					&quot;y2&quot;: {
						&quot;scale&quot;: &quot;y&quot;,
						&quot;value&quot;: 0
					},
					&quot;fill&quot;: {
						&quot;value&quot;: &quot;steelblue&quot;
					},
					&quot;width&quot;: {
						&quot;scale&quot;: &quot;x&quot;,
						&quot;band&quot;: true,
						&quot;offset&quot;: -1
					}
				}
			}
		}
	]
}&lt;/graph&gt;

== Social Value Act 2012 ==
S/E Ratio is the leading provider of metrics for delivery of public sector procurement under the UK Social Value Act 2012 legislation&lt;ref&gt;{{Cite journal|url = |title = Social Value in UK Public Procurement Longitudinal Study of West Midlands Fire Service|last = Daly|first = John|date = March 2016|journal = 25th IPSERA Conference|doi = |pmid = |access-date = |publisher = |location = Dortmund|last2 = Brook|first2 = Ben|last3 = McCabe|first3 = Steve|last4 = Taeed|first4 = Olinga}}&lt;/ref&gt;. Currently some UK£ 3.15 billion of public sector procurement is measured through the Social Earnings Ratio. The Lord Young Review (February 2015) of the Social Value Act described S/E as &quot;  &lt;blockquote&gt;''&quot;The Centre for Citizenship, Enterprise and'' &lt;ref&gt;{{Cite web|title = Ben Television {{!}} Sky 182, Breaking, World, Business, Sports, Entertainment and Video News   » Modern Slavery Bill becomes law:  Inaugural Conference Monday 30th March|url = http://bentelevision.com/modern-slavery-bill-becomes-law-inaugural-conference-monday-30th-march/|website = bentelevision.com|accessdate = 2015-12-02}}&lt;/ref&gt;''Governance (CCEG) has developed the social earnings ratio as a quick, low cost, high volume way to assess social impact. It is calculated by dividing the social value by the money spent on it. This can be calculated using very simple information (e.g. the CSR budget, the carbon reduction, and the number of people helped), and is meant to provide a single metric that can be used as a quick benchmark&quot;''&lt;ref&gt;{{Cite book|title = Social Value Act Review|last = Young|first = Lord|publisher = Cabinet Office|year = February 2015|isbn = |location = London|pages = 42}}&lt;/ref&gt;&lt;/blockquote&gt;

== Personal [http://www.serat.io Value] ==
Pre-launched at the [[Clipper Round the World Yacht Race]] in September 2015, PV targets 1 million users. It is a campaign backed by celebrities such as Former First Lady [[Cherie Blair]], broadcaster [[Jonathan Dimbleby]], Italian footballer [[Gianluigi Buffon]], politician Rt Hon [[Peter Hain]], as well as the [[Desmond Tutu]] Foundation. Personal Value will be formally launched in December 2015.  

== Modern Slavery Act 2015 ==
[http://www.seratio.com Seratio], the SaaS licensing arm of CCEG announced that in Q1 2016 it will launch a Freedom metric to support the UK Modern Slavery Act 2015. This promotes Transparency in the Supply Chain for all companies over UK£ 36 million turnover - an estimated 12,000 companies need to comply to this new piece of legislation as from October 2015. CCEG chairs the [https://procurement-forum.eu/ EU PROCUREMENT] Social Value &amp; Transparency in Supply Chain Forum.   

= Supporting Infrastructure =
The Social Earnings Ratio was created in 2011. During 2012-14 it underwent 3 years of development at and testing for sustainability and scalability a the [[University of Northampton]] Business School alongside other universities including [[Birmingham City University]], [[Aston University]], and others globally in a wiki-university approach. For this reason the metric remains neutral benefiting from the non-partisan, agnostic, no-agenda roles of universities.       

== History ==      

The process of adoption accelerated with the formation of a dedicated research centre around the Social Earnings Ratio and subsequent key events.      
{| class=&quot;wikitable&quot;
!
!EVENT
|-
|Nov 2011
|commission to develop single number metric of social value
|-
|Apr 2013
|launch [http://www.cceg.org.uk Centre for Citizenship, Enterprise and Governance]
|-
|Jun 2013
|[http://www.northampton.ac.uk/news/university-of-northampton-professor-urges-individuals-worth-100-billion-collectively-to-consider-global-enterprise Dorchester conference] with billionaires Lakshmi Mittal and Hinduja Brothers
|-
|Oct 2013
|[http://www.northampton.ac.uk/news/university-of-northampton-collaborates-with-the-loomba-foundation-to-empower-245-million-widows London Guildhall] gala with DPM Nick Clegg, Cherie Blair, Treasury Minister Danny Alexander
|-
|May 2014
|Annual [http://www.northampton.ac.uk/news/university-of-northampton-professor-receives-unprecedented-feedback-for-inspirational-speech-at-institute-of-financial-services-event Institute of Financial Service] Lecture
|-
|Jun 2014
|100 Indian CEO's attend [http://www.northampton.ac.uk/news/professor-olinga-ta-eed-hosts-united-nations-international-widows-day-at-the-house-of-lords House of Lords]
|-
|Jun 2014
|Speech at [https://www.youtube.com/watch?feature=youtube_gdata&amp;v=nWgCCbOnYLY&amp;app=desktop The Vatican] 
|-
|Oct 2014
|National [https://www.youtube.com/watch?v=WAyPxiWbOoc video] [https://www.youtube.com/watch?v=lbw_GKy8cow&amp;feature=youtu.be interviews]
|-
|Nov 2014
|deliver report to [https://drive.google.com/a/cceg.org.uk/folderview?id=0B51i5nTXo9qtV1ROTDk5QWR5cUk&amp;usp=sharing# Bombay Stock Exchange]
|-
|Nov 2014
|Question Time with [http://www.theinformationdaily.com/2014/11/19/answertime01 Hazel Blears MP]
|-
|Jan 2015
|university spin-out [http://www.seratio.com Seratio] created
|-
|Feb 2015
|[https://www.incae.edu/es/en/illuminate/leading-change-in-the-region-the-incae-presidents-club.php INCAE keynote speech] Guatemala
|-
|Jun 2015
|keynote to United Nations PRiME, Glasgow
|-
|Sep 2015
|constituted CCEG as a not-for-profit entity 
|-
|Oct 2015
|keynote speech [https://www.youtube.com/watch?v=0Mpe9bBvoII&amp;feature=youtu.be Socialchefsdagama] 2015, Sweden
|-
|Oct 2015
|global partnership with [[Chartered Institute of Procurement &amp; Supply|CIPS]]
|}

== Capacity Development ==
To educate the marketplace the Centre for Citizenship, Enterprise and Governance (CCEG) offers several instruments:
# [https://prezi.com/8hf9kanbvobx/social-value-and-intangibles/?utm_campaign=share&amp;utm_medium=copy Advisory services to governments]
# International conferences&lt;ref&gt;{{Cite web|title = SOCIAL VALUE AND PROCUREMENT: Transparency in Supply Chains|url = https://www.picatic.com/event14260502730168494|website = Picatic|accessdate = 2015-11-29}}&lt;/ref&gt;
# [[Massive open online course|MOOC]] learning site&lt;ref&gt;{{Cite web|title = CCEG {{!}} Knowledge Cloud|url = http://www.seratio.org|website = www.seratio.org|accessdate = 2015-11-29}}&lt;/ref&gt;

== People ==
The Board of Centre for Citizenship, Enterprise and Governance includes [[Nick Petford|Professor Nick Petford]] (Vice Chancellor of University of Northampton), and [https://uk.linkedin.com/in/barbara-mellish-a3671a16 Barbara Mellish] (CEO of Seratio). CCEG currently has 28,541 members&lt;ref&gt;{{Cite web|title = CCEG Nov 2015: Newsletter|url = http://us4.campaign-archive1.com/?u=d75ffce5c8e276e11b76c6663&amp;id=7ee2ea1f45&amp;e=e9025ffb98|website = us4.campaign-archive1.com|accessdate = 2015-11-29}}&lt;/ref&gt;, including 3,832 heads of CSR/Sustainability from the world's largest companies (as at 29 November 2015). The Board of Seratio includes [https://uk.linkedin.com/in/karen-bryson-42538316 Karen Bryson]. Seratio currently has over 50 full and part-time staff (29 November 2015).        

= References =

{{AFC submission|||ts=20151202032239|u=Social Earnings Ratio|ns=118}}</text>
      <sha1>s9nsd69zdmx6dek9mnz2jinyt5bw67h</sha1>
    </revision>
  </page>
  <page>
    <title>Partial likelihood methods for panel data</title>
    <ns>0</ns>
    <id>48673786</id>
    <revision>
      <id>692842052</id>
      <parentid>692841977</parentid>
      <timestamp>2015-11-28T18:26:58Z</timestamp>
      <contributor>
        <username>Qwertyus</username>
        <id>196471</id>
      </contributor>
      <minor />
      <comment>Qwertyus moved page [[Partial Likelihood Methods for Panel Data]] to [[Partial likelihood methods for panel data]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve">{{Multiple issues|{{one source|date=November 2015}}{{sections|date=November 2015}}}}

Partial (pooled) likelihood estimation for [[panel data]] assumes that density of ''y&lt;sub&gt;it&lt;/sub&gt;'' given ''x&lt;sub&gt;it&lt;/sub&gt;'' is correctly specified for each time period but it allows for misspecification in the conditional density of ''y&lt;sub&gt;i&lt;/sub&gt;≔(y&lt;sub&gt;i1&lt;/sub&gt;,…,y&lt;sub&gt;iT&lt;/sub&gt;) given x&lt;sub&gt;i&lt;/sub&gt;≔(x&lt;sub&gt;i1&lt;/sub&gt;,…,x&lt;sub&gt;iT&lt;/sub&gt;)''. Concretely, partial likelihood estimation uses the product of conditional densities as the density of the joint conditional distribution. This generality facilitates [[maximum likelihood]] methods in panel data setting because fully specifying conditional distribution of ''y&lt;sub&gt;i&lt;/sub&gt;'' can be computationally demanding &lt;ref name= &quot;Woolridge&quot;&gt;Wooldridge, J.M., Econometric Analysis of Cross Section and Panel Data, MIT Press, Cambridge, Mass.&lt;/ref&gt;. On the other hand, allowing for misspecification generally results in violation of information equality and thus requires robust [[standard error estimator]] for inference.

In the following exposition, we follow the treatment in Wooldridge &lt;ref name= &quot;Woolridge&quot; /&gt;. Particularly, the asymptotic derivation is done under fixed-T, growing-N setting.

Writing the conditional density of y&lt;sub&gt;it&lt;/sub&gt; given ''x&lt;sub&gt;it&lt;/sub&gt;'' as ''f&lt;sub&gt;t&lt;/sub&gt;'' (''y&lt;sub&gt;it&lt;/sub&gt;'' | ''x&lt;sub&gt;it&lt;/sub&gt;'';θ), the partial maximum likelihood estimator solves:

: &lt;math&gt;
  \underset{\theta\in\Theta}{\operatorname{max}}\sum_{i=1}^N\sum_{t=1}^T \log f_t(y_{it} \mid x_{it}; \theta) &lt;/math&gt;


In this formulation, the joint conditional density of ''y&lt;sub&gt;i&lt;/sub&gt;'' given ''x&lt;sub&gt;i&lt;/sub&gt;'' is modeled as ''Π&lt;sub&gt;t&lt;/sub&gt;'' ''f&lt;sub&gt;t&lt;/sub&gt;'' (''y&lt;sub&gt;it&lt;/sub&gt;'' | ''x&lt;sub&gt;it&lt;/sub&gt;'' ; θ). We assume that ''f&lt;sub&gt;t&lt;/sub&gt; (y&lt;sub&gt;it&lt;/sub&gt; |x&lt;sub&gt;it&lt;/sub&gt; ; θ)'' is correctly specified for each ''t'' = 1,…,''T'' and that there exists ''θ&lt;sub&gt;0&lt;/sub&gt;'' ∈ Θ that uniquely maximizes ''E[f&lt;sub&gt;t&lt;/sub&gt; (y&lt;sub&gt;it&lt;/sub&gt;│x&lt;sub&gt;it&lt;/sub&gt; ; θ)].  
But, it is not assumed that the joint conditional density is correctly specified. Under some regularity conditions, partial MLE is consistent and asymptotically normal. 

By the usual argument for M-estimator (details in Wooldridge &lt;ref name= &quot;Woolridge&quot; /&gt;), the asymptotic variance of ''√N  (θ&lt;sub&gt;MLE&lt;/sub&gt;- θ&lt;sub&gt;0&lt;/sub&gt;)  is A&lt;sup&gt;-1&lt;/sup&gt; BA&lt;sup&gt;-1&lt;/sup&gt;'' where ''A&lt;sup&gt;-1&lt;/sup&gt; = E[ ∑&lt;sub&gt;t&lt;/sub&gt;∇&lt;sup&gt;2&lt;/sup&gt;&lt;sub&gt;θ&lt;/sub&gt;  logf&lt;sub&gt;t&lt;/sub&gt; (y&lt;sub&gt;it&lt;/sub&gt;│x&lt;sub&gt;it&lt;/sub&gt; ; θ)]&lt;sup&gt;-1&lt;/sup&gt; and B=E[( ∑&lt;sub&gt;t&lt;/sub&gt;∇&lt;sub&gt;θ&lt;/sub&gt; logf&lt;sub&gt;t&lt;/sub&gt; (y&lt;sub&gt;it&lt;/sub&gt;│x&lt;sub&gt;it&lt;/sub&gt; ; θ) ) ( ∑&lt;sub&gt;t&lt;/sub&gt;∇&lt;sub&gt;θ&lt;/sub&gt;  logf&lt;sub&gt;t&lt;/sub&gt; (y&lt;sub&gt;it&lt;/sub&gt;│x&lt;sub&gt;it&lt;/sub&gt;; θ ) )&lt;sup&gt;T&lt;/sup&gt;]''.  If the joint conditional density of y&lt;sub&gt;i&lt;/sub&gt; given x&lt;sub&gt;i&lt;/sub&gt; is correctly specified, the above formula for asymptotic variance simplifies because information equality says ''B=A''. Yet, except for special circumstances, the [[joint density]] modeled by partial MLE is not correct.  Therefore, for valid inference, the above formula for asymptotic variance should be used. For information equality to hold, one sufficient condition is that scores of the densities for each time period are uncorrelated.  In dynamically complete models, the condition holds and thus simplified asymptotic variance is valid &lt;ref name= &quot;Woolridge&quot; /&gt;.

{{Reflist}}

{{uncategorised|date=November 2015}}</text>
      <sha1>833k8pozjt6y9ohgqcy03okknrcvwke</sha1>
    </revision>
  </page>
  <page>
    <title>Two-step M-estimators involving MLE</title>
    <ns>0</ns>
    <id>48673808</id>
    <revision>
      <id>692842648</id>
      <parentid>692842608</parentid>
      <timestamp>2015-11-28T18:32:40Z</timestamp>
      <contributor>
        <username>Qwertyus</username>
        <id>196471</id>
      </contributor>
      <comment>added [[Category:M-estimators]]; removed {{uncategorized}} using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve">{{Multiple issues|{{more footnotes|date=November 2015}}{{sections|date=November 2015}}}}

Two-step M-estimator involving [[Maximum Likelihood Estimator]] is a special case of general two-step M-estimator. Thus, [[consistency]] and [[asymptotic normality]] of the estimator follows from the general result on two-step M-estimators.  Yet, when the first step estimation is MLE, under some assumptions, two-step M-estimator is more efficient [i.e. has smaller asymptotic variance] than M-estimator with known first-step parameter &lt;ref name= &quot;Woolridge&quot;&gt;Wooldridge, J.M., Econometric Analysis of Cross Section and Panel Data, MIT Press, Cambridge, Mass.&lt;/ref&gt;

Let {V&lt;sub&gt;i&lt;/sub&gt;,W&lt;sub&gt;i&lt;/sub&gt;,Z&lt;sub&gt;i&lt;/sub&gt;}&lt;sub&gt;i=1&lt;/sub&gt;&lt;sup&gt;n&lt;/sup&gt; be a random sample and the second-step M-estimator &lt;math&gt;\widehat{\theta}&lt;/math&gt; is the following:

&lt;math&gt;\widehat{\theta}&lt;/math&gt; ≔ &lt;math&gt;
  \underset{\theta\in\Theta}{\operatorname{arg\max}}\sum_{i}m(\,v_i,w_i,z_i: \theta\,,\widehat{\gamma }) &lt;/math&gt;

where  &lt;math&gt; \widehat{\gamma } &lt;/math&gt; is the parameter estimated by ML procedure in the first step.  For the MLE,

&lt;math&gt;\widehat{\gamma }&lt;/math&gt; ≔ &lt;math&gt;
  \underset{\gamma\in\Gamma}{\operatorname{arg\max}}\sum_{i}\log f(v_{it} : z_{i} , \gamma)  &lt;/math&gt; 

where ''f'' is the conditional density of ''V'' given ''Z''.  Now, suppose that given ''Z, V'' is conditionally independent of ''W''. This assumption is called [[conditional independence assumption]] or selection on observables &lt;ref&gt;Heckman, J.J., and R. Robb, 1985, Alternative Methods for Evaluating the Impact of Interventions: An Overview, Journal of Econometrics, 30, 239-267.&lt;/ref&gt; &lt;ref name= &quot;Woolridge&quot; /&gt;. Intuitively, this condition means that Z is a good predictor of V so that once conditioned on ''Z, V'' has no systematic dependence on ''W''. Under the conditional independence assumption, the [[asymptotic variance]] of the two-step estimator is:

''E[∇&lt;sub&gt;θ&lt;/sub&gt;  s(θ&lt;sub&gt;0&lt;/sub&gt;,γ&lt;sub&gt;0&lt;/sub&gt;)]&lt;sup&gt;-1&lt;/sup&gt;  E[g(θ&lt;sub&gt;0&lt;/sub&gt;,γ&lt;sub&gt;0&lt;/sub&gt; )g(θ&lt;sub&gt;0&lt;/sub&gt;,γ&lt;sub&gt;0&lt;/sub&gt; )']E[∇&lt;sub&gt;θ&lt;/sub&gt;  s(θ&lt;sub&gt;0&lt;/sub&gt;,γ&lt;sub&gt;0&lt;/sub&gt;)]&lt;sup&gt;-1&lt;/sup&gt;''

where ''g(θ,γ) ≔ s(θ,γ)-E[ s(θ , γ) ∇&lt;sub&gt;γ&lt;/sub&gt; d(γ)&lt;sup&gt;'&lt;/sup&gt; ]E[∇&lt;sub&gt;γ&lt;/sub&gt; d(γ) ∇&lt;sub&gt;γ&lt;/sub&gt; d(γ)&lt;sup&gt;'&lt;/sup&gt; ]&lt;sup&gt;-1&lt;/sup&gt; d(γ),

s(θ,γ) ≔ ∇&lt;sub&gt;θ&lt;/sub&gt; m(V, W, Z: θ, γ) ,  d(γ) ≔ ∇&lt;sub&gt;γ&lt;/sub&gt;  log f (V : Z, γ)'', 
and ∇ represents partial derivative with respect to a row vector. In the case where ''γ&lt;sub&gt;0&lt;/sub&gt;'' is known, the asymptotic variance is 
''E[∇&lt;sub&gt;θ&lt;/sub&gt;  s(θ&lt;sub&gt;0&lt;/sub&gt;,γ&lt;sub&gt;0&lt;/sub&gt;)]&lt;sup&gt;-1&lt;/sup&gt;  E[s(θ&lt;sub&gt;0&lt;/sub&gt;,γ&lt;sub&gt;0&lt;/sub&gt; )s(θ&lt;sub&gt;0&lt;/sub&gt;,γ&lt;sub&gt;0&lt;/sub&gt; )&lt;sup&gt;'&lt;/sup&gt;]E[∇&lt;sub&gt;θ&lt;/sub&gt;  s(θ&lt;sub&gt;0&lt;/sub&gt;,γ&lt;sub&gt;0&lt;/sub&gt;)]&lt;sup&gt;-1&lt;/sup&gt;   and therefore, unless E[ s(θ, γ) ∇&lt;sub&gt;γ&lt;/sub&gt; d(γ)&lt;sup&gt;'&lt;/sup&gt; ]=0'', the two-step M-estimator is more efficient than the usual M-estimator.  This fact suggests that even when ''γ&lt;sub&gt;0&lt;/sub&gt;'' is known a priori, there is efficiency gain by estimating ''γ'' by MLE. An application of this result can be found, for example, in treatment effect estimation  &lt;ref name= &quot;Woolridge&quot; /&gt;.



{{Reflist}}



[[Category:M-estimators]]</text>
      <sha1>58lwriz75v2hpv6xb3drwtbn1yl8o6x</sha1>
    </revision>
  </page>
  <page>
    <title>Fixed effect Poisson model</title>
    <ns>0</ns>
    <id>48673854</id>
    <revision>
      <id>693451497</id>
      <parentid>692839108</parentid>
      <timestamp>2015-12-02T17:39:52Z</timestamp>
      <contributor>
        <username>Postcard Cathy</username>
        <id>1744116</id>
      </contributor>
      <comment>added [[Category:Econometrics]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve">Hausman, Hall, and Griliches pioneered work in [[Fixed Effect]] [[Poisson models]] for [[static panel data]] in the mid 1980s. Their outcome of interest was the number of patents filed by firms, where they wanted to develop methods to control for the firm fixed effects &lt;ref&gt;Hausman, J. A., B. H. Hall, and Z. Griliches (1984): Econometric Models for Count Data with an Application to the Patents-R&amp;D Relationship. Econometrica (46), pp. 909-938&lt;/ref&gt;.  Linear panel data models use the linear additivity of the fixed effects to difference them out and circumvent the [[incidental parameter problem]]. Even though Poisson models are inherently nonlinear, the use of the linear index and the exponential link function lead to multiplicative [[separability]], more specifically &lt;ref&gt;Cameron, C. A. and P. K. Trivedi (2015) Count Panel Data, Oxford Handbook of Panel Data, ed. by B. Baltagi, Oxford University Press, pp. 233-256&lt;/ref&gt;

''E[y&lt;sub&gt;it&lt;/sub&gt; ∨ x&lt;sub&gt;i1&lt;/sub&gt;... x&lt;sub&gt;iT&lt;/sub&gt;, c&lt;sub&gt;i&lt;/sub&gt; ]= m(x&lt;sub&gt;it&lt;/sub&gt;, c&lt;sub&gt;i&lt;/sub&gt;, b&lt;sub&gt;0&lt;/sub&gt; ) = exp(c&lt;sub&gt;i&lt;/sub&gt;+ x&lt;sub&gt;it&lt;/sub&gt; b&lt;sub&gt;0&lt;/sub&gt; )= a&lt;sub&gt;i&lt;/sub&gt; exp(x&lt;sub&gt;it&lt;/sub&gt; b&lt;sub&gt;0&lt;/sub&gt; )= μ&lt;sub&gt;ti&lt;/sub&gt;''				(1)

This formula looks very similar to the standard Poisson premultiplied by the term ''a&lt;sub&gt;i&lt;/sub&gt;''. As the conditioning set includes the observables over all periods, we are in the static panel data world and are imposing [[strict exogeneity]]&lt;ref&gt;Wooldridge, J. (2002): Econometric Analysis of Cross Section and Panel Data, MIT Press, Cambridge, Mass.&lt;/ref&gt;.  Hausman, Hall, and Griliches then use Andersen's conditional Maximum Likelihood methodology to estimate ''b&lt;sub&gt;0&lt;/sub&gt;''. Using ''n&lt;sub&gt;i&lt;/sub&gt;=∑ y&lt;sub&gt;it&lt;/sub&gt;'' allows them to obtain the following nice distributional result of  ''y&lt;sub&gt;i&lt;/sub&gt;''

''y&lt;sub&gt;i&lt;/sub&gt; ∨ n&lt;sub&gt;i&lt;/sub&gt;, x&lt;sub&gt;i&lt;/sub&gt;, c&lt;sub&gt;i&lt;/sub&gt; ∼ Multinomial (n&lt;sub&gt;i&lt;/sub&gt;, p&lt;sub&gt;1&lt;/sub&gt; (x&lt;sub&gt;i&lt;/sub&gt;, b&lt;sub&gt;0&lt;/sub&gt;),...,p&lt;sub&gt;T&lt;/sub&gt; (x&lt;sub&gt;i&lt;/sub&gt;, b&lt;sub&gt;0&lt;/sub&gt; ))''						(2) where


&lt;math&gt; p_{t}(x_{i}, b_{0})  =  \frac{m(x_{it}, b_{0})}{\sum m(x_{it}, b_{0})} &lt;/math&gt;. 

&lt;ref&gt;Andersen, E. B. (1970): Asymptotic Properties of Conditonal Maximum Likelihood Estimators. Journal of the Royal Statistical Society, Series B, 32, pp. 283-301&lt;/ref&gt;

At this point, the estimation of the Fixed Effects Poisson model is transformed in a useful way and can be estimated by Maximum Likelihood estimation techniques for [[multinomial]] log likelihoods. This is computationally not necessarily very restrictive, but the [[distributional]] assumptions up to this point are fairly stringent. Wooldridge provided evidence that these models have nice robustness properties as long as the conditional mean assumption (i.e. equation 1) holds. &lt;ref&gt;Wooldridge, J. M. (1999): Distribution-Free Estimation of Some Nonlinear Panel Data Models. Journal of Econometrics (90), pp. 77-97&lt;/ref&gt;  Chamberlain also provided [[semi-parametric efficiency bounds]] for these estimators under slightly weaker exogeneity assumptions. However, these bounds are practically difficult to attain, as the proposed methodology needs [[high-dimensional]] [[nonparametric regressions]] for attaining these bounds. 


{{Reflist}}

[[Category:Econometrics]]</text>
      <sha1>fqo6yxr2thvfy1j16dv453562bsf05w</sha1>
    </revision>
  </page>
  <page>
    <title>Cross-sectional and panel fractional models</title>
    <ns>0</ns>
    <id>48673866</id>
    <revision>
      <id>692843412</id>
      <parentid>692843079</parentid>
      <timestamp>2015-11-28T18:39:32Z</timestamp>
      <contributor>
        <username>Qwertyus</username>
        <id>196471</id>
      </contributor>
      <comment>copyedit</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve">{{sections|date=November 2015}}

Fractional models are, to some extent, related to [[binary response models]]. However, instead of estimating the probability of being in one bin of a dichotomous variable, the fractional model typically deals with variables that take on all possible values in the [[unit interval]]. One can easily generalize this model to take on values on any other interval by appropriate transformations.&lt;ref&gt;Wooldridge, J. (2002): Econometric Analysis of Cross Section and Panel Data, MIT Press, Cambridge, Mass.&lt;/ref&gt;  Examples range from participation rates in [[401(k)]] plans &lt;ref&gt;Papke, L.E. and J. M. Wooldridge (1996): Econometric Methods for Fractional Response Variables with an Aplication to 401(k) Plan Participation Rates. Journal of Applied Econometrics (11), pp. 619-632&lt;/ref&gt;  to [[television]] ratings of [[NBA]] games. &lt;ref&gt;Hausman, J.A. and G.K. Leonard (1997): Superstars in the National Basketball Association: Economic Value and Policy. Journal of Labor Economics (15), pp. 586-624&lt;/ref&gt;

There have been two approaches to modeling this problem. Even though they both rely on an [[index]] that is linear in {{mvar|x&lt;sub&gt;i&lt;/sub&gt;}} combined with a [[link function]]&lt;ref&gt;McCullagh, P. and J. A. Nelder (1989): Generalized Linear Models, CRC Monographs on Statistics and Applied Probability (Book 37), 2nd Edition, Chapman and Hall, London.&lt;/ref&gt;, this is not strictly necessary.  The first approach uses a [[log-odds]] transformation of {{mvar|y}} as a linear function of {{mvar|x&lt;sub&gt;i&lt;/sub&gt;}}, i.e., &lt;math&gt;\log \frac {y}{1-y} = x\beta&lt;/math&gt;. This approach is problematic for two distinct reasons. The {{mvar|y}} variable can not take on boundary values 1 and 0, and the interpretation of the coefficients is not straightforward. The second approach circumvents these issues by using the logistic regression as a link function. More specifically,

:&lt;math&gt;E[y \lor x] =  \frac {exp(x\beta)}{1+exp(x\beta)}&lt;/math&gt;

It immediately becomes clear that this set up is very similar to the [[binary logit model]], with that difference that the {{mvar|y}} variable can actually take on values in the unit interval. Many of the [[estimation]] techniques for the binary logit model, such as [[non-linear least squares]] and [[quasi-MLE]], carry over in a natural way, just like [[heteroskedasticity]] adjustments and [[partial effects]] calculations.&lt;ref&gt;Wooldridge, J. (2002): Econometric Analysis of Cross Section and Panel Data, MIT Press, Cambridge, Mass.&lt;/ref&gt;

Extensions to this [[cross-sectional]] model have been provided that allow for taking into account important econometric issues, such as endogenous explanatory variables and unobserved heterogeneous effects. Under [[strict exogeneity]] assumptions, it is possible to difference out these unobserved effects using [[panel data]] techniques, although weaker exogeneity assumptions can also result in consistent estimators.&lt;ref&gt;Papke, L.E. and J. M. Wooldridge (1996): Panel Data Methods for Fractional Response Variables with an Application to Test Pass Rates. Journal of Econometrics (145), pp. 121-133&lt;/ref&gt;  [[Two step]] [[control function]] techniques to deal with endogeneity concerns have also been proposed.&lt;ref&gt;Wooldridge, J.M. (2005): Unobserved heterogeneity and estimation of average partial effects. Identification and Inference for Econometric Models: Essays in Honor of Thomas Rothenberg, ed. by Andrews, D.W.K. and J.H. Stock,  Cambridge University Press, Cambridge, pp. 27-55&lt;/ref&gt;

==References==
{{Reflist}}

[[Category:Cross-sectional analysis]]
[[Category:Panel data]]</text>
      <sha1>sx61dsxmwky2p9hfl8dsx3ui9z6yu6a</sha1>
    </revision>
  </page>
  <page>
    <title>Maximum likelihood estimation with flow data</title>
    <ns>0</ns>
    <id>48673870</id>
    <revision>
      <id>692841788</id>
      <parentid>692841717</parentid>
      <timestamp>2015-11-28T18:24:38Z</timestamp>
      <contributor>
        <username>Qwertyus</username>
        <id>196471</id>
      </contributor>
      <comment>refs section</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve">{{sections|date=November 2015}}

[[Maximum Likelihood Estimation]] with flow data is a [[parametric]] approach to deal with [[flow sampling]] data. Assume that we have observations of ''a&lt;sub&gt;i&lt;/sub&gt;'' the time a person enters the state of interest, some observables x&lt;sub&gt;i&lt;/sub&gt;, and the [[censoring]] of the flow data takes on a particular form. In particular &lt;math&gt;t_i = \min(t_i^U,L) &lt;/math&gt;, where {{mvar|t&lt;sub&gt;i&lt;/sub&gt;}} is the observed duration outcome, &lt;math&gt; t_i^U &lt;/math&gt; is the underlying [[continuous variable]] and {{mvar|L}} is the censoring threshold.&lt;ref&gt; Wooldridge, J. (2002): Econometric Analysis of Cross Section and Panel Data, MIT Press, Cambridge, Mass.&lt;/ref&gt;  For instance, when thinking about [[unemployment spells]], {{mvar|a&lt;sub&gt;i&lt;/sub&gt;}} is the data of entering unemployment, {{mvar|x&lt;sub&gt;i&lt;/sub&gt;}} is a vector of worker characteristics, and {{mvar|t&lt;sub&gt;i&lt;/sub&gt;}} is the observed unemployment duration. If we only follow the workers for a certain period of time, this variable is necessarily a censored version of the true unemployment duration. 

Two key assumptions allow for setting up the [[loglikelihood]]. First, a [[distributional]] form for the latent variable &lt;math&gt;t_i^U&lt;/math&gt; needs to be assumed. Second, [[independence]] between the true duration and the starting point of the spell is assumed, i.e.,

:&lt;math&gt;F(t_i^U \lor x_i, a_i, L) = F(t_i^U \lor x_i)&lt;/math&gt;

where ''F'' is the conditional distribution of the underlying duration variable&lt;ref&gt;Wooldridge, J. (2002): Econometric Analysis of Cross Section and Panel Data, MIT Press, Cambridge, Mass.&lt;/ref&gt;.  This latter assumption allows us to model the probability that the variable is censored, i.e.,

:&lt;math&gt;\Pr(t_i^U \ge L \lor x_i) = 1 - F(L \lor x_i)&lt;/math&gt;

which leads to the following [[log likelihood]]:

:&lt;math&gt; \sum_{i=1}^n [ d_i log(f(t_i \lor x_i)) + (1-d_i) log(1-F(L \lor x_i )) ]&lt;/math&gt;

where ''f'' is the density associated with the distribution {{mvar|F}} and {{mvar|d&lt;sub&gt;i&lt;/sub&gt;}} is an indicator denoting whether {{math|''t&lt;sub&gt;i&lt;/sub&gt;'' {{=}} ''L''}}.&lt;ref&gt;Hayashi, F. (2000): Econometrics. Princeton University Press, New Jersey.&lt;/ref&gt;  Additionally, it is possible to have the threshold vary at the observational level, by replacing {{mvar|L}} by {{mvar|L&lt;sub&gt;i&lt;/sub&gt;}} in the formulas above. &lt;ref&gt; Wooldridge, J. (2002): Econometric Analysis of Cross Section and Panel Data, MIT Press, Cambridge, Mass.&lt;/ref&gt;

[[Tests of specification]] in duration models encompass testing for the validity of the imposed functional form. Tests of restrictions on the [[functional form]] are similar to those testing for [[unobserved heterogeneity]], where the restriction imposes no such heterogeneity. Nevertheless, it is often desirable to test for such heterogeneity, as this can [[bias]] the estimation of the hazard rate.&lt;ref&gt; Cameron A. C. and P. K. Trivedi (2005): Microeconometrics: Methods and Applications. Cambridge University Press, New York.&lt;/ref&gt;  Similarly, tests for censoring exist that compare the distribution of the generalized error under the censored and the uncensored assumption. &lt;ref&gt; 	Jaggia, S. and P. K. Trivedi (1994): Joint and Separate Score Test for Heterogeneity in a Censored Exponential Model. Review of Economics and Statistics, 79, pp. 340-343.&lt;/ref&gt;

==References==
{{Reflist}}

{{uncategorised|date=November 2015}}</text>
      <sha1>qqhxekkfg45jjpuaixlkwlj9ogj4y67</sha1>
    </revision>
  </page>
  <page>
    <title>Multiple treatments</title>
    <ns>0</ns>
    <id>48673880</id>
    <revision>
      <id>693202400</id>
      <parentid>692839895</parentid>
      <timestamp>2015-12-01T01:35:48Z</timestamp>
      <contributor>
        <username>GeoffreyT2000</username>
        <id>21491290</id>
      </contributor>
      <comment>Added references section.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve">{{multiple issues|
{{cleanup redlinks|date=December 2015}}
{{orphan|date=December 2015}}
{{refimprove|date=December 2015}}
}}
'''Multiple treatments''', like [[multivalued treatments]], generalize the [[binary treatment effects]] framework. But rather than focusing on a treatment effect that can take on different values, the focus now is on different types of treatment. One example could be a [[job training program]], where different types of job training are offered to the participants. The case of multiple treatments is relatively difficult to handle, as it can require additional [[functional form]] restrictions, especially when addressing the [[counterfactual]] or potential outcomes framework.&lt;ref&gt; Wooldridge, J. (2005): Econometric Analysis of Cross Section and Panel Data, MIT Press, Cambridge, Mass.&lt;/ref&gt;  Nevertheless, the general [[instrumental variable]] framework used to analyze binary treatment effects has been extended to allow for multiple treatments. &lt;ref&gt; Wooldridge, J. (2000): Instrumental Variables Estimation of the Average Treatment Effect in the Correlated Random Coefficient Model. Mimeo, Michigan State University Department of Economics.&lt;/ref&gt;

There are different approaches available to analyze multiple treatment effects. One can think of treatment effects within this framework as the difference in the counterfactual outcomes that would have been observed if the agent faced different general [[choice sets]], with [[multinomial]] choices being a natural way to analyze multiple treatments.&lt;ref&gt; Heckman, J. J., and E. J. Vytlacil (2007): Econometric Evaluation of Social Programs, Part II: Using the Marginal Treatment Effect to Organize Alternative Econometric Estimators to Evaluate Social Programs, and to Forecast the Effects in New Environments. Handbook of Econometrics, Vol 6, ed. by J. J. Heckman and E. E. Leamer. North Holland.&lt;/ref&gt;  More formally, assume there are ''J'' options available and the value to the agent of choosing option ''j'' is 

''R&lt;sub&gt;j&lt;/sub&gt; (Z&lt;sub&gt;j&lt;/sub&gt; )=v&lt;sub&gt;j&lt;/sub&gt; (Z&lt;sub&gt;j&lt;/sub&gt; ) -ϵ&lt;sub&gt;j&lt;/sub&gt;''

where ''ε&lt;sub&gt;j&lt;/sub&gt;'' is some [[unobserved]] [[random shock]]. Then the agent will choose alternative ''j'' such that ''R&lt;sub&gt;j&lt;/sub&gt;'' ≥ ''R&lt;sub&gt;k&lt;/sub&gt;'' for all ''k≠j''. There is a potential outcome associated with each possible state, ''Y&lt;sub&gt;j&lt;/sub&gt;'' = ''μ&lt;sub&gt;j&lt;/sub&gt;(X&lt;sub&gt;j&lt;/sub&gt;, U&lt;sub&gt;j&lt;/sub&gt;)'', where ''X'' is a vector of observered characteristics and ''U'' is a vector of unobserved characteristics. The observed outcome is 
''Y= &lt;math&gt; \sum_{j=0}^J  &lt;/math&gt; D&lt;sub&gt;j&lt;/sub&gt;  Y&lt;sub&gt;j&lt;/sub&gt;'' where ''D&lt;sub&gt;j&lt;/sub&gt;'' is an indicator that equals 1 when the treatment equals ''j'' and 0 when it does not equal ''j''. The [[parameters]] of interest are the treatment effects ''Y&lt;sub&gt;j&lt;/sub&gt; - Y&lt;sub&gt;k&lt;/sub&gt;'' for pairs ''k'' and ''j''. 

Other frameworks focus less on the choice dynamics and consider a [[random coefficient]] setting for the different treatment options, or use dummy variables for the different possible treatments. For instance, when thinking about two possible treatments that are not [[mutually exclusive]], four [[indicator]] variables can fully specify the different treatment options. &lt;ref&gt; 	 Wooldridge, J. (2002): Econometric Analysis of Cross Section and Panel Data, MIT Press, Cambridge, Mass.&lt;/ref&gt;

Some authors have suggested that when testing for multiple outcomes, one might want to adjust the p-values or significance levels when testing hypotheses.&lt;ref&gt; 	 Feise, R. J. (2002): Do multiple outcome measures require p-value adjustment? BMC Medical Research Methodology 2:8.&lt;/ref&gt;  In a [[Bayesian]] setting, it has been argued that the problem of multiple treatments can be incorporated in [[multilevel models]], which addresses the multiple comparisons problems while yielding more efficient estimates if the [[model specification]] is correct.  &lt;ref&gt; 	 Gelman, A., J. Hill and Y. Masanao (2008): Why we (usually) don't have to worry about multiple comparisons.Journal of Research on Educational Effectiveness 5, pp. 189-211.&lt;/ref&gt;

==References==
{{Reflist}}

[[Category:Statistics]]</text>
      <sha1>oqwzksqdl1ce120drt0qmjxmshuc9e0</sha1>
    </revision>
  </page>
  <page>
    <title>Roy model</title>
    <ns>0</ns>
    <id>48673882</id>
    <revision>
      <id>692934356</id>
      <parentid>692840629</parentid>
      <timestamp>2015-11-29T09:30:49Z</timestamp>
      <contributor>
        <username>Yobot</username>
        <id>7328338</id>
      </contributor>
      <minor />
      <comment>[[WP:CHECKWIKI]] error fixes, added [[CAT:O|orphan]] tag using [[Project:AWB|AWB]] (11749)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve">{{Orphan|date=November 2015}}

The '''Roy model''' is one of the earliest works in economics on [[self-selection]] due to [[Arthur Roy]]. The basic model considers two types of workers that choose occupation in one of two sectors. Roy's original paper deals with workers selecting into fishing and hunting professions, where there is no uncertainty about the amount of goods (fishes or rabbits) will be caught in a given period, but fishing is more costly as it requires more skill. The central question that Roy tries to answer in the original paper is whether the best hunters will hunt and the best fishermen will fish. While the discussion is non-mathematical, it is observed that choices will depend on the distribution of skills, the correlation between these skills in the population, and the technology available to use these skills.&lt;ref&gt;Roy, A. (1951): Some Thoughts on the Distribution of Earnings. Oxford Economic Papers 3(2), pp. 135-146.&lt;/ref&gt;

[[George Borjas]] was the first to formalize the model of Roy in a mathematical sense and apply it to self-selection in [[immigration]]. Specifically, assume source country 0 and destination country 1, with log earnings in a country ''i'' given by ''w&lt;sub&gt;i&lt;/sub&gt;= a&lt;sub&gt;i&lt;/sub&gt; + e&lt;sub&gt;i&lt;/sub&gt;'', where ''e&lt;sub&gt;i&lt;/sub&gt;∼N(0, &lt;math&gt;s_i^2&lt;/math&gt; )''. Additionally, assume there is a cost ''C'' associated with migrating from country 0 to country 1 and workers now all parameters and their own realization of e&lt;sub&gt;0&lt;/sub&gt; and e&lt;sub&gt;1&lt;/sub&gt;. Borjas then uses the implications of the Roy model to infer something about what wages for immigrants in country 1 would have been had they stayed in country 0 and what wages for non-immigrants in country 0 would have been had they migrated. The third, and final, element needed for this is the correlation between the wages in the two countries, ''ρ''. A worker will choose to immigrate if ''a&lt;sub&gt;1&lt;/sub&gt; - a&lt;sub&gt;0&lt;/sub&gt; - C + e&lt;sub&gt;1&lt;/sub&gt; - e&lt;sub&gt;0&lt;/sub&gt; &gt; 0'' which will happen with probability 
''1 - Φ ( v )'' where ''v'' is &lt;math&gt; \frac {(a_{1} - a_{0}-{C})}{s_{v}} &lt;/math&gt; , ''sv'' is the standard deviation of ''e&lt;sub&gt;1&lt;/sub&gt; – e&lt;sub&gt;0&lt;/sub&gt;'', and  Φ is the standardnormal cdf.&lt;ref&gt;	Borjas, G. J. (1987): Self-Selection and the Earnings of Immigrants. American Economic Review 77(4), pp. 531-553.&lt;/ref&gt;  This leads to the famous central result that the expected wage for immigrants depends on the selection mechanism, as shown in equation (1), where ϕ is the standardnormal pdf and, like before, Φ is the standardnormal cdf.

''E[w&lt;sub&gt;0&lt;/sub&gt;'' |''Immigrate''] = a&lt;sub&gt;0&lt;/sub&gt; +ρs&lt;sub&gt;0&lt;/sub&gt; &lt;math&gt;  (\frac {\phi ({v})}{1-\phi ({v}) })	&lt;/math&gt;											(1)

While Borjas was the first to mathematically formalize the Roy model in mathematical terms, the Roy model has guided thinking in other fields of research as well. A famous example by [[James Heckman]] and [[Bo Honoré]] who study [[labor market participation]] using the Roy model, where the choice equation leads to the [[Heckman selection correction]] procedure.&lt;ref&gt;	Heckman, J. J., Honoré, B. E. (1990): The Empirical Content of the Roy Model. Econometrica 58(5), pp. 1121-1149.&lt;/ref&gt;  More generally, Heckman and Vytlacil propose the Roy model as an alternative to the LATE framework proposed by Angrist and Imbens.&lt;ref&gt;	Heckman, J. J., Vytlacil, E. (2007): Econometric evaluation of social programs, part I: Causal models, structural models and econometric policy evaluation. Handbook of Econometrics, Vol. 6, ed. by J. J. Heckman, and E. E. Leamer. North Holland.&lt;/ref&gt;&lt;ref&gt;	Imbens, G. W., Angrist, J. D. (1994): Identification and Estimation of Local Average Treatment Effects.  Econometrica 62(2), pp. 467-475.&lt;/ref&gt;

{{Reflist}}

[[Category:Economics]]</text>
      <sha1>gxkui80599gs7fnahwqzcoh2q07bu12</sha1>
    </revision>
  </page>
  <page>
    <title>Binary response model with continuous endogenous explanatory variables</title>
    <ns>0</ns>
    <id>48673896</id>
    <revision>
      <id>693415313</id>
      <parentid>693413667</parentid>
      <timestamp>2015-12-02T12:04:16Z</timestamp>
      <contributor>
        <username>AnomieBOT</username>
        <id>7611264</id>
      </contributor>
      <minor />
      <comment>Dating maintenance tags: {{Context}}</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve">{{context|date=December 2015}}
Given a [[probit model]] &lt;ref&gt;  Greene, W. H. (2003), Econometric Analysis , Prentice Hall , Upper Saddle River, NJ .&lt;/ref&gt;  ''y=1[y* &gt; 0]'' where  ''y* = x&lt;sub&gt;1&lt;/sub&gt; β + zδ + u, and u ~ N(0,1)'', without losing generality, ''z'' can be represented as ''z = x&lt;sub&gt;1&lt;/sub&gt; θ&lt;sub&gt;1&lt;/sub&gt; + x&lt;sub&gt;2&lt;/sub&gt; θ&lt;sub&gt;2&lt;/sub&gt; + v''. When ''u'' is correlated with ''v'', there will be an issue of [[endogeneity]]. This can be caused by omitted variables and [[measurement errors]] &lt;ref&gt;  Fuller, Wayne A. (1987), Measurement error models, John Wiley &amp; Sons, Inc, ISBN 0-471-86187-1&lt;/ref&gt; . There are also many cases where ''z'' is partially determined by ''y'' and endogeneity issue arises. For instance, in a model evaluating the effect of different patient features on their choice of whether going to hospital, ''y''  is the choice and ''z''  is the amount of the medicine a respondent took, then it is very intuitive that more often the respondent goes to hospital, it is more likely that she took more medicine, hence endogeneity issue arises &lt;ref&gt;Bruce A. Rayton. (2006): “Examining the interconnection of job satisfaction and organizational commitment: an application of the bivariate probit model”, The International Journal of Human Resource Management, Vol. 17, Iss. 1.&lt;/ref&gt;. When there are endogenous explanatory variables, the estimator generated by usual estimation procedure will be inconsistent, then the corresponding estimated Average Partial Effect (APE) &lt;ref&gt;  Wooldridge, J. (2002): Econometric Analysis of Cross Section and Panel Data, MIT Press, Cambridge, Mass, pp 22.&lt;/ref&gt;  will be inconsistent, too. 

To address this issue, there are usually two different estimation procedure to generate [[consistent estimators]].  Under the normality assumption ''v~N(0,σ&lt;sup&gt;2&lt;/sup&gt;), u = ρv + ε'' must hold, where ''ρ = cov(u , v)/σ&lt;sup&gt;2&lt;/sup&gt;'' and ''ε~N(0,1-ρ&lt;sup&gt;2&lt;/sup&gt; σ&lt;sup&gt;2&lt;/sup&gt;)''. Then the equation for ''y&lt;sup&gt;*&lt;/sup&gt;'' can be rewritten as ''y&lt;sup&gt;*&lt;/sup&gt; = x&lt;sub&gt;1&lt;/sub&gt; β + zδ + ρv + ε''. 

This model can be consistently estimated by [[2-Stage Least Square (2SLS)]]:

1) Regress ''z'' on ''(x&lt;sub&gt;1&lt;/sub&gt;, x&lt;sub&gt;2&lt;/sub&gt;)'' and obtain the consistent estimator  &lt;math&gt;\widehat{\theta}&lt;/math&gt; and the residual &lt;math&gt;\hat{v}&lt;/math&gt; ; 

2) Estimate the binary response model on ''(x&lt;sub&gt;1&lt;/sub&gt;, z, &lt;math&gt;\hat{v}&lt;/math&gt;)'' and get the consistent estimator for the scaled coefficients ''(β&lt;sub&gt;ρσ&lt;/sub&gt;, δ&lt;sub&gt;ρσ&lt;/sub&gt;, ρ&lt;sub&gt;ρσ&lt;/sub&gt;) ≡ (β, δ, ρ)/√(1 - ρ&lt;sup&gt;2&lt;/sup&gt; σ&lt;sup&gt;2&lt;/sup&gt; );''

Then ''(y = 1│x, z)'' = Φ ( x&lt;sub&gt;1&lt;/sub&gt; &lt;math&gt;\hat{\beta}&lt;/math&gt;&lt;sub&gt;ρσ&lt;/sub&gt; + z&lt;math&gt;\hat{\delta}&lt;/math&gt;&lt;sub&gt;ρσ&lt;/sub&gt; + &lt;math&gt;\hat{\rho}&lt;/math&gt;&lt;sub&gt;ρσ&lt;/sub&gt;&lt;math&gt;\hat{v}&lt;/math&gt;) . Since the APE of variable &lt;math&gt; \tilde{w} &lt;/math&gt; at (&lt;math&gt; \tilde{x} &lt;/math&gt;,&lt;math&gt; \tilde{z} &lt;/math&gt;) is given by

E&lt;sub&gt;v&lt;/sub&gt; 
&lt;math&gt;[ \frac  {\partial \phi ( x_{1} \beta_{\rho\sigma} + z\delta_{\rho\sigma} + \rho_{\rho\sigma} v)} {\partial \tilde{w}} | _(\tilde{x} &lt;/math&gt;,&lt;math&gt; \tilde{z} &lt;/math&gt;)] 


By Law of Large Number, a consistent estimator is given as

&lt;math&gt;\frac {1}{N}&lt;/math&gt;&lt;math&gt;\sum_{i=1}^n&lt;/math&gt;&lt;math&gt; \frac  {\partial \phi ( x_{1} \hat{\beta}_{\rho\sigma} + z\hat{\delta}_{\rho\sigma} + \hat{\rho}_{\rho\sigma} \hat{v}_i)} {\partial \tilde{w}} | _(\tilde{x} &lt;/math&gt;,&lt;math&gt; \tilde{z} &lt;/math&gt;) 

This model can also be consistently estimated by conditional [[Maximum Likelihood Method]]&lt;ref&gt;This issue can also be addressed under the Semiparametric setting, for more details, refere to: Richard W. Blundell;  James L. Powell. (2004): ”Endogeneity in Semiparametric Binary Response Models”,Review of Economic Studies 71 (3), pp: 655-679&lt;/ref&gt; . Because ''P(y, z│x) = P (y│z, x) P (z| x)'' where ''P (y│x, z)'' is given by

&lt;math&gt;[ \phi  (\frac {x_1 (\beta - \rho\theta_1) - x_2 \rho\theta_2 + z(\delta + \rho )}{\sqrt{(1 - \rho ^2 \sigma^2 }})]^y &lt;/math&gt; &lt;math&gt;[ 1- \phi  (\frac {x_1 (\beta - \rho\theta_1) - x_2 \rho\theta_2 + z(\delta + \rho )}{\sqrt{(1 - \rho ^2 \sigma^2 }})]^ {1-y} &lt;/math&gt;

and ''P(z│x)'' is given by 
&lt;math&gt; \phi  (\frac {z - x_1\theta_1 - x_2\theta_2} {\sigma}) &lt;/math&gt;

Then the log-likelihood function for maximization is given by:

&lt;math&gt;\sum_{i=1}^n&lt;/math&gt; &lt;math&gt; y_i \log [ \phi  (\frac {x_1i (\beta - \rho\theta_1) - x_2i \rho\theta_2 + z_i(\delta + \rho )}{\sqrt{(1 - \rho ^2 \sigma^2 }})] &lt;/math&gt;

&lt;math&gt;+  (1- y_i) \log [ 1- \phi  (\frac {x_1i (\beta - \rho\theta_1) - x_2i \rho\theta_2 + z_i(\delta + \rho )}{\sqrt{(1 - \rho ^2 \sigma^2 }})] &lt;/math&gt;

&lt;math&gt;+  \log [ \phi  (\frac {z - x_1\theta_1 - x_2\theta_2} {\sigma})] &lt;/math&gt;


Once the [[consistent estimators]] are obtained, APE can be calculated following the same procedure given above. All the discussion above is mainly about the [[probit model]]. When the distribution assumption is changed, the same logic still applies. 



{{Reflist}}

[[Category:Econometrics]]</text>
      <sha1>oe4hnkw38b0rsk5cwri1nk53e9v7cvv</sha1>
    </revision>
  </page>
  <page>
    <title>Dynamic unobserved effects model</title>
    <ns>0</ns>
    <id>48673924</id>
    <revision>
      <id>692840394</id>
      <parentid>692840364</parentid>
      <timestamp>2015-11-28T18:12:47Z</timestamp>
      <contributor>
        <username>Qwertyus</username>
        <id>196471</id>
      </contributor>
      <comment>Added {{[[Template:lead missing|lead missing]]}} tag to article ([[WP:TW|TW]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve">{{lead missing|date=November 2015}}
The “dynamic” here means the dependence of the dependent variable on its past history, this is usually used to model the “state dependence” in economics. For instance, a person who cannot find a job this year, it will be hard for her to find a job next year because the fact that she doesn’t have a job this year will be a very negative signal for the potential employers. The “unobserved effects” means that one or some of the explanatory variables are unobservable. For example, one’s preference affects quite a lot her consumption choice of the ice cream with a certain taste, but preference is unobservable. A typical dynamic unobserved effects model is represented &lt;ref&gt;  Wooldridge, J. (2002): Econometric Analysis of Cross Section and Panel Data, MIT Press, Cambridge, Mass, pp 495.&lt;/ref&gt;  as:

P(y&lt;sub&gt;it&lt;/sub&gt; = 1│y&lt;sub&gt;i,t-1&lt;/sub&gt;, … , y&lt;sub&gt;i,0&lt;/sub&gt; , z&lt;sub&gt;i&lt;/sub&gt; , c&lt;sub&gt;i&lt;/sub&gt; ) = G (z&lt;sub&gt;it&lt;/sub&gt; δ + ρ y&lt;sub&gt;i,t-1&lt;/sub&gt; + c&lt;sub&gt;i&lt;/sub&gt;)

where c&lt;sub&gt;i&lt;/sub&gt; is a unobservable explanatory variable, z&lt;sub&gt;it&lt;/sub&gt; is explanatory variables which are exogenous conditional on the c&lt;sub&gt;i&lt;/sub&gt;, and G(∙) is a [[cumulative distribution function]].

In this type of model, economists have a special interest in ρ, which is used to characterize the state dependence. For example, ''y&lt;sub&gt;i,t&lt;/sub&gt;'' can be a woman’s choice whether work or not, ''z&lt;sub&gt;it&lt;/sub&gt;'' includes the ''i''-th individual’s age, education level, numbers of kids and so on. ''c&lt;sub&gt;i&lt;/sub&gt;'' can be some individual specific characteristic which cannot be observed by economists &lt;ref&gt;James J. Heckman (1981): Studies in Labor Markets, University of Chicago Press, Chapter Heterogeneity and State Dependence&lt;/ref&gt;. It is a reasonable conjecture that one’s labor choice in period ''t'' should depend on his or her choice in period ''t'' - 1 due to habit formation or other reasons. This is dependence is characterized by parameter ''ρ''.

There are several [[MLE]]-based approaches to estimate ''δ'' and ''ρ'' consistently. The simplest way is to treat ''y&lt;sub&gt;i,0&lt;/sub&gt;'' as non-stochastic and assume ''c&lt;sub&gt;i&lt;/sub&gt;'' is [[independent]] with ''z&lt;sub&gt;i&lt;/sub&gt;''. Then integrate ''P(y&lt;sub&gt;i,t&lt;/sub&gt; , y&lt;sub&gt;i,t-1&lt;/sub&gt; , … , y&lt;sub&gt;i,1&lt;/sub&gt; | y&lt;sub&gt;i,0&lt;/sub&gt; , z&lt;sub&gt;i&lt;/sub&gt; , c&lt;sub&gt;i&lt;/sub&gt;)'' against the density of ''c&lt;sub&gt;i&lt;/sub&gt;'', we can obtain the conditional density P(y&lt;sub&gt;i,t&lt;/sub&gt; , y&lt;sub&gt;i,t-1&lt;/sub&gt; , … , y&lt;sub&gt;i,1&lt;/sub&gt; |y&lt;sub&gt;i,0&lt;/sub&gt; , z&lt;sub&gt;i&lt;/sub&gt;).  The objective function for the conditional [[MLE]] can be represented as:  ''&lt;math&gt; \sum_{i=1}^N &lt;/math&gt; log (P (y&lt;sub&gt;i,t&lt;/sub&gt; , y&lt;sub&gt;i,t-1&lt;/sub&gt;, … , y&lt;sub&gt;i,1&lt;/sub&gt; | y&lt;sub&gt;i,0&lt;/sub&gt; , z&lt;sub&gt;i&lt;/sub&gt;)).'' 

Treating ''y&lt;sub&gt;i,0&lt;/sub&gt;'' as non-stochastic implicitly assumes the independence of ''y&lt;sub&gt;i,0&lt;/sub&gt;'' on ''z&lt;sub&gt;i&lt;/sub&gt;''. But in most of the cases in reality, ''y&lt;sub&gt;i,0&lt;/sub&gt;'' depends on ''c&lt;sub&gt;i&lt;/sub&gt;'' and ''c&lt;sub&gt;i&lt;/sub&gt;'' also depends on ''z&lt;sub&gt;i&lt;/sub&gt;''. An improvement on the approach above is to assume a density of ''y&lt;sub&gt;i,0&lt;/sub&gt;'' conditional on (''c&lt;sub&gt;i&lt;/sub&gt;, z&lt;sub&gt;i&lt;/sub&gt;'') and conditional likelihood ''P(y&lt;sub&gt;i,t&lt;/sub&gt;) , y&lt;sub&gt;i,t-1&lt;/sub&gt; , … , y&lt;sub&gt;t,1&lt;/sub&gt;,y&lt;sub&gt;i,0&lt;/sub&gt; | c&lt;sub&gt;i&lt;/sub&gt;, z&lt;sub&gt;i&lt;/sub&gt;)'' can be obtained. Integrate this likelihood against the density of ''c&lt;sub&gt;i&lt;/sub&gt;'' conditional on ''z&lt;sub&gt;i&lt;/sub&gt;'' and we can obtain the conditional density ''P(y&lt;sub&gt;i,t&lt;/sub&gt; , y&lt;sub&gt;i,t-1&lt;/sub&gt; , … , y&lt;sub&gt;i,1&lt;/sub&gt; , y&lt;sub&gt;i,0&lt;/sub&gt; | z&lt;sub&gt;i&lt;/sub&gt;)''.  The objective function for the [[conditional MLE]] &lt;ref&gt;  Greene, W. H. (2003), Econometric Analysis , Prentice Hall , Upper Saddle River, NJ .&lt;/ref&gt; is ''&lt;math&gt; \sum_{i=1}^N &lt;/math&gt; log (P (y&lt;sub&gt;i,t&lt;/sub&gt; , y&lt;sub&gt;i,t-1&lt;/sub&gt;, … , y&lt;sub&gt;i,1&lt;/sub&gt; | y&lt;sub&gt;i,0&lt;/sub&gt; , z&lt;sub&gt;i&lt;/sub&gt;)).'' 

Based on the estimates for (''δ, ρ'') and the corresponding variance, test about the coefficients can be implemented &lt;ref&gt;  Whitney K. Newey, Daniel McFadden, Chapter 36 Large sample estimation and hypothesis testing, In: Robert F. Engle and Daniel L. McFadden, Editor(s), Handbook of Econometrics, Elsevier, 1994, Volume 4, Pages 2111-2245, ISSN 1573-4412, ISBN 9780444887665,&lt;/ref&gt; and the average partial effect  can be calculated. &lt;ref&gt;Chamberlain, G. (1980), “Analysis of Covariance with Qualitative Data,” Journal of Econometrics 18, 5-46&lt;/ref&gt;



{{Reflist}}</text>
      <sha1>lzg6t0h6f6b8208osmpurafcckcqh8v</sha1>
    </revision>
  </page>
  <page>
    <title>Smoothed maximum score estimator</title>
    <ns>0</ns>
    <id>48673945</id>
    <revision>
      <id>692840195</id>
      <parentid>692840175</parentid>
      <timestamp>2015-11-28T18:11:05Z</timestamp>
      <contributor>
        <username>Qwertyus</username>
        <id>196471</id>
      </contributor>
      <comment>added [[Category:Estimation theory]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve">When modelling discrete choice model, it is always assumed that the choice is determined by the comparison of the underlying latent utility&lt;ref&gt;  For more example, refer to: Smith, Michael D. and Brynjolfsson, Erik, Consumer Decision-Making at an Internet Shopbot (October 2001). MIT Sloan School of Management Working Paper No. 4206-01. &lt;/ref&gt; . Denote the population of the agents as ''T'', the common choice set for each agent as ''C'' . For agent ''t∈T'' , denote her choice as ''y&lt;sub&gt;t,i&lt;/sub&gt;'' , which is equal to 1 if choice ''i∈C''  is chosen and 0 otherwise.  Assume the linearity of the parameters and the additivity of the error term: for an agent ''t∈T'' , 
   	 
''y&lt;sub&gt;t,i&lt;/sub&gt; = 1 ↔ x&lt;sub&gt;t,i&lt;/sub&gt;β + ε&lt;sub&gt;t,i&lt;/sub&gt; &gt; x&lt;sub&gt;t,j&lt;/sub&gt;β + ε&lt;sub&gt;t,j&lt;/sub&gt;, ∀ j ≠ i and j ∈ C''

where ''x&lt;sub&gt;t,i&lt;/sub&gt;''  and ''x&lt;sub&gt;t,j&lt;/sub&gt;''  are the  ''q-'' dimensional observable covariates about the agent and the choice, and ''ε&lt;sub&gt;t,i&lt;/sub&gt;''  and ''ε&lt;sub&gt;t,j&lt;/sub&gt;''  are the decision errors caused by some cognitive reasons or information incompleteness. The construction of the observable covariates is very general. For instance, if ''C''  is a set of different brands of coffee, then  ''x&lt;sub&gt;t,i&lt;/sub&gt;''  includes the characteristics both of the agent ''t'' , such as age, gender, income and ethnicity, and of the coffee ''i'' , such as price, taste and whether it is local or imported.

Manski (1975) proposed a [[non-parametric model]] to estimate the parameters. In this model, denote the number of the elements of the choice set as ''J'' , the total number of the agents as ''N'' , and  ''W ( J - 1) &gt; W (J - 2) &gt; ... &gt; W (1) &gt; W (0)'' is a sequence of real numbers. The Maximum Score (MS) &lt;ref&gt;  Charles F. Manski (1975), “Maximum Score Estimation of the Stochastic Utility Model of Choice”, Journal of Econometrics 3, pp. 205-228.&lt;/ref&gt;estimator&lt;sup&gt;2&lt;/sup&gt; is defined as:

 &lt;math&gt; \hat {b}_{MS} =  {\operatorname{arg\max}}_b \frac {1}{N} \sum_{t=1}^N \sum_{i=1}^J y_{t,i} W (\sum\nolimits_{j \in C, j \neq i} 1 (x_{t,i}b &gt; x_{t,j}b)) &lt;/math&gt;  


	 
Here,  &lt;math&gt; (\sum\nolimits_{j \in C, j \neq i} 1 (x_{t,i}b &gt; x_{t,j}b)) &lt;/math&gt;  is the ranking of the certainty part of the underlying utility of choosing ''i'' . Under certain conditions, the maximum score estimator can be [[weak consistent]], but its asymptotic property will be very complicated&lt;ref&gt;  Jeankyung Kim; David Pollard (1990), “Cube Root Asymptotics”, The Annals of Statistics 1, pp.191-219.&lt;/ref&gt;. This issue mainly comes from the non-smooth of the objective function. Horowitz (1992) proposed a Smoothed Maximum Score (SMS) &lt;ref&gt;  Joel L. Horowitz (1992), “A Smoothed Maximum Score Estimator for the Binary Response Model”, Econometrica 3, pp. 505-531.&lt;/ref&gt; estimator which has much better asymptotic property. The basic idea of this new estimator is just to replace the non-smoothed weight function &lt;math&gt;W (\sum\nolimits_{j \in C, j \neq i} 1 (x_{t,i}b &gt; x_{t,j}b)) &lt;/math&gt;   with a smoothed one. Define a smooth [[kernel function]] ''K'' satisfying following conditions:

(1)  |K(·)| is bounded over R ;

(2) &lt;math&gt; \lim_{u\to -\infty} K (u) = 0  and \lim_{u\to +\infty} K (u) =1 &lt;/math&gt; ;

(3) &lt;math&gt; \dot {K} (u) = \dot {K} (-u) &lt;/math&gt;
           
	 
Here, the kernel function is analogous to a CDF whose PDF is symmetric around 0. Then, the SMS estimator is defined as:
	 
&lt;math&gt; \hat {b}_{SMS} =  {\operatorname{arg\max}}_b \frac {1}{N} \sum_{t=1}^N \sum_{i=1}^J y_{t,i}  \sum\nolimits_{j \in C, j \neq i} K ( X_ {t,i}b - x_{t,j} b / h_N)&lt;/math&gt;  


where &lt;math&gt; (h_N, N = 1,2, ...) &lt;/math&gt;  is a sequence of strictly positive numbers and &lt;math&gt; \lim_{N\to +\infty} h_N = 0 &lt;/math&gt; . Here, the intuition is the same with the construction of the traditional MS estimator: it is more likely to choose the choice with higher certainty part of the utility. Under certain conditions, SMS estimator is consistent, and more importantly, it has an asymptotic normal distribution. Therefore, all the testing and inference based on [[asymptotic distribution]] can be implemented &lt;ref&gt;  For a survey study, refer to: Jin Yan (2012), “A Smoothed Maximum Score Estimator for Multinomial Discrete Choice Models”, Working Paper.&lt;/ref&gt; . 




{{Reflist}}

[[Category:Estimation theory]]</text>
      <sha1>b2lwdtr6kql3c8tybrxlb93uwlvijuy</sha1>
    </revision>
  </page>
  <page>
    <title>Probit model for panel data with heterogeneity and endogenous explanatory variables</title>
    <ns>0</ns>
    <id>48673954</id>
    <revision>
      <id>692840134</id>
      <parentid>692840083</parentid>
      <timestamp>2015-11-28T18:10:23Z</timestamp>
      <contributor>
        <username>Qwertyus</username>
        <id>196471</id>
      </contributor>
      <minor />
      <comment>Qwertyus moved page [[Probit Model for Panel Data with Heterogeneity and Endogenous Explanatory Variables]] to [[Probit model for panel data with heterogeneity and endogenous explanatory variables]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve">In many cases, there is an unobservable heterogeneity in the [[probit model]]. For instance, when modelling the consumption choice of a certain brand, consumers’ personal preference is unobserved but needs to be considered in the model &lt;ref&gt;  Pradeep K. Chintagunta, Dipak C. Jain and Naufel J. Vilcassim (1991), “Investigating Heterogeneity in Brand Preferences in Logit Models for Panel Data”, Journal of Marketing Research, Vol. 28, pp. 417-428.&lt;/ref&gt; . Owing to omitted variable or measurement error, [[endogeneity]] issue also could arise&lt;ref&gt;  Wooldridge, J. (2002): Econometric Analysis of Cross Section and Panel Data, MIT Press, Cambridge, Mass, pp 473.&lt;/ref&gt; . A [[probit model]] including both of these two issues can be represented as:

&lt;math&gt; y_it = 1 [ y_it^* &gt; 0] &lt;/math&gt;
 
&lt;math&gt; y_it ^ * = x_{it}^{(1)} \beta + z_{it} \delta + c_i + u_{it} &lt;/math&gt; 

&lt;math&gt; z_it = x_{it}^{(1)} \gamma_1 + x_{it}^{(2)} \gamma_2+v_{it} &lt;/math&gt;

where &lt;math&gt; c_i &lt;/math&gt; is the unobservable heterogeneity effect and &lt;math&gt; u_{it} \mid x_i ~ N (0, 1), v_{it} | x_i ~ N ( 0, \sigma^{2}) &lt;/math&gt;. If &lt;math&gt; v_{it} &lt;/math&gt; and &lt;math&gt; u_{it} &lt;/math&gt; are independent, this model will degenerate to a [[probit model]] with unobservable heterogeneity. In this case, we can just integrate &lt;math&gt; P (y_{iT},\ldots,y_{i0} \mid x_i,c_i) &lt;/math&gt; against the density of &lt;math&gt; c_i &lt;/math&gt; conditional on &lt;math&gt; x_i &lt;/math&gt;, then &lt;math&gt; P(y_{iT},\ldots,y_{i0} |x_i) &lt;/math&gt; can be obtained &lt;ref&gt;  Wooldridge, J. (2002): Econometric Analysis of Cross Section and Panel Data, MIT Press, Cambridge, Mass, pp 494.&lt;/ref&gt; and the objective for the conditional [[Maximum Likelihood Estimation]] is 

&lt;math&gt; \sum_{i=1}^N \log [P(y_{iT},\ldots,y_{i0} |x_i)] &lt;/math&gt;

If &lt;math&gt; v_{it} &lt;/math&gt; and &lt;math&gt; u_{it} &lt;/math&gt; are correlated, under the normality assumption, it can be assumed that &lt;math&gt; v_{it} &lt;/math&gt; =&lt;math&gt; \rho u_{it} + \epsilon_{it} &lt;/math&gt; &lt;ref&gt;  For more details, refer to Whitney K. Newey (1987), “Efficient Estimation of Limited Dependent Variable Models with Endogenous Explanatory Variables”, Journal of Econometrics 36, pp. 231-250.&lt;/ref&gt;  , where &lt;math&gt; \epsilon_{it} \sim _{iid} N (0, \sigma^2 - \rho^2) &lt;/math&gt; and &lt;math&gt; \epsilon_i &lt;/math&gt; is independent with &lt;math&gt; v_i &lt;/math&gt; and &lt;math&gt; u_i &lt;/math&gt;. Then the model can be rewritten as:

&lt;math&gt; y_{it} = 1 [x_{it}^{(1)} (\beta + \delta\gamma_1 ) + x_{it}^{(2)}  \delta\gamma_2 + c_i+ \omega_{it} &gt; 0] &lt;/math&gt;

where &lt;math&gt; \omega_{it} = (1+\rho\delta) u_{it} + \delta\epsilon_{it}, \omega_{it} /sim N (0, (1+\rho\delta)^2 + \delta^2 (\sigma^2 - \rho^2 )) &lt;/math&gt; and &lt;math&gt; corr(\omega_{it} , \omega _{(it-s)} ) = \frac {(1 + \rho\delta)^2 corr (u_{it}, u_{(it-s)} )} { ((1 + \rho\delta)^2 + \delta^2 (\sigma^2 - \rho^2 ) )}. &lt;/math&gt; 

Based on this, following the same [[Maximum Likelihood Estimation]] procedure and the scaled parameter &lt;math&gt; (\beta + \delta\gamma_1 , \delta\gamma_2) / \sqrt {(1+\rho\delta)^2 + \delta^2 (\sigma^2 - \rho^2  )} &lt;/math&gt; can be [[consistently estimated]], then the APE &lt;ref&gt;  Wooldridge, J. (2002): Econometric Analysis of Cross Section and Panel Data, MIT Press, Cambridge, Mass, pp 22.&lt;/ref&gt; can be consistently estimated correspondingly.



{{Reflist}}

[[Category:Regression analysis]]
[[Category:Panel data]]</text>
      <sha1>1syvh46bp74c5pdekq02e2f6sq20pec</sha1>
    </revision>
  </page>
  <page>
    <title>Testing in binary response index models</title>
    <ns>0</ns>
    <id>48673958</id>
    <revision>
      <id>692844192</id>
      <parentid>692844104</parentid>
      <timestamp>2015-11-28T18:46:16Z</timestamp>
      <contributor>
        <username>Qwertyus</username>
        <id>196471</id>
      </contributor>
      <minor />
      <comment>Qwertyus moved page [[Testing in Binary Response Index Model]] to [[Testing in binary response index models]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve">{{sections|date=November 2015}}

Denote a binary response index model as: &lt;math&gt; P[Y_i = 1 \mid X_i ] = G (X_i  \beta)&lt;/math&gt;, &lt;math&gt;[Y_i = 0\mid X_i ] = 1-G (X_i'  \beta)&lt;/math&gt; where &lt;math&gt;X_i  \in  R^N &lt;/math&gt;. This type of model is applied in many economic contexts, especially in modelling the choice-making behavior. For instance, &lt;math&gt; Y_i&lt;/math&gt;  here denotes whether consumer &lt;math&gt; i &lt;/math&gt;  chooses to purchase a certain kind of chocolate, and &lt;math&gt; X_i &lt;/math&gt;  includes many variables characterizing the features of consumer &lt;math&gt; i &lt;/math&gt; . Through function &lt;math&gt; G(\cdot) &lt;/math&gt; , the probability of choosing to purchase is determined &lt;ref&gt;  For an application example, refer to: Rayton, B. A. (2006): “Examining the Interconnection of Job Satisfaction and Organizational Commitment: an Application of the Bivariate Probit Model”,The International Journal of Human Resource Management, Vol. 17, Iss. 1.&lt;/ref&gt; .

Now, suppose its [[maximum likelihood estimator]] (MLE)  &lt;math&gt; \hat {\beta}_{u} &lt;/math&gt;  has an asymptotic distribution as &lt;math&gt; \sqrt {n} ( \hat {\beta}_{u} - \beta) \xrightarrow{d} N (0, V) &lt;/math&gt; and there is a feasible consistent estimator for the asymptotic variance &lt;math&gt; V &lt;/math&gt; denoted as &lt;math&gt; \hat {V} &lt;/math&gt; . Usually, there are two different types of hypothesis needed to be tested in binary response index model. 

The first type is testing the multiple exclusion restrictions, namely, testing &lt;math&gt; \beta_2 = 0 with =[\beta_1; \beta_2] where \beta _ 2 \in R^Q &lt;/math&gt;. If the unrestricted [[MLE]] can be easily computed, it is convenient to use the [[Wald test]] &lt;ref&gt;  Greene, W. H. (2003), Econometric Analysis , Prentice Hall , Upper Saddle River, NJ .&lt;/ref&gt;  whose test statistic is constructed as:

&lt;math&gt; (D \hat {\beta}_{u})^{T} (D\hat {V}D^{T}/n)^{-1}  (D\hat {\beta}_{u}) \xrightarrow{d}  X_{Q}^{2} &lt;/math&gt;

Where ''D'' is a diagonal matrix with the last ''Q'' diagonal entries as 0 and others as 1. If the restricted MLE can be easily computed, it is more convenient to use the [[Score test (LM test)]]. Denote the maximum likelihood estimator under the restricted model as &lt;math&gt; (\hat {\beta}_{r}) &lt;/math&gt; and define &lt;math&gt; \hat {u}_{i} \equiv Y_i - G(X_i' \hat {\beta}_{r}), \hat {G}_{i} \equiv G(X_i' \hat {\beta}_{r}) &lt;/math&gt;
and &lt;math&gt; \hat {g}_{i} \equiv g(X_i' \hat {\beta}_{r}&lt;/math&gt;, where &lt;math&gt; g(\cdot) = G' (\cdot) &lt;/math&gt;.  Then run the [[OLS regression]] &lt;math&gt; \frac {\hat {u}_{i}} {\sqrt{(\hat {G}_{i}(1-\hat {G}_{i}) }} &lt;/math&gt; on &lt;math&gt; \frac {\hat {g}_{i}} {\sqrt{(\hat {G}_{i}(1-\hat {G}_{i}) }}X_{1i}', \frac {\hat {g}_{i}} {\sqrt{(\hat {G}_{i}(1-\hat {G}_{i}) }}X_{2i}' &lt;/math&gt;, where &lt;math&gt; X_i =  [X_{1i} ; X_{2i} ] &lt;/math&gt; and &lt;math&gt; X_{2i}\varepsilon R^Q &lt;/math&gt;. The LM statistic is equal to the explained sum of squares from this regression &lt;ref&gt;  Wooldridge, J. (2002): Econometric Analysis of Cross Section and Panel Data, MIT Press, Cambridge, Mass.&lt;/ref&gt;  and it is asymptotically distributed as &lt;math&gt; X_Q^2 &lt;/math&gt;. If the [[MLE]] can be computed easily under both of the restricted and unrestricted models, [[Likelihood-ratio test]] is also a choice: let &lt;math&gt; L_u &lt;/math&gt; denote the value of the log-likelihood function under the unrestricted model and let &lt;math&gt; L_r &lt;/math&gt; denote the value under the restricted model, then &lt;math&gt; 2(L_u - L_r) &lt;/math&gt; has an asymptotic &lt;math&gt; X_Q^2 &lt;/math&gt; distribution.

The second type is testing a nonlinear hypothesis about &lt;math&gt; \beta &lt;/math&gt;, which can be represented as  &lt;math&gt; H_0 : c(\beta) = 0 &lt;/math&gt;  where &lt;math&gt; c(\beta) &lt;/math&gt; is a Q×1 vector of possibly nonlinear functions satisfying the differentiability and rank requirements. In most of the cases, it is not easy or even feasible to compute the [[MLE]] under the restricted model when &lt;math&gt; c(\beta) &lt;/math&gt;  include some complicated nonlinear functions. Hence, [[Wald test]] is usually used to deal with this problem. The test statistic is constructed as:

&lt;math&gt; c(\hat {\beta}_{u}') [\nabla_\beta c(\hat {\beta}_{u}) \hat {V}\nabla_\beta c(\hat {\beta}_{u})']^{-1} c(\hat {\beta}_{u}) \xrightarrow{d} X_Q^2 &lt;/math&gt;

where &lt;math&gt;\nabla_\beta c(\hat {\beta}_{u}) &lt;/math&gt; is the Q×N Jacobian of &lt;math&gt; c(\beta) &lt;/math&gt; evaluated at &lt;math&gt; \hat {\beta}_{u} &lt;/math&gt;. 

For the tests with very general and complicated alternatives, the formula of the test statistics might not have the exactly same representation as above. But we can still derive the formulas as well as its asymptotic distribution by [[Delta Method]] &lt;ref&gt;  Casella, G., and Berger, R. L. (2002). Statistical inference. Duxbury Press.&lt;/ref&gt; and implement [[Wald test]], [[Score test]] or [[Likelihood-ratio test]] &lt;ref&gt;  Engle, Robert F. (1983). &quot;Wald, Likelihood Ratio, and Lagrange Multiplier Tests in Econometrics&quot;. In Intriligator, M. D.; and Griliches, Z. Handbook of Econometrics II. Elsevier. pp. 796–801. ISBN 978-0-444-86185-6.&lt;/ref&gt;. Which test should be used is determined by the relative computation difficulty of the [[MLE]] under restricted and unrestricted models.



{{Reflist}}

{{uncategorised|date=November 2015}}</text>
      <sha1>ovlh57n02oo8n80j3sno4de5la3q5v7</sha1>
    </revision>
  </page>
  <page>
    <title>Neglected Heterogeneity in Tobit Model</title>
    <ns>0</ns>
    <id>48678008</id>
    <revision>
      <id>693078700</id>
      <parentid>692914121</parentid>
      <timestamp>2015-11-30T07:28:52Z</timestamp>
      <contributor>
        <username>BG19bot</username>
        <id>14508071</id>
      </contributor>
      <minor />
      <comment>[[WP:CHECKWIKI]] error fix for #61.  Punctuation goes before References. Do [[Wikipedia:GENFIXES|general fixes]] if a problem exists. - using [[Project:AWB|AWB]] (11751)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve">{{sections|date=November 2015}}

In a [[Tobit model]] &lt;ref&gt;Tobin, James (1958). &quot;Estimation of relationships for limited dependent variables&quot;. Econometrica 26 (1): pp24–36.&lt;/ref&gt; ''y=y&lt;sup&gt;*&lt;/sup&gt;'''1''' [ y&lt;sup&gt;*&lt;/sup&gt; &gt; 0]''   where ''y&lt;sup&gt;*&lt;/sup&gt; = xβ + u'' and ''u| x'' ~ N (0, ''σ''&lt;sup&gt;2&lt;/sup&gt;), if a [[heterogeneity]] component  ''v'' is neglected, i.e. the true model should be ''y&lt;sup&gt;*&lt;/sup&gt; = xβ + γv + u'' , the neglected heterogeneity issue &lt;ref&gt;For the details about testing the heterogeneity issue, refer to Andrew Chesher (1984), “Testing for Neglected Heterogeneity”, Econometrica 4, pp. 865-872&lt;/ref&gt;  will arise. For instance, if  ''y=y&lt;sup&gt;*&lt;/sup&gt;''  is the profit of a firm,  ''y'' is the observed profit of the firm, and ''x''  only includes the variables about demand side, then the model neglects the variables about the supply side ''v'' , which should be included in the true model. In this example, the neglected heterogeneity issue arises.

If the neglected heterogeneity satisfies the independent condition &lt;ref&gt;Wooldridge, J. (2002): Econometric Analysis of Cross Section and Panel Data, MIT Press, Cambridge, Mass, pp 529.&lt;/ref&gt;   ''v| x'' ~ N (0, ''s''&lt;sup&gt;2&lt;/sup&gt;) and ''v''  is independent with ''u'' , for instance ''v''  is the price of the raw materials which are unrelated with the demand side features, the true model can be rewritten as:

''y=y&lt;sup&gt;*&lt;/sup&gt;'''1''' [ y&lt;sup&gt;*&lt;/sup&gt; &gt; 0]'' , where ''&lt;math&gt; y^* = x\beta + \tilde {u} &lt;/math&gt;, &lt;math&gt; \tilde {u} &lt;/math&gt; | x ~ ♦ (0, γ&lt;sup&gt;2&lt;/sup&gt; s&lt;sup&gt;2&lt;/sup&gt; + σ&lt;sup&gt;2&lt;/sup&gt; )''

Then, if run the [[Tobit model]] of ''y'' on ''x'' , the estimate for ''β''  will still be [[consistent]] but the error variance estimate will be for ''γ&lt;sup&gt;2&lt;/sup&gt; s&lt;sup&gt;2&lt;/sup&gt; + σ&lt;sup&gt;2&lt;/sup&gt;''  rather than ''σ&lt;sup&gt;2&lt;/sup&gt;'' . In this simple case, the estimation Average Partial Effect (APE)&lt;ref&gt;Wooldridge, J. (2002): Econometric Analysis of Cross Section and Panel Data, MIT Press, Cambridge, Mass, pp 22.&lt;/ref&gt; ''∂'''E''' [ y'' | ''x ] / ∂x&lt;sub&gt;i&lt;/sub&gt;''   can be computed based on those estimates.

If ''v''  is correlated with ''x'',&lt;ref&gt;Wooldridge, J. (2002): Econometric Analysis of Cross Section and Panel Data, MIT Press, Cambridge, Mass, pp 531-533.&lt;/ref&gt; for instance,  ''v'' denotes the advertisement cost which has strong interactive relationship with the demand side, then estimation through the [[Tobit model]] without considering the neglected heterogeneity will cause an [[endogeneity]] issue implicitly. Now, rewrite the model as:

''y&lt;sup&gt;*&lt;/sup&gt; = x&lt;sub&gt;1&lt;/sub&gt;β&lt;sub&gt;1&lt;/sub&gt; + x&lt;sub&gt;2&lt;/sub&gt;β&lt;sub&gt;2&lt;/sub&gt; + γv + u'' ;
''x&lt;sub&gt;2&lt;/sub&gt; = x&lt;sub&gt;1&lt;/sub&gt;δ&lt;sub&gt;1&lt;/sub&gt; + z δ&lt;sub&gt;2&lt;/sub&gt; + η'' .

where  η is a normal error and only correlated only with ''v'' . Then ''v''  can be represented as ''v = θη + ε'' where  ε is independent with ''u'' . Then, the model can be rewritten as:
 
''y&lt;sup&gt;*&lt;/sup&gt; = x&lt;sub&gt;1&lt;/sub&gt;β&lt;sub&gt;1&lt;/sub&gt; + x&lt;sub&gt;2&lt;/sub&gt;β&lt;sub&gt;2&lt;/sub&gt; + γθη + γε + u '' ;

''x&lt;sub&gt;2&lt;/sub&gt; = x&lt;sub&gt;1&lt;/sub&gt;δ&lt;sub&gt;1&lt;/sub&gt; + z δ&lt;sub&gt;2&lt;/sub&gt; + η''  .

or more succinctly

''y&lt;sup&gt;*&lt;/sup&gt; = x&lt;sub&gt;1&lt;/sub&gt;β&lt;sub&gt;1&lt;/sub&gt; + x&lt;sub&gt;2&lt;/sub&gt;β&lt;sub&gt;2&lt;/sub&gt; + γθη + ũ '';

''x&lt;sub&gt;2&lt;/sub&gt; = x&lt;sub&gt;1&lt;/sub&gt;δ&lt;sub&gt;1&lt;/sub&gt; + z δ&lt;sub&gt;2&lt;/sub&gt; + η'' .

where &lt;math&gt; \tilde {u} \mid x, z \sim (0, \tilde {\sigma}^2) &lt;/math&gt; . Then the coefficients  ''β, δ , γθ''  and &lt;math&gt; \tilde {\sigma}^2 &lt;/math&gt;  can be consistently estimated by a 2-stage procedure:

(1)	 Regress  ''x&lt;sub&gt;2&lt;/sub&gt;'' on ''x&lt;sub&gt;1&lt;/sub&gt;''  and ''z'' , obtain the estimate for the coefficient ''&lt;math&gt; \hat {\delta} &lt;/math&gt;''  and the residual ''&lt;math&gt; \hat {\eta} &lt;/math&gt;'' ;

(2)	Run the [[Tobit model]] of ''y''  on ''x''  and ''&lt;math&gt; \hat {\eta} &lt;/math&gt;''  , obtain the estimate for the coefficients ''β'' , ''γθ''  as well as the &lt;math&gt; \tilde {\sigma}^2 &lt;/math&gt; .

The APE can be computed correspondingly. The key idea here is to use the first-stage regression to purify the error term and then estimate the model without the endogeneity issue based on the assumption about the correlation structure. Another important assumption here is the normality distributional assumption, which cannot be relaxed under the Tobit model framework.

{{Reflist}}

{{uncategorized|date=November 2015}}</text>
      <sha1>18f0tw7em54lmcdtddiuyv310bp61pk</sha1>
    </revision>
  </page>
  <page>
    <title>Quantile regression averaging</title>
    <ns>0</ns>
    <id>48678962</id>
    <revision>
      <id>693232766</id>
      <parentid>693160196</parentid>
      <timestamp>2015-12-01T06:35:24Z</timestamp>
      <contributor>
        <username>BG19bot</username>
        <id>14508071</id>
      </contributor>
      <minor />
      <comment>[[WP:CHECKWIKI]] error fix for #61.  Punctuation goes before References. Do [[Wikipedia:GENFIXES|general fixes]] if a problem exists. - using [[Project:AWB|AWB]] (11751)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve">'''Quantile Regression Averaging (QRA)''' is a [[Consensus forecast|forecast combination]] approach to the computation of [[prediction interval]]s. It involves applying [[quantile regression]] to the point forecasts of a small number of individual forecasting models or experts. It has been introduced in 2014 by Jakub Nowotarski and Rafał Weron&lt;ref&gt;{{Cite journal|title = Computing electricity spot price prediction intervals using quantile regression and forecast averaging|url = http://link.springer.com/article/10.1007/s00180-014-0523-0|journal = Computational Statistics|date = 2015|issn = 0943-4062|pages = 791–803|volume = 30|issue = 3|doi = 10.1007/s00180-014-0523-0|first = Jakub|last = Nowotarski|first2 = Rafał|last2 = Weron|separator = |others = [Open Access]}}&lt;/ref&gt; and originally used for [[probabilistic forecasting]] of electricity prices&lt;ref&gt;{{Cite journal|title = Electricity price forecasting: A review of the state-of-the-art with a look into the future|url = http://www.sciencedirect.com/science/article/pii/S0169207014001083|journal = International Journal of Forecasting|date = 2014|pages = 1030–1081|volume = 30|issue = 4|doi = 10.1016/j.ijforecast.2014.08.008|first = Rafał|last = Weron|others = [Open Access]}}&lt;/ref&gt;&lt;ref name=&quot;:0&quot;&gt;{{Cite journal|title = Probabilistic forecasting of electricity spot prices using Factor Quantile Regression Averaging|url = http://www.sciencedirect.com/science/article/pii/S0169207014001848|journal = International Journal of Forecasting|doi = 10.1016/j.ijforecast.2014.12.004|first = Katarzyna|last = Maciejowska|first2 = Jakub|last2 = Nowotarski|first3 = Rafał|last3 = Weron|date = 2016}}&lt;/ref&gt; and loads.&lt;ref&gt;{{Cite journal|title = Probabilistic Load Forecasting via Quantile Regression Averaging on Sister Forecasts|url = http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7137662|journal = IEEE Transactions on Smart Grid|date = 2015|issn = 1949-3053|pages = 1-1|volume = PP|issue = 99|doi = 10.1109/TSG.2015.2437877|first = B.|last = Liu|first2 = J.|last2 = Nowotarski|first3 = T.|last3 = Hong|first4 = R.|last4 = Weron}}&lt;/ref&gt;&lt;ref&gt;{{Cite web|title = Probabilistic Electric Load Forecasting: A Tutorial Review|url = http://blog.drhongtao.com/2015/08/probabilistic-electric-load-forecasting-a-tutorial-review.html|website = blog.drhongtao.com|accessdate = 2015-11-28|last = Hong|first = Tao|last2 = Fan|first2 = Shu}}&lt;/ref&gt; Despite its simplicity it has been found to perform extremely well in practice - the top two performing teams in the ''price track'' of the [[Global Energy Forecasting Competition]] (GEFCom2014) used variants of QRA.&lt;ref&gt;{{Cite web|title = Semi-parametric models and robust aggregation for GEFCom2014 probabilistic electric load and electricity price forecasting|url = http://www.researchgate.net/publication/279212563_Semi-parametric_models_and_robust_aggregation_for_GEFCom2014_probabilistic_electric_load_and_electricity_price_forecasting|website = ResearchGate|accessdate = 2015-11-28|last = Gaillard|first = Pierre|last2 = Goude|first2 = Yannig|last3 = Nedellec|first3 = Raphael}}&lt;/ref&gt;&lt;ref&gt;{{Cite journal|title = A hybrid model for GEFCom2014 probabilistic electricity price forecasting|url = https://ideas.repec.org/p/wuu/wpaper/hsc1506.html|date = 2015|first = Katarzyna|last = Maciejowska|first2 = Jakub|last2 = Nowotarski|series = HSC Research Report, HSC/15/06}}&lt;/ref&gt;

== Introduction ==
The individual point forecasts are used as [[independent variables]] and the corresponding observed target variable as the [[dependent variable]] in a standard [[quantile regression]] setting.&lt;ref&gt;{{Cite book|title = Quantile Regresssion|url = http://onlinelibrary.wiley.com/doi/10.1002/9780470057339.vnn091/abstract|publisher = John Wiley &amp; Sons, Ltd|date = 2005|isbn = 9780470057339|doi = 10.1002/9780470057339.vnn091|first = Roger|last = Koenker}}&lt;/ref&gt; The Quantile Regression Averaging method yields an interval forecast of the target variable, but does not use the prediction intervals of the individual methods. One of the reasons for using point forecasts (and not interval forecasts) is their availability. For years, forecasters have focused on obtaining accurate point predictions. Computing [[Probabilistic forecasting|probabilistic forecasts]], on the other hand, is generally a much more complex task and has not been discussed in the literature nor developed by practitioners so extensively. Therefore QRA may be found particularly attractive from a practical point of view as it allows to leverage existing development of point forecasting.

== Computation ==
[[File:QRA.png|thumb|Visualization of the Quantile Regression Averaging (QRA) probabilistic forecasting technique.]]
The [[quantile regression]] problem can be written as follows:

&lt;math&gt;Q_y(q|X_t) = X_t\beta_q&lt;/math&gt;,

where &lt;math&gt;Q_y(q | \cdot)&lt;/math&gt; is the conditional ''q''-th [[quantile]] of the dependent variable (&lt;math&gt;y_t&lt;/math&gt;),  &lt;math&gt;X_t=[1,\hat{y}_{1,t},...,\hat{y}_{m,t}]&lt;/math&gt; is a vector of point forecasts of &lt;math&gt;m&lt;/math&gt; individual models (i.e. independent variables) and ''β&lt;sub&gt;q&lt;/sub&gt;'' is a vector of parameters (for quantile ''q''). The parameters are estimated by minimizing the loss function for a particular ''q''-th quantile:

&lt;math&gt;\min\limits_{\beta_q}
\left[ \sum\limits_{\{t:y_t \geq X_t\beta_q \}} q |y_t - X_t\beta_q | +
\sum\limits_{\{t:y_t &lt; X_t\beta_q \}} (1-q)|y_t - X_t\beta_q |\right] = \min\limits_{\beta_q} \left[ \sum\limits_{t}(q - \mathbf{1}_{y_t &lt; X_t\beta_q }) (y_t - X_t\beta_q ) \right]&lt;/math&gt;

QRA assigns weights to individual forecasting methods and combines them to yield forecasts of chosen quantiles. Although the QRA method is based on quantile regression, not [[least squares]], it still suffers from the same problems: the exogenous variables should not be correlated strongly and the number of variables included in the model has to be relatively small in order for the method to be computationally efficient.

== Factor Quantile Regression Averaging (FQRA) ==
[[File:FQRA.png|thumb|Visualization of the Factor Quantile Regression Averaging (FQRA) probabilistic forecasting technique.]]
The main difficulty associated with applying QRA comes from the fact that only individual models that perform well and (preferably) are distinct should be used. However, there may be many well performing models or many different specifications of each model (with or without exogenous variables, with all or only selected lags, etc.) and it may not be optimal to include all of them in Quantile Regression Averaging.

In '''Factor Quantile Regression Averaging (FQRA)''',&lt;ref name=&quot;:0&quot; /&gt; instead of selecting individual models ''a priori'', the relevant information contained in all forecasting models at hand is extracted using [[principal component analysis]] (PCA). The [[prediction interval]]s are then constructed on the basis of the common factors (&lt;math&gt;f_t&lt;/math&gt;) obtained from the panel of point forecasts, as independent variables in a quantile regression. More precisely, in the FQRA method &lt;math&gt;X_t=[1,\hat{f}_{1,t},...,\hat{f}_{k,t}]&lt;/math&gt; is a vector of &lt;math&gt;k&lt;m&lt;/math&gt; factors extracted from a panel of point forecasts of &lt;math&gt;m&lt;/math&gt; individual models, not a vector of point forecasts of the individual models themselves. A similar principal component-type approach was proposed in the context of obtaining point forecasts from the [[Survey of Professional Forecasters]] data.&lt;ref&gt;{{Cite journal|title = Forecast combination through dimension reduction techniques|url = http://www.sciencedirect.com/science/article/pii/S0169207010000221|journal = International Journal of Forecasting|date = 2011|pages = 224–237|volume = 27|issue = 2|doi = 10.1016/j.ijforecast.2010.01.012|first = Pilar|last = Poncela|first2 = Julio|last2 = Rodríguez|first3 = Rocío|last3 = Sánchez-Mangas|first4 = Eva|last4 = Senra}}&lt;/ref&gt;

Instead of considering a (large) panel of forecasts of the individual models, FQRA concentrates on a small number of common factors, which - by construction - are orthogonal to each other, and hence are contemporaneously uncorrelated. FQRA can be also interpreted as a [[Consensus forecast|forecast averaging]] approach. The factors estimated within PCA are linear combinations of individual vectors of the panel and FQRA can therefore be used to assign weights to the forecasting models directly.

== QRA and LAD regression ==
QRA may be viewed as an extension of combining point forecasts. The well-known [[ordinary least squares]] (OLS) averaging&lt;ref&gt;{{Cite journal|title = Improved methods of combining forecasts|url = http://onlinelibrary.wiley.com/doi/10.1002/for.3980030207/abstract|journal = Journal of Forecasting|date = 1984|issn = 1099-131X|pages = 197–204|volume = 3|issue = 2|doi = 10.1002/for.3980030207|first = Clive W. J.|last = Granger|first2 = Ramu|last2 = Ramanathan}}&lt;/ref&gt; uses linear regression to estimate weights of the point forecasts of individual models. Replacing the quadratic loss function with the absolute loss function leads to quantile regression for the median, or in other words, [[Least absolute deviations|least absolute deviation (LAD) regression]].&lt;ref&gt;{{Cite journal|title = An empirical comparison of alternative schemes for combining electricity spot price forecasts|url = http://www.sciencedirect.com/science/article/pii/S0140988314001716|journal = Energy Economics|date = 2014|pages = 395–412|volume = 46|doi = 10.1016/j.eneco.2014.07.014|first = Jakub|last = Nowotarski|first2 = Eran|last2 = Raviv|first3 = Stefan|last3 = Trück|first4 = Rafał|last4 = Weron}}&lt;/ref&gt;

== See also ==
* [[Consensus forecast]]'', also known as combining forecasts'', ''forecast averaging'' or ''model averaging'' (in econometrics and statistics) and ''[[committee machine]]s'', ''[[ensemble averaging]]'' or ''expert aggregation'' (in machine learning)
* [[Electricity price forecasting]]
* [[Energy forecasting]]
* [[Forecasting]]
* [[Global Energy Forecasting Competition]]s
* [[Economic forecasting]]
* [[Prediction interval]]
* [[Probabilistic forecasting]]
* [[Quantile regression]]

== Implementations ==
* Matlab code for computing interval forecasts using QRA is available from RePEc: https://ideas.repec.org/c/wuu/hscode/m14003.html

== References ==
&lt;references /&gt;

[[Category:Prediction]]
[[Category:Economic forecasting]]
[[Category:Econometrics]]
[[Category:Regression analysis]]
[[Category:Probability assessment]]</text>
      <sha1>08q2n4obub5ahi6zwu3hhcs8y98mmwx</sha1>
    </revision>
  </page>
  <page>
    <title>Maximum score estimator</title>
    <ns>0</ns>
    <id>48681293</id>
    <revision>
      <id>693456365</id>
      <parentid>693373774</parentid>
      <timestamp>2015-12-02T18:15:56Z</timestamp>
      <contributor>
        <username>Postcard Cathy</username>
        <id>1744116</id>
      </contributor>
      <comment>added [[Category:Econometrics]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve">When modelling discrete choice model, it is always assumed that the choice is determined by the comparison of the underlying latent utility &lt;ref&gt;For more example, refer to: Smith, Michael D. and Brynjolfsson, Erik, Consumer Decision-Making at an Internet Shopbot (October 2001). MIT Sloan School of Management Working Paper No. 4206-01.&lt;/ref&gt;. Denote the population of the agents as &lt;math&gt; T &lt;/math&gt;, the common choice set for each agent as &lt;math&gt; C &lt;/math&gt; . For agent &lt;math&gt; t \in T &lt;/math&gt; , denote her choice as &lt;math&gt; y_{t,i} &lt;/math&gt; , which is equal to 1 if choice &lt;math&gt; i &lt;/math&gt;  is chosen and 0 otherwise.  Assume the latent utility is linear with the parameters and the error term is additive, then for an agent &lt;math&gt; t \in T &lt;/math&gt; ,

&lt;math&gt; y_{t,i} = 1 \leftrightarrow x_{t,i}\beta + \epsilon_{t,i} &gt; x_{i,j}\beta + \epsilon_{t,j} , \forall j \neq i&lt;/math&gt; and &lt;math&gt;j \in C &lt;/math&gt;

where &lt;math&gt; x_{t,i} &lt;/math&gt; and &lt;math&gt; x_{t,j} &lt;/math&gt;  are the &lt;math&gt; q &lt;/math&gt; -dimensional observable covariates about the agent and the choice, and  &lt;math&gt;\epsilon_{t,i} &lt;/math&gt;  and  &lt;math&gt; \epsilon_{t,j} &lt;/math&gt; are the decision errors caused by some cognitive reasons or information incompleteness. The construction of the observable covariates is very general. For instance, if  &lt;math&gt; C &lt;/math&gt; is a set of different brands of coffee, then &lt;math&gt; x_{t,i} &lt;/math&gt;  includes the characteristics both of the agent &lt;math&gt; t &lt;/math&gt;  , such as age, gender, income and ethnicity, and of the coffee &lt;math&gt; i &lt;/math&gt;  , such as price, taste and whether it is local or imported. All of the error terms are assumed i.i.d and we need estimate  &lt;math&gt; \beta &lt;/math&gt; which characterize the effect of different factors on the agent’s choice.

Usually some specific distribution assumption on the error term is imposed, such that the parameter  &lt;math&gt; \beta &lt;/math&gt; is [[estimated parametrically]]. For instance, if the distribution of error term is assumed to be normal, then the model is just a [[multinomial probit model]] &lt;ref&gt;Wooldridge, J. (2002): Econometric Analysis of Cross Section and Panel Data, MIT Press, Cambridge, Mass, pp 457-460.&lt;/ref&gt; ; if it is assumed to be an extreme value distribution, then the model becomes a [[multinomial logit model]]. The [[parametric model]] &lt;ref&gt;For an concrete example, refer to: Tetsuo Yai, Seiji Iwakura, Shigeru Morichi, Multinomial probit with structured covariance for route choice behavior, Transportation Research Part B: Methodological, Volume 31, Issue 3, June 1997, Pages 195-207, ISSN 0191-2615&lt;/ref&gt;  is convenient for computation but might not be [[consistent]] once the distribution of the error term is misspecified &lt;ref&gt;Jin Yan (2012), “A Smoothed Maximum Score Estimator for Multinomial Discrete Choice Models”, Working Paper.&lt;/ref&gt; .

To make the estimator more robust to the distributional assumption, Manski (1975) proposed a [[non-parametric model]] to estimate the parameters. In this model, denote the number of the elements of the choice set as &lt;math&gt; J &lt;/math&gt; , the total number of the agents as &lt;math&gt;N&lt;/math&gt;, and &lt;math&gt; W (J -1) &gt; W (J - 2) &gt; \dots &gt; W (1) &gt; W (0) &lt;/math&gt;  is a sequence of real numbers. The Maximum Score Estimator &lt;ref&gt;Charles F. Manski (1975), “Maximum Score Estimation of the Stochastic Utility Model of Choice”, Journal of Econometrics 3, pp. 205-228.&lt;/ref&gt; is defined as:

&lt;math&gt; \hat{b}={\operatorname{arg\max}}_b \frac{1}{N} \sum_{t=1}^N \sum_{i=1}^J y_{t,i} W (\sum\nolimits_{j \in C, j \neq i} 1 (x_{t,i}b &gt; x_{t,j}b))&lt;/math&gt;

Here, &lt;math&gt; \sum\nolimits_{j \in C, j \neq i} 1 (x_{t,i}b &gt; x_{t,j}b)&lt;/math&gt;    is the ranking of the certainty part of the underlying utility of choosing &lt;math&gt; i &lt;/math&gt; . The intuition in this model is that the ranking is higher, the more weight will be assigned to the choice, based on which, the optimization objective function similar to the likelihood function in parametric model is constructed. For more about the consistency and asymptotic property about the maximum score estimator, refer to Manski (1975).

== References ==
{{Reflist}}

[[Category:Econometrics]]</text>
      <sha1>oaj3kjuh6988dam85ezwn466lasxyae</sha1>
    </revision>
  </page>
  <page>
    <title>Electricity price forecasting</title>
    <ns>0</ns>
    <id>48684895</id>
    <revision>
      <id>693392108</id>
      <parentid>693345371</parentid>
      <timestamp>2015-12-02T07:23:43Z</timestamp>
      <contributor>
        <username>BG19bot</username>
        <id>14508071</id>
      </contributor>
      <minor />
      <comment>[[WP:CHECKWIKI]] error fix for #61.  Punctuation goes before References. Do [[Wikipedia:GENFIXES|general fixes]] if a problem exists. - using [[Project:AWB|AWB]] (11751)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve">'''Electricity price forecasting (EPF)''' is a branch of [[energy forecasting]] which focuses on predicting the [[Spot contract|spot]] and [[forward price]]s in wholesale [[electricity market]]s. Over the last 15 years electricity price forecasts have become a fundamental input to energy companies’ decision-making mechanisms at the corporate level.

== Background ==
Since the early 1990s, the process of [[deregulation]] and the introduction of [[Electricity market|competitive electricity markets]] have been reshaping the landscape of the traditionally monopolistic and government-controlled power sectors. Throughout Europe, North America and Australia, electricity is now traded under market rules using [[Spot contract|spot]] and [[Derivative (finance)|derivative contracts]].&lt;ref name=&quot;:0&quot;&gt;{{Cite journal|title = Electricity price forecasting: A review of the state-of-the-art with a look into the future|url = http://www.sciencedirect.com/science/article/pii/S0169207014001083|journal = International Journal of Forecasting|date = 2014|pages = 1030–1081|volume = 30|issue = 4|doi = 10.1016/j.ijforecast.2014.08.008|first = Rafał|last = Weron|others = [Open Access]}}&lt;/ref&gt;&lt;ref name=&quot;:1&quot;&gt;{{Cite book|title = Modelling Prices in Competitive Electricity Markets|publisher = Wiley|year = 2004|isbn = 978-0-470-84860-9|location = |pages = |editor-last = Bunn|editor-first = Derek W.|url = http://eu.wiley.com/WileyCDA/WileyTitle/productCd-047084860X.html}}&lt;/ref&gt; However, electricity is a very special commodity: it is economically non-storable and power system stability requires a constant balance between production and consumption. At the same time, electricity demand depends on weather (temperature, wind speed, precipitation, etc.) and the intensity of business and everyday activities ([[Load profile|on-peak vs. off-peak hours]], weekdays vs. weekends, holidays, etc.). These unique characteristics lead to price dynamics not observed in any other market, exhibiting daily, weekly and often annual [[seasonality]] and abrupt, short-lived and generally unanticipated [[price spike]]s.

Extreme [[Volatility (finance)|price volatility]], which can be up to two orders of magnitude higher than that of any other commodity or financial asset, has forced market participants to hedge not only volume but also price risk. Price forecasts from a few hours to a few months ahead have become of particular interest to power portfolio managers. A power market company able to forecast the volatile wholesale prices with a reasonable level of accuracy can adjust its bidding strategy and its own production or consumption schedule in order to reduce the risk or maximize the profits in day-ahead trading.&lt;ref name=&quot;:2&quot;&gt;{{Cite book|title = Modeling and Forecasting Electricity Loads and Prices: A Statistical Approach|last = Weron|first = Rafał|publisher = Wiley|year = 2006|isbn = 978-0-470-05753-7|location = |pages = |url = http://eu.wiley.com/WileyCDA/WileyTitle/productCd-047005753X.html}}&lt;/ref&gt; A ballpark estimate of savings from a 1% reduction in the [[mean absolute percentage error]] (MAPE) of short-term price forecasts is $300,000 per year for a [[Public utility|utility]] with 1GW [[Load profile|peak load]].&lt;ref&gt;{{Cite journal|url = http://www.energybiz.com/magazine/article/404587/crystal-ball-lessons-predictive-analytics|title = Crystal Ball Lessons in Predictive Analytics|last = Hong|first = Tao|date = 2015|journal = EnergyBiz Magazine|doi = |pmid = |access-date = |volume = Spring|pages = 35–37}}&lt;/ref&gt;

== Taxonomy of modeling approaches ==
[[File:EPF taxonomy wiki v2.png|thumb|A taxonomy of electricity price forecasting (EPF) and modeling approaches according to Weron (2014).]]
A variety of methods and ideas have been tried for EPF over the last 15 years, with varying degrees of success. They can be broadly classified into six groups.&lt;ref name=&quot;:0&quot; /&gt;

=== Multi-agent models ===
''Multi-agent'' ([[Agent-based model|''multi-agent simulation'']]'', equilibrium, [[Game theory|game theoretic]]'') models simulate the operation of a system of heterogeneous agents (generating units, companies) interacting with each other, and build the price process by matching the demand and supply in the market.&lt;ref&gt;{{Cite journal|title = Electricity market modeling trends|url = http://www.sciencedirect.com/science/article/pii/S0301421503003161|journal = Energy Policy|date = 2005|pages = 897–913|volume = 33|issue = 7|doi = 10.1016/j.enpol.2003.10.013|first = Mariano|last = Ventosa|first2 = Álvaro|last2 = Baı́llo|first3 = Andrés|last3 = Ramos|first4 = Michel|last4 = Rivier}}&lt;/ref&gt; This class includes ''cost-based models'' (or ''production-cost models'', PCM),&lt;ref&gt;{{Cite book|title = Power Generation, Operation and Control|last = Wood|first = A.J.|publisher = Wiley|year = 1996|isbn = |location = |pages = |last2 = Wollenberg|first2 = B.F.}}&lt;/ref&gt; ''equilibrium'' or ''game theoretic'' approaches (like the Nash-Cournot framework, supply function equilibrium - SFE, strategic production-cost models - SPCM)&lt;ref&gt;{{Cite journal|title = Forecasting the Mean and the Variance of Electricity Prices in Deregulated Markets|url = http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4414354|journal = IEEE Transactions on Power Systems|date = 2008|issn = 0885-8950|pages = 25–32|volume = 23|issue = 1|doi = 10.1109/TPWRS.2007.913195|first = C.M.|last = Ruibal|first2 = M.|last2 = Mazumdar}}&lt;/ref&gt;&lt;ref&gt;{{Cite journal|title = Stochastic models for bidding strategies on oligopoly electricity market|url = http://link.springer.com/article/10.1007/s00186-008-0252-7|journal = Mathematical Methods of Operations Research|date = 2009|issn = 1432-2994|pages = 579–592|volume = 69|issue = 3|doi = 10.1007/s00186-008-0252-7|first = Magdalena|last = Borgosz-Koczwara|first2 = Aleksander|last2 = Weron|first3 = Agnieszka|last3 = Wyłomańska}}&lt;/ref&gt;&lt;ref&gt;{{Cite journal|title = A strategic production costing model for electricity market price analysis|url = http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1388494|journal = IEEE Transactions on Power Systems|date = 2005|issn = 0885-8950|pages = 67–74|volume = 20|issue = 1|doi = 10.1109/TPWRS.2004.831266|first = Carlos|last = Batlle|first2 = J.|last2 = Barquin}}&lt;/ref&gt; and [[agent-based model]]s.&lt;ref&gt;{{Cite journal|title = Learning Agents in an Artificial Power Exchange: Tacit Collusion, Market Power and Efficiency of Two Double-auction Mechanisms|url = http://link.springer.com/article/10.1007/s10614-008-9127-5|journal = Computational Economics|date = 2008|issn = 0927-7099|pages = 73–98|volume = 32|issue = 1-2|doi = 10.1007/s10614-008-9127-5|first = Eric|last = Guerci|first2 = Stefano|last2 = Ivaldi|first3 = Silvano|last3 = Cincotti}}&lt;/ref&gt;

Multi-agent models generally focus on qualitative issues rather than quantitative results. They may provide insights as to whether or not prices will be above marginal costs, and how this might influence the players’ outcomes. However, they pose problems if more quantitative conclusions have to be drawn, particularly if electricity prices have to be predicted with a high level of precision.

=== Fundamental models ===
''Fundamental'' (''structural'') methods try to capture the basic physical and economic relationships which are present in the production and trading of electricity.&lt;ref name=&quot;:3&quot;&gt;{{Cite book|title = Managing Energy Risk: An Integrated View on Power and Other Energy Markets|url = http://doi.wiley.com/10.1002/9781119209102|doi = 10.1002/9781119209102|last = Burger|first = M.|last2 = Graeber|first2 = B.|last3 = Schindlmayr|first3 = G.|publisher = Wiley|year = 2007}}&lt;/ref&gt; The functional associations between fundamental drivers (loads, weather conditions, system parameters, etc.) are postulated, and the fundamental inputs are modeled and predicted independently, often via statistical, reduced-form or [[computational intelligence]] techniques. In general, two subclasses of fundamental models can be identified: ''parameter rich models''&lt;ref name=&quot;:4&quot;&gt;{{Cite book|title = Energy and Power Risk Management: New Developments in Modeling, Pricing, and Hedging|last = Eydeland|first = Alexander|publisher = Wiley|year = 2003|isbn = 978-0-471-10400-1|location = |pages = |last2 = Wolyniec|first2 = Krzysztof|url = http://eu.wiley.com/WileyCDA/WileyTitle/productCd-0471104000.html}}&lt;/ref&gt; and ''parsimonious structural models''&lt;ref&gt;{{Cite book|title = A Survey of Commodity Markets and Structural Models for Electricity Prices|url = http://link.springer.com/chapter/10.1007/978-1-4614-7248-3_2|publisher = Springer New York|date = 2014|isbn = 978-1-4614-7247-6|pages = 41–83|doi = 10.1007/978-1-4614-7248-3_2|first = René|last = Carmona|first2 = Michael|last2 = Coulon|editor-first = Fred Espen|editor-last = Benth|editor-first2 = Valery A.|editor-last2 = Kholodnyi|editor-first3 = Peter|editor-last3 = Laurence}}&lt;/ref&gt; of supply and demand.

Two major challenges arise in the practical implementation of fundamental models: data availability and incorporation of stochastic fluctuations of the fundamental drivers. In building the model, we make specific assumptions about physical and economic relationships in the marketplace, and therefore the price projections generated by the models are very sensitive to violations of these assumptions.

=== Reduced-form models ===
''Reduced-form'' (''quantitative, [[Stochastic model|stochastic]]'') models characterize the statistical properties of electricity prices over time, with the ultimate objective of [[Derivative (finance)|derivatives valuation]] and [[risk management]].&lt;ref name=&quot;:1&quot; /&gt;&lt;ref name=&quot;:2&quot; /&gt;&lt;ref name=&quot;:3&quot; /&gt; Their main intention is not to provide accurate hourly price forecasts, but rather to replicate the main characteristics of daily electricity prices, like [[marginal distribution]]s at future time points, price dynamics, and correlations between commodity prices. If the price process chosen is not appropriate for capturing the main properties of electricity prices, the results from the model are likely to be unreliable. However, if the model is too complex, the computational burden will prevent its use on-line in trading departments. Depending on the type of market under consideration, reduced-form models can be classified as:
* ''Spot price models'', which provide a parsimonious representation of the dynamics of spot prices. Their main drawback is the problem of pricing derivatives, i.e., the identification of the risk premium linking spot and forward prices.&lt;ref&gt;{{Cite journal|title = Revisiting the relationship between spot and futures prices in the Nord Pool electricity market|url = http://www.sciencedirect.com/science/article/pii/S0140988314000541|journal = Energy Economics|date = 2014|pages = 178–190|volume = 44|doi = 10.1016/j.eneco.2014.03.007|first = Rafał|last = Weron|first2 = Michał|last2 = Zator}}&lt;/ref&gt; The two most popular subclasses include [[Jump diffusion|jump-diffusion]]&lt;ref&gt;{{Cite journal|title = Market price of risk implied by Asian-style electricity options and futures|url = http://www.sciencedirect.com/science/article/pii/S014098830700076X|journal = Energy Economics|date = 2008|pages = 1098–1115|volume = 30|issue = 3|doi = 10.1016/j.eneco.2007.05.004|first = Rafał|last = Weron}}&lt;/ref&gt;&lt;ref&gt;{{Cite journal|title = A critical empirical study of three electricity spot price models|url = http://www.sciencedirect.com/science/article/pii/S0140988311002866|journal = Energy Economics|date = 2012|pages = 1589–1616|volume = 34|issue = 5|doi = 10.1016/j.eneco.2011.11.012|first = Fred Espen|last = Benth|first2 = Rüdiger|last2 = Kiesel|first3 = Anna|last3 = Nazarova}}&lt;/ref&gt; and [[Markov regime-switching]]&lt;ref&gt;{{Cite journal|title = An empirical comparison of alternate regime-switching models for electricity spot prices|url = http://www.sciencedirect.com/science/article/pii/S0140988310000885|journal = Energy Economics|date = 2010|pages = 1059–1073|volume = 32|issue = 5|doi = 10.1016/j.eneco.2010.05.008|first = Joanna|last = Janczura|first2 = Rafal|last2 = Weron}}&lt;/ref&gt; models.
* ''Forward price models'' allow for the pricing of derivatives in a straightforward manner (but only of those written on the forward price of electricity). However, they too have their limitations; most importantly, the lack of data that can be used for calibration and the inability to derive the properties of spot prices from the analysis of forward curves.&lt;ref name=&quot;:4&quot; /&gt;&lt;ref&gt;{{Cite book|title = Stochastic Modeling of Electricity and Related Markets|url = http://www.worldscientific.com/worldscibooks/10.1142/6811|doi = 10.1142/6811|first = Fred Espen|last = Benth|first2 = Jūratė Šaltytė|last2 = Benth|first3 = Steen|last3 = Koekebakker|publisher = World Scientific|year = 2008}}&lt;/ref&gt;

=== Statistical models ===
''Statistical'' (''[[Econometric model|econometric]], [[technical analysis]]'') methods forecast the current price by using a mathematical combination of the previous prices and/or previous or current values of exogenous factors, typically consumption and production figures, or weather variables.&lt;ref name=&quot;:0&quot; /&gt; The two most important categories are ''additive'' and ''multiplicative'' models. They differ in whether the predicted price is the sum (additive) of a number of components or the product (multiplicative) of a number of factors. The former are far more popular, but the two are closely related - a multiplicative model for prices can be transformed into an additive model for log-prices. Statistical models are attractive because some physical interpretation may be attached to their components, thus allowing engineers and system operators to understand their behavior. They are often criticized for their limited ability to model the (usually) nonlinear behavior of electricity prices and related fundamental variables. However, in practical applications, their performances are not worse than those of the non-linear [[computational intelligence]] methods (see below). For instance, in the ''load forecasting track'' of the [[Global Energy Forecasting Competition|Global Energy Forecasting Competition (GEFCom2012)]] attracting hundreds of participants worldwide, the top four winning entries used regression-type models.
[[File:EPF ANN wiki.png|thumb|A taxonomy of the [[artificial neural network]] architectures that are most popular in EPF (see Weron, 2014). Input nodes are denoted by filled circles, output nodes by empty circles, and nodes in the hidden layer by empty circles with a dashed outline. The [[activation function]]s for [[Radial basis function network|RBF networks]] are radial basis functions, whereas [[Multi-layer perceptron|multi-layer perceptrons (MLP)]] typically use piecewise linear or sigmoid activation functions (illustrated in circles).]]
Statistical models constitute a very rich class which includes:
* Similar-day and [[exponential smoothing]]&lt;ref name=&quot;:5&quot;&gt;{{Cite journal|title = Forecasting Electricity Spot Prices Accounting for Wind Power Predictions|url = http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6313966|journal = IEEE Transactions on Sustainable Energy|date = 2013|issn = 1949-3029|pages = 210–218|volume = 4|issue = 1|doi = 10.1109/TSTE.2012.2212731|first = T.|last = Jonsson|first2 = P.|last2 = Pinson|first3 = H.A.|last3 = Nielsen|first4 = H.|last4 = Madsen|first5 = T.S.|last5 = Nielsen}}&lt;/ref&gt; methods.
* [[Regression model]]s.&lt;ref&gt;{{Cite journal|title = Forecasting electricity prices: The impact of fundamentals and time-varying coefficients|url = http://www.sciencedirect.com/science/article/pii/S0169207008001076|journal = International Journal of Forecasting|date = 2008|pages = 764–785|volume = 24|series = Energy Forecasting|issue = 4|doi = 10.1016/j.ijforecast.2008.09.008|first = Nektaria V.|last = Karakatsani|first2 = Derek W.|last2 = Bunn}}&lt;/ref&gt;
* Time series models without ([[Autoregressive model|AR]], [[Autoregressive–moving-average model|ARMA]], [[Autoregressive integrated moving average|ARIMA]], [[Autoregressive fractionally integrated moving average|Fractional ARIMA - FARIMA]], Seasonal ARIMA - SARIMA, Threshold AR - TAR) and with [[exogenous variables]] (ARX, ARMAX, ARIMAX, SARIMAX, TARX).&lt;ref name=&quot;:2&quot; /&gt;&lt;ref name=&quot;:5&quot; /&gt;&lt;ref name=&quot;:6&quot;&gt;{{Cite journal|title = Forecasting electricity prices for a day-ahead pool-based electric energy market|url = http://www.sciencedirect.com/science/article/pii/S0169207004001311|journal = International Journal of Forecasting|date = 2005|pages = 435–462|volume = 21|issue = 3|doi = 10.1016/j.ijforecast.2004.12.005|first = Antonio J.|last = Conejo|first2 = Javier|last2 = Contreras|first3 = Rosa|last3 = Espínola|first4 = Miguel A.|last4 = Plazas}}&lt;/ref&gt;&lt;ref&gt;{{Cite journal|title = Forecasting spot electricity prices: A comparison of parametric and semiparametric time series models|url = http://www.sciencedirect.com/science/article/pii/S0169207008000952|journal = International Journal of Forecasting|date = 2008|pages = 744–763|volume = 24|series = Energy Forecasting|issue = 4|doi = 10.1016/j.ijforecast.2008.08.004|first = Rafał|last = Weron|first2 = Adam|last2 = Misiorek}}&lt;/ref&gt;&lt;ref&gt;{{Cite book|title = Price-based energy management in competitive
electricity markets|last = Zareipour|first = Hamid|publisher = VDM Verlag Dr. Müller|year = 2008|isbn = |location = |pages = }}&lt;/ref&gt;
* [[Heteroscedasticity|Heteroskedastic]] time series models ([[Autoregressive conditional heteroskedasticity|GARCH]], AR-GARCH).&lt;ref name=&quot;:2&quot; /&gt;&lt;ref&gt;{{Cite journal|title = Periodic Seasonal Reg-ARFIMA–GARCH Models for Daily Electricity Spot Prices|url = http://dx.doi.org/10.1198/016214506000001022|journal = Journal of the American Statistical Association|date = 2007|issn = 0162-1459|pages = 16–27|volume = 102|issue = 477|doi = 10.1198/016214506000001022|first = Siem Jan|last = Koopman|first2 = Marius|last2 = Ooms|first3 = M. Angeles|last3 = Carnero}}&lt;/ref&gt;

=== Computational intelligence models ===
''[[Computational intelligence]]'' (''[[Artificial intelligence|artificial intelligence-based]], [[machine learning]], non-parametric, non-linear statistical'') techniques combine elements of learning, evolution and fuzziness to create approaches that are capable of adapting to complex dynamic systems, and may be regarded as &quot;intelligent&quot; in this sense. [[Artificial neural network]]s,&lt;ref name=&quot;:6&quot; /&gt;&lt;ref name=&quot;:7&quot;&gt;{{Cite journal|title = Day-ahead price forecasting of electricity markets by a new fuzzy neural network|url = http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1626395|journal = IEEE Transactions on Power Systems|date = 2006|issn = 0885-8950|pages = 887–896|volume = 21|issue = 2|doi = 10.1109/TPWRS.2006.873409|first = N.|last = Amjady}}&lt;/ref&gt; [[fuzzy systems]]&lt;ref name=&quot;:7&quot; /&gt;&lt;ref&gt;{{Cite journal|title = Energy price forecasting in the Ontario competitive power system market|url = http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1266590|journal = IEEE Transactions on Power Systems|date = 2004|issn = 0885-8950|pages = 366–374|volume = 19|issue = 1|doi = 10.1109/TPWRS.2003.821470|first = C.P.|last = Rodriguez|first2 = G.J.|last2 = Anders}}&lt;/ref&gt; and [[support vector machine]]s (SVM)&lt;ref&gt;{{Cite journal|title = Mid-term electricity market clearing price forecasting: A hybrid LSSVM and ARMAX approach|url = http://www.sciencedirect.com/science/article/pii/S0142061513001658|journal = International Journal of Electrical Power &amp; Energy Systems|date = 2013|pages = 20–26|volume = 53|doi = 10.1016/j.ijepes.2013.04.006|first = Xing|last = Yan|first2 = Nurul A.|last2 = Chowdhury}}&lt;/ref&gt; are unquestionably the main classes of computational intelligence techniques in EPF. Their major strength is the ability to handle complexity and non-linearity. In general, computational intelligence methods are better at modeling these features of electricity prices than the statistical techniques (see above). At the same time, this flexibility is also their major weakness. The ability to adapt to nonlinear, spiky behaviors will not necessarily result in better point or probabilistic forecasts.

=== Hybrid models ===
Many of the modeling and price forecasting approaches considered in the literature are ''hybrid'' solutions, combining techniques from two or more of the groups listed above. Their classification is non-trivial, if possible at all.

== Forecasting horizons ==
It is customary to talk about short-, medium- and long-term forecasting,&lt;ref name=&quot;:0&quot; /&gt; but there is no consensus in the literature as to what the thresholds should actually be:
* ''Short-term forecasting'' generally involves horizons from a few minutes up to a few days ahead, and is of prime importance in day-to-day market operations.
* ''Medium-term'' ''forecasting'', from a few days to a few months ahead, is generally preferred for [[balance sheet]] calculations, [[risk management]] and [[Derivative (finance)|derivatives pricing]]. In many cases, especially in electricity price forecasting, evaluation is based not on the actual point forecasts, but on the distributions of prices over certain future time periods. As this type of modeling has a long-standing tradition in [[finance]], an inflow of &quot;finance solutions&quot; is observed.
* ''Long-term'' ''forecasting'', with lead times measured in months, quarters or even years, concentrates on [[Valuation (finance)|investment profitability analysis]] and planning, such as determining the future sites or fuel sources of power plants.

== A look into the future of electricity price forecasting ==
In his extensive review paper, Weron&lt;ref name=&quot;:0&quot; /&gt; looks ahead and speculates on the directions EPF will or should take over the next decade or so:

=== Fundamental price drivers and input variables ===
On the one hand, the electricity price exhibits seasonality at the daily and weekly levels, and the annual level to some extent. In ''short-term forecasting'', the latter is usually ignored, but the daily and weekly seasonalities (including a separate treatment of holidays) are of prime importance. In ''mid-term forecasting'', the daily patterns become irrelevant and most EPF models work with average daily prices. However, the long-term trend-cycle component plays a crucial role. Its misspecification can introduce bias, which may lead to a bad estimate of the the mean reversion level or of the price spike intensity and severity, and consequently, to underestimating the risk.&lt;ref&gt;{{Cite journal|title = Identifying spikes and seasonal components in electricity spot price data: A guide to robust modeling|url = http://www.sciencedirect.com/science/article/pii/S0140988313000625|journal = Energy Economics|date = 2013|pages = 96–110|volume = 38|doi = 10.1016/j.eneco.2013.03.013|first = Joanna|last = Janczura|first2 = Stefan|last2 = Trück|first3 = Rafał|last3 = Weron|first4 = Rodney C.|last4 = Wolff}}&lt;/ref&gt; Finally, in the ''long term'', when the time horizon is measured in years, the daily, weekly and even annual seasonality may be ignored, and long-term trends dominate. Adequate treatment - both in-sample and [[Out of sample testing|out-of-sample]] - of seasonality has not been given enough attention in the literature so far.

On the other hand, the electricity spot price is dependent on a large set of fundamental drivers, including system loads, weather variables, fuel costs, reserve margin (i.e., available generation minus/over predicted demand) and information about scheduled maintenance and forced [[Power outage|outages]]. Although &quot;pure price&quot; models are sometimes used for EPF, in the most common day-ahead forecasting scenario most authors select a combination of these fundamental drivers, based on the heuristics and experience of the forecaster.&lt;ref name=&quot;:10&quot;&gt;{{Cite journal|title = Energy price forecasting - problems and proposals for such predictions|url = http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1597990|journal = IEEE Power and Energy Magazine|date = 2006|issn = 1540-7977|pages = 20–29|volume = 4|issue = 2|doi = 10.1109/MPAE.2006.1597990|first = N.|last = Amjady|first2 = M.|last2 = Hemmati}}&lt;/ref&gt; However, the optimal choice remains an open question. The development of an objective method of selecting a set of the most effective input variables would be very valuable.

=== Spike forecasting and the reserve margin ===
When predicting spike occurrences or spot price volatility, one of the most influential fundamental variables is the [[reserve margin]], also called [[surplus generation]]. It relates the available capacity (generation, supply), &lt;math&gt;C_t&lt;/math&gt;, to the demand (load), &lt;math&gt;D_t&lt;/math&gt;, at a given moment in time &lt;math&gt;t&lt;/math&gt;. The traditional engineering notion of the reserve margin defines it as the difference between the two, i.e., &lt;math&gt;RM = C_t - D_t&lt;/math&gt;, but many authors prefer to work with dimensionless ratios &lt;math&gt;\rho_t=D_t/C_t&lt;/math&gt;, &lt;math&gt;R_t=C_t/D_t -1&lt;/math&gt; or the so-called capacity utilization &lt;math&gt;CU_t=1 - D_t/C_t&lt;/math&gt;.&lt;ref name=&quot;:0&quot; /&gt;  Its rare application in EPF can be justified only by the difficulty of obtaining good quality reserve margin data. Given that more and more system operators (see e.g. http://www.elexon.co.uk) are disclosing such information nowadays, reserve margin data should be playing a significant role in EPF in the near future.

=== Probabilistic forecasts ===
The use of [[prediction interval]]s (PI) and densities, or [[probabilistic forecasting]], has become much more common over the past three decades, as practitioners have come to understand the limitations of point forecasts.&lt;ref&gt;{{Cite journal|title = 25 years of time series forecasting|url = http://www.sciencedirect.com/science/article/pii/S0169207006000021|journal = International Journal of Forecasting|date = 2006|pages = 443–473|volume = 22|series = Twenty five years of forecasting|issue = 3|doi = 10.1016/j.ijforecast.2006.01.001|first = Jan G.|last = De Gooijer|first2 = Rob J.|last2 = Hyndman}}&lt;/ref&gt; Despite the bold move by the organizers of the [[Global Energy Forecasting Competition|Global Energy Forecasting Competition 2014]] to require the participants to submit forecasts of the 99 [[percentile]]s of the predictive distribution (day-ahead in the price track) and not the point forecasts as in the 2012 edition,&lt;ref&gt;{{Cite journal|title = Global Energy Forecasting Competition 2012|url = http://www.sciencedirect.com/science/article/pii/S0169207013000745|journal = International Journal of Forecasting|date = 2014|pages = 357–363|volume = 30|issue = 2|doi = 10.1016/j.ijforecast.2013.07.001|first = Tao|last = Hong|first2 = Pierre|last2 = Pinson|first3 = Shu|last3 = Fan}}&lt;/ref&gt; this does not seem to be a common case in EPF as yet.

If PIs are computed at all, they usually are distribution-based (and approximated by the standard deviation of the model residuals&lt;ref name=&quot;:0&quot; /&gt;) or empirical. The latter method resembles the estimation of the [[Value at risk|Value-at-Risk]] via [[Historical simulation (finance)|historical simulation]], and consists of computing sample [[quantile]]s of the empirical distribution of the one-step-ahead prediction errors. A new forecast combination (see below) technique has been introduced recently in the context of EPF. [[Quantile regression averaging|Quantile Regression Averaging (QRA)]] involves applying [[quantile regression]] to the point forecasts of a small number of individual forecasting models or experts, hence allows to leverage existing development of point forecasting.&lt;ref name=&quot;:11&quot;&gt;{{Cite journal|title = Computing electricity spot price prediction intervals using quantile regression and forecast averaging|url = http://link.springer.com/article/10.1007/s00180-014-0523-0|journal = Computational Statistics|date = 2015|issn = 0943-4062|pages = 791–803|volume = 30|issue = 3|doi = 10.1007/s00180-014-0523-0|first = Jakub|last = Nowotarski|first2 = Rafał|last2 = Weron|others = [Open Access]}}&lt;/ref&gt;

=== Combining forecasts ===
[[Consensus forecast]]s, also known as ''combining forecasts'', ''forecast averaging'' or ''model averaging'' (in [[econometrics]] and [[statistics]]) and ''[[committee machine]]s'', ''[[ensemble averaging]]'' or ''expert aggregation'' (in [[machine learning]]), are predictions of the future that are created by combining together several separate forecasts which have often been created using different methodologies. Despite their popularity in econometrics, averaged forecasts have not been used extensively in the context of [[electricity market]]s to date. There is some limited evidence on the adequacy of combining forecasts of electricity demand,&lt;ref&gt;{{Cite journal|title = Using combined forecasts with changing weights for electricity demand profiling|url = http://www.palgrave-journals.com/doifinder/10.1057/palgrave.jors.2600856|journal = Journal of the Operational Research Society|pages = 72–82|volume = 51|issue = 1|doi = 10.1057/palgrave.jors.2600856|first = J W|last = Taylor|first2 = S|last2 = Majithia|year = 2000}}&lt;/ref&gt; but it was only very recently that combining was used in EPF and only for point forecasts.&lt;ref name=&quot;:12&quot;&gt;{{Cite journal|title = Combining day-ahead forecasts for British electricity prices|url = http://www.sciencedirect.com/science/article/pii/S0140988311002921|journal = Energy Economics|date = 2013|pages = 88–103|volume = 35|series = Quantitative Analysis of Energy Markets|doi = 10.1016/j.eneco.2011.12.001|first = Silvano|last = Bordignon|first2 = Derek W.|last2 = Bunn|first3 = Francesco|last3 = Lisi|first4 = Fany|last4 = Nan}}&lt;/ref&gt;&lt;ref name=&quot;:13&quot;&gt;{{Cite journal|title = An empirical comparison of alternative schemes for combining electricity spot price forecasts|url = http://www.sciencedirect.com/science/article/pii/S0140988314001716|journal = Energy Economics|date = 2014|pages = 395–412|volume = 46|doi = 10.1016/j.eneco.2014.07.014|first = Jakub|last = Nowotarski|first2 = Eran|last2 = Raviv|first3 = Stefan|last3 = Trück|first4 = Rafał|last4 = Weron}}&lt;/ref&gt; Combining probabilistic (i.e., interval and density) forecasts is much less popular, even in econometrics in general, mainly because of the increased complexity of the problem. Since [[Quantile regression averaging|Quantile Regression Averaging (QRA)]] allows to leverage existing development of point forecasting,&lt;ref name=&quot;:11&quot; /&gt; it is particularly attractive from a practical point of view and may become a popular tool in EPF in the near future.

=== Multivariate factor models ===
The literature on forecasting daily electricity prices has concentrated largely on models that use only information at the aggregated (i.e., daily) level. On the other hand, the very rich body of literature on forecasting intra-day prices has used disaggregated data (i.e., hourly or half-hourly), but generally has not explored the complex dependence structure of the multivariate price series.&lt;ref name=&quot;:0&quot; /&gt; If we want to explore the structure of intra-day electricity prices, we need to use dimension reduction methods; for instance, factor models with factors estimated as [[principal components]] (PC). Empirical evidence indicates that there are forecast improvements from incorporating disaggregated (i.e., hourly or zonal) data for predicting daily system prices, especially when the forecast horizon exceeds one week.&lt;ref name=&quot;:14&quot;&gt;{{Cite journal|title = Forecasting of daily electricity prices with factor models: utilizing intra-day and inter-zone relationships|url = http://link.springer.com/article/10.1007/s00180-014-0531-0|journal = Computational Statistics|date = 2015|issn = 0943-4062|pages = 805–819|volume = 30|issue = 3|doi = 10.1007/s00180-014-0531-0|first = Katarzyna|last = Maciejowska|first2 = Rafał|last2 = Weron|others = [Open Access]}}&lt;/ref&gt;&lt;ref&gt;{{Cite journal|title = Forecasting day-ahead electricity prices: Utilizing hourly prices|url = http://www.sciencedirect.com/science/article/pii/S0140988315001668|journal = Energy Economics|date = 2015|pages = 227–239|volume = 50|doi = 10.1016/j.eneco.2015.05.014|first = Eran|last = Raviv|first2 = Kees E.|last2 = Bouwman|first3 = Dick|last3 = van Dijk}}&lt;/ref&gt; With the increase of computational power, the real-time calibration of these complex models will become feasible and we may expect to see more EPF applications of the multivariate framework in the coming years.

=== A universal test ground ===
All major review publications conclude that there are problems with comparing the methods developed and used in the EPF literature.&lt;ref name=&quot;:0&quot; /&gt;&lt;ref name=&quot;:10&quot; /&gt; This is due mainly to the use of different datasets, different software implementations of the forecasting models and different error measures, but also to the lack of statistical rigor in many studies. This calls for a comprehensive, thorough study involving (i) the same datasets, (ii) the same robust error evaluation procedures, and (iii) statistical testing of the significance of one model's outperformance of another. To some extent, the [[Global Energy Forecasting Competition|Global Energy Forecasting Competition 2014]] has addressed these issues. Yet more has to be done. A selection of the better-performing measures (weighted-MAE, seasonal MASE or RMSSE) should be used either exclusively or in conjunction with the more popular ones (MAPE, RMSE). The empirical results should be further tested for the significance of the differences in forecasting accuracies of the models.&lt;ref name=&quot;:12&quot; /&gt;&lt;ref name=&quot;:13&quot; /&gt;&lt;ref name=&quot;:14&quot; /&gt;

== See also ==
* [[Energy forecasting]]
* [[Global Energy Forecasting Competition]]s

== References ==
&lt;references /&gt;

[[Category:Prediction]]
[[Category:Economic forecasting]]
[[Category:Econometrics]]
[[Category:Statistics]]
[[Category:Regression analysis]]
[[Category:Time series models]]
[[Category:Artificial neural networks]]
[[Category:Energy economics]]
[[Category:Electricity markets]]</text>
      <sha1>c8avd4z74jb8hmyxbr7041hy8xfzn25</sha1>
    </revision>
  </page>
  <page>
    <title>Varian's theorems</title>
    <ns>0</ns>
    <id>48688319</id>
    <revision>
      <id>693239248</id>
      <parentid>693116975</parentid>
      <timestamp>2015-12-01T08:01:32Z</timestamp>
      <contributor>
        <username>Erel Segal</username>
        <id>7637243</id>
      </contributor>
      <comment>linear utilities represent substitute goods, not independent goods.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve">{{more footnotes|date=November 2015}}

In [[welfare economics]], '''Varian's theorems''' are several theorems related to [[fair division|fair allocation of homogeneous divisible resources]]. They describe conditions under which there exists a [[Pareto efficient]] (PE) [[envy-free]] (EF) allocation. They were published by [[Hal Varian]] in the 1970s.&lt;ref name=V74&gt;{{cite journal|doi=10.1016/0022-0531(74)90075-1|title=Equity, envy, and efficiency|journal=Journal of Economic Theory|volume=9|pages=63|year=1974|last1=Varian|first1=Hal R}}&lt;/ref&gt;&lt;ref name=V76&gt;{{cite journal|doi=10.1016/0047-2727(76)90018-9|title=Two problems in the theory of fairness|journal=Journal of Public Economics|volume=5|issue=3–4|pages=249|year=1976|last1=Varian|first1=Hal R.}}&lt;/ref&gt;

== Examples ==
All examples involve an economy with two [[good (economics)|goods]], x and y, and two agents, Alice and Bob.

A. '''Many PEEF allocations:''' Alice and Bob have [[linear utilities]], representing [[substitute good]]s:
:&lt;math&gt;u_A(x,y)=2x+y&lt;/math&gt;,
:&lt;math&gt;u_B(x,y)=x+2y&lt;/math&gt;.
The total endowment is (4,4). If Alice receives at least 3 units of x, then her utility is 6 and she does not envy Bob. Similarly, if Bob receives at least 3 units of y, he does not envy Alice. So the allocation [(3,0);(1,4)] is PEEF with utilities (6,9). Similarly, the allocations [(4,0);(0,4)] and [(4,0.5);(0,3.5)] are PEEF. On the other hand, the allocation [(0,0);(4,4)] is PE but not EF (Alice envies Bob); the allocation [(2,2);(2,2)] is EF but not PE (the utilities are (6,6) but they can be improved e.g. to (8,8)).

B. '''Essentially-single PEEF allocation:''' Alice and Bob have [[Leontief utilities]], representing [[complementary good]]s:
:&lt;math&gt;u_A(x,y)=u_B(x,y)=\min(x,y)&lt;/math&gt;.
The total endowment is (4,2). The equal allocation [(2,1);(2,1)] is PEEF with utility vector (1,1). EF is obvious (every equal allocation is EF). Regarding PE, note that both agents now want only y, so the only way to increase the utility of an agent is to take some y from the other agent, but this decreases the utility of the other agent. While there are other PEEF allocations, e.g. [(1.5,1);(2.5,1)], all have the same utility vector of (1,1), since it is not possible to give both agents more than 1. &lt;ref&gt;Note that a similar economy appears in the 1974 paper{{rp|70}} as an example that a PEEF allocation does ''not'' exist. This is probably a typo - the &quot;min&quot; should be &quot;max&quot;, as in example C below. See this [http://economics.stackexchange.com/q/9494/385 economics stack-exchange thread].&lt;/ref&gt;

C. '''No PEEF allocations:''' Alice and Bob have concave utilities:
:&lt;math&gt;u_A(x,y)=u_B(x,y)=\max(x,y)&lt;/math&gt;.
The total endowment is (4,2). The equal allocation [(2,1);(2,1)] is EF with utility vector (2,2). Moreover, ''every'' EF allocation must give both agents equal utility (since they have the same utility function) and this utility can be at most 2.  However, no such allocation is PE, since it is Pareto-dominated by the allocation [(4,0);(0,2)] whose utility vector is (4,2).

== Existence of PEEF allocations with monotone convex preferences ==
Varian's theorem says that:&lt;ref name=V74/&gt;{{rp|80}}

::'''If the preferences of all agents are [[Monotone preferences|monotone]] and [[Convex preferences|convex]], then PEEF allocations exist.'''

In the [[#Examples]], the preferences are always monotone. However, only in examples A and B the preferences are convex.

The proof relies on the existence of a [[competitive equilibrium]] with equal incomes. Assume that all resources in an economy are divided equally between the agents. I.e, if the total endowment of the economy is &lt;math&gt;E&lt;/math&gt;, then each agent &lt;math&gt;i\in 1,\dots,n:&lt;/math&gt; receives an initial endowment &lt;math&gt;E_i = E/n&lt;/math&gt;.

Since the preferences are ''convex'', the [[Arrow–Debreu model]] implies that a competitive equilibrium exists. I.e, there is a price vector &lt;math&gt;P&lt;/math&gt; and a partition &lt;math&gt;X&lt;/math&gt; such that:
* (CE) All agents maximize their utilities given their budget. I.e, if &lt;math&gt;P\cdot Y \leq P\cdot X_i&lt;/math&gt; then &lt;math&gt;Y \preceq_i X_i&lt;/math&gt;.
* (EI) All agents have the same income in the equilibrium prices: for all &lt;math&gt;i,j: P\cdot X_i = P\cdot X_j&lt;/math&gt;.

Such an allocation is always EF. Proof: by the (EI) condition, for every &lt;math&gt;i,j: P\cdot X_j \leq P\cdot X_i&lt;/math&gt;. Hence, by the (CE) condition, &lt;math&gt;X_j \preceq_i X_i&lt;/math&gt;.

Since the preferences are ''monotonic'', any such allocation is also PE, since monotonicity implies [[local nonsatiation]]. See [[fundamental theorems of welfare economics]].

== References ==
{{reflist}}

[[Category:Fair division]]
[[Category:Economics theorems]]</text>
      <sha1>rmwteiignqba1npfrozirlg0ahz1s1s</sha1>
    </revision>
  </page>
  <page>
    <title>History of algorithms</title>
    <ns>0</ns>
    <id>48688443</id>
    <revision>
      <id>693261065</id>
      <parentid>693243394</parentid>
      <timestamp>2015-12-01T12:23:14Z</timestamp>
      <contributor>
        <username>BbcNkl</username>
        <id>16786737</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve">{{Multiple issues|
{{orphan|date=November 2015}}
{{unreferenced|date=November 2015}}
}}

[[Algorithms]] are as old as mathematic. Even the ancient civilizations used algorithms, both in the design of important things, as well as in everyday life. The name itself was created in the 12th century as a synonym for the mathematical method. With the advent of computers begins formalization of algorithms, and they become the basis of the software. Today they are used in almost every area.

== Origin of the word ==
In the 9th century AD Persian mathematician and astronomer [[Muhammad ibn Musa al-Khwarizmi|Muhammad al-Khwarizmi]], often called as ,,the father of [[algebra]]&quot; wrote a book ,,about Indian numbers&quot; in which the scientist gave a description of a set of procedures and precise rules for various calculations. This work will be translated in the 12th century into Latin as ,,Algorithms de numero Indorum&quot; where the word ,,algorithms&quot; is supposed to represent latin surname scientists. However, the word becomes a synonym for further computational methods. &lt;span class=&quot;cx-segment&quot; data-segmentid=&quot;14&quot;&gt;&lt;/span&gt;

==  Algorithms by the ancients  ==

=== Babylonian mathematics ===
Babylonian mathematics is in many ways was more advanced than the Egyptian mathematics. The [[Babylonians]] were able to calculate square and [[cube root]], knew the [[Pythagorean theorem]] 1.200 years before [[Pythagoras]] officially defined, known for the [[Pi|number π]], could solve polynomial eighth degree, [[linear equations]], and a variety of calculating the [[circle]]. Unlike the [[Greeks]], Babylonian mathematicians were more focused on [[algebra]] and not on [[geometry]].&lt;span class=&quot;cx-segment&quot; data-segmentid=&quot;30&quot;&gt;&lt;/span&gt;

==== Babylonian Numerals ====

[[Cuneiform Numbers and Punctuation|Cuneiform numbers]] could be written using a combination of two symbols: a vertical wedge for '1' and a corner wedge for '10'. The Babylonians had a sexagesimal [[Numeral system|system]] and used the concept of place value to write numbers larger than 60. So they had 59 symbols for the numbers 1-59, and then the [[symbols]] were repeated in different columns for larger numbers. For example, a '2' in the second column from the right meant (2 x 60)=120, and a '2' in the column third from the right meant (2 x 60'''2''')=7200. 
The numbers 1-59 are written below:
[[File:Vavilonskibrojevi.gif|centre|thumb|620x620px|'''Figure 1''': Babylonian cuneiform numbers]]
To use sexagesimal  system in contemporary language, should split columns with commas, so that the number of 7627 = 2 (60&lt;sup&gt;2&lt;/sup&gt;) + 1 (60) + 7 note as 2,1,7. There are some problems with this system. The first is that there is no way to separate columns except to insert gaps in the numbers, like the number ,, 2 &quot; looks similar to ,, 61&quot; (1, 1). Even more serious problem is that there is no symbol for [[zero]] to add to the empty column, so that the 1 ,, &quot;,, seamlessly varies from 60&quot;. Later Babylonian civilization have introduced zero, so that they were aware of this shortcoming. The Babylonian number system with base 60 has retained to the present time because we still have 60 minutes per hour, 60 seconds per minute, 360 degrees within 60 minutes and in degree. Even the 24-hour clock heritage of the ancient Babylonians.&lt;span class=&quot;cx-segment&quot; data-segmentid=&quot;49&quot;&gt;&lt;/span&gt;

==== Babylonian Number Tables ====
The common part of the Egyptians and Babylonian is that of making tables to ease the effort of calculations. Tables were used to calculate things like [[square roots]] with as much accuracy as mathematicians in the times of the [[Renaissance]]. &lt;span class=&quot;cx-segment&quot; data-segmentid=&quot;54&quot;&gt;&lt;/span&gt;

===== Reciprocal Tables =====
The Babylonians had no special [[algorithm]] for long division, and instead used the fact that  &lt;math display=&quot;block&quot;&gt;a/b=a\times{(1/b)}&lt;/math&gt; 
They created tables of reciprocals converted to sexagesimal notation. In the notation introduced earlier, we can use a semi-colon to indicate a decimal point. Then the number 1/2 would be written as (0;30)= 0(1)+30(60&lt;sup&gt;−1&lt;/sup&gt;).  60 is a useful base here because many numbers have finite base 60 [[fractions]], e.g. 1/2, 1/3, 1/4, 1/5, 1/6, 1/10, 1/12, 1/15 and 1/20. However, some numbers (such as 1/7, 1/13) were infinite fractions, and only their approximations were given. It is a shame the Babylonians did not consider these numbers further, since they would have yielded periodically repeating sexagesimal fractions that could have provoked investigation into [[infinite series]]. &lt;span class=&quot;cx-segment&quot; data-segmentid=&quot;63&quot;&gt;&lt;/span&gt;

===== Tables of Squares =====
The Babylonian method of multiplication is quite ingenious and only relies on knowing the [[Square number|square of numbers]]. They used the formulas &lt;span class=&quot;cx-segment&quot; data-segmentid=&quot;70&quot;&gt;&lt;/span&gt;

&lt;math&gt;ab={{(a+b)^2-a^2-b^2\over 2}}&lt;/math&gt;

&lt;math&gt;ab={{(a+b)^2-(a-b)^2 \over 4 }}&lt;/math&gt;

for easy [[multiplication]] of two numbers. They didn't always use this method though; sometimes it was just as simple to multiply and add, e.g. to multiply by 39 you multiply by 30 and 9 and add the results together.

===== Square and Cube Roots =====
The Babylonians were able to find [[square root]] of two with a difference of 0.0000006 from the true value. They were also able to find square roots for other values. They used two possible methods of approximating square roots.&lt;span class=&quot;cx-segment&quot; data-segmentid=&quot;84&quot;&gt;&lt;/span&gt;

The first of these uses the [[approximation]]

&lt;math&gt;\sqrt{a^2+b} \approx a + {b \over 2a}&lt;/math&gt;

which is derived from the first few terms of the expansion of the [[binomial series]].

The second method uses an algorithm which was later ascribed to the Greeks.

Let а = а&lt;sup&gt;1&lt;/sup&gt; be an initial approximation. If  &lt;math display=&quot;inline&quot;&gt;a^1 &lt; \sqrt 2&lt;/math&gt; then  &lt;math&gt;2/a^1 &gt; \sqrt 2&lt;/math&gt;. So as a better approximation take &lt;math&gt;a^2 = {(a^1 + {2 \over a^1}) \over 2}&lt;/math&gt;. Repeat the process until you have an answer as accurate as you want.

===== Quadratic Equations and the n&lt;sup&gt;3&lt;/sup&gt; + n&lt;sup&gt;2&lt;/sup&gt;  table =====
One important table for Babylonian algebra was that of the values of ''n&lt;sup&gt;3&lt;/sup&gt; + n&lt;sup&gt;2&lt;/sup&gt;'' for [[Integer|integer values]] of n from 1 to 30. These tables could be used to solve [[cubic equations]] of the form

&lt;math display=&quot;inline&quot;&gt;ax^3 + bx^2 = c&lt;/math&gt;

It is likely that the Babylonians were able to solve [[quadratic equations]], and to solve them they had used the method similar to our method for solving [[quadratic equations]].
[[File:plimpton.gif|thumb|239x239px|'''Figure 2:''' The Plimpton Table]]

===== Exponentials and Logarithms =====
Ancient tablets have been found listing successive [[Exponentiation|powers]] of numbers. However, '[[logarithm]] tables' were not used for general calculation but were only used to solve specific problems.

===== Pythagorean Triples =====
A Pythagorean triple in this table consists of three integers which satisfy the equation &lt;math&gt;a^2 + b^2 = c^2&lt;/math&gt;

===     Egyptian mathematics ===
[[File:Egipat.jpg|thumb|253x253px|'''Figure 3:''' Egyptian numbers]]
[[Ancient Egypt|The ancient Egyptian]] were possibly the first civilisation to practice the scientific arts. Where the Egyptian really excelled was in [[medicine]] and [[applied mathematics]]. But although there is a large body of papyrus literature describing their achievements in medicine, there is no records of how they reached their [[mathematical]] conclusions. Of course they must have had an advanced understanding of the subject because their exploits in [[engineering]], [[astronomy]] and [[administration]] would not have been possible without it.

The Egyptian had a decimal system using seven different symbols.
[[File:Egipat2.gif|thumb|251x251px|
* '''Figure 4:''' Comparison of the Egyptians and decimal numbers.]]
* 1 is shown by a single stroke.
* 10 is shown by a drawing of a hobble for cattle.
* 100 is represented by a coil of rope.
* 1,000 is a drawing of a lotus plant.
* 10,000 is represented by a finger.
* 100,000 by a tadpole or frog.
* 1,000,000 is the figure of a god with arms raised above his head.
The conventions for reading and writing numbers is quite simple; the higher number is always written in front of the lower number and where there is more than one row of numbers the reader should start at the top.

=== Greece mathematics ===

==== Tales ====
Greek philosopher who is considered the founder of Greek science, [[mathematics]] and [[philosophy]]. He visited [[Ancient Egypt|Egypt]], and possibly [[Babylon]], and returned with knowledge of [[astronomy]] and [[geometry]]. He introduced [[deductive mathematics]]. According to him was named [[Thales' theorem]].&lt;span class=&quot;cx-segment&quot; data-segmentid=&quot;189&quot;&gt;&lt;/span&gt;

==== Euclidean algorithm ====
Greek geometer who wrote,, Elements', a book of [[geometry]]. The book contains a previous knowledge of geometry, and has been used for centuries in [[Western Europe]] as a textbook in geometry. [[Euclid]] proved what is generally known as Euclid's second theorem: the number of [[primes]] is infinite.&lt;span class=&quot;cx-segment&quot; data-segmentid=&quot;199&quot;&gt;&lt;/span&gt;

He is known for [[Euclidean algorithm]], a method for finding the greatest common divisor of two numbers.&lt;span class=&quot;cx-segment&quot; data-segmentid=&quot;204&quot;&gt;&lt;/span&gt;&lt;syntaxhighlight&gt;
- divide the number a by b, the remainder is r
- replace a by b
- replace b by r
- continue until a can't be more divided. 
In this case, a is the gcd.
&lt;/syntaxhighlight&gt;Not established nor old nor the place of his birth, nor the circumstances of his death, although it is known that he lived and worked in [[Alexandria]] for most of his life.

==== The algorithm of Archimedes ====
[[Archimedes]] is considered to be the greatest mathematician of antiquity. Archimedes has performed numerous geometric proofs using rigid geometric formalism of Euclid. He was particularly proud of his discovery to finding the [[volume]] of [[sphere]] and [[Cylinder (geometry)|cylinder]]. Also important is the result of calculating the ratio of the [[circumference]] and [[diameter]] of the [[circle]] and placing this relationship in the border between π = 3 1/7 and 3 10/71, known as the [[Pi|Archimedes algorithm]] to calculate the approximate value of the number π. Archimedes found to the upper and lower limits of the number π drawing a regular [[hexagon]] inside and outside the circle, and successively doubling the number of pages until he reached the 96-ostranog regular [[polygons]]. Counting the framework of this platform, proved that the 223/71 &lt;π &lt;7.22 (or 3.1408 &lt;π &lt;3.1429). Archimedes also was an outstanding engineer, formulated [[Archimedes principle|Archimedes' principle]] of buoyancy and principle of [[levers]], as well as a lot of other discoveries. Archimedes was killed by a Roman soldier in the [[Second Punic War]]. Narrates that his last sentence was: Noli turbare circulos meos! (Do not disturb my circles!)&lt;span class=&quot;cx-segment&quot; data-segmentid=&quot;236&quot;&gt;&lt;/span&gt;

==== Sieve Of Eratosthenes Algorithm ====
[[File:Sieve_of_Eratosthenes_animation.gif|thumb|370x370px|'''Figure 5:''' Sieve Of Eratosthenes Algorithm for prime numbers to 120.]]
He worked as a librarian in the great library in [[Alexandria]], and wrote various works on [[mathematics]], [[geography]], [[philosophy]] and [[astronomy]]. He also wrote a poem called,, [[Hermes]] &quot;where he described the basics of astronomy in verse! Although most [[Eratosthenes]] writings lost, many are preserved through the writings of people.&lt;span class=&quot;cx-segment&quot; data-segmentid=&quot;252&quot;&gt;&lt;/span&gt;

Among its achievements is a precise measurement of the diameter of the Earth. Unfortunately, as the lost original work measuring the diameter of the Earth, Eratosthenes details of these proceedings are not known. [[Eratosthenes]] introduced a system of terrestrial coordinates, prepared a [[star chart]] containing 675 stars, suggested to be a [[leap year]] every fourth year, tried to build precisely dated [[history]], and developed the &quot;[[Sieve of Eratosthenes]]&quot; algorithm for finding [[prime numbers]]:&lt;syntaxhighlight&gt;
1. Write all numbers from 2 to n 
2. Starting with the first number on the list (2), cross out the list of all numbers divisible by two and write that two is a prime number. 
3. Repeat the process with the next uncrossed number m. 
So, cross out all the numbers divisible by m, and himself check that it is prime.
&lt;/syntaxhighlight&gt;

=== Later mathematical discoveries ===

==== Gaussian elimination method ====
In [[linear algebra]], [[Gaussian elimination|Gaussian elimination method]] is an algorithm for solving systems of [[linear equations]]. This method was named after [[Carl Friedrich Gauss]], although it was known to the Chinese mathematician [[Liu Hui]] from the 3rd century. A system of n linear equations are solved by equalizing the number of unknown and the number of equations. Pseudocode:
 '''for''' k = 1 ... min(m,n):
    ''Find the k-th pivot:''
    i_max  := argmax (i = k ... m, abs(A[i, k]))
    '''if''' A[i_max, k] = 0
      '''error''' &quot;Matrix is singular!&quot;
    '''swap rows'''(k, i_max)
    ''Do for all rows below pivot:''
    '''for''' i = k + 1 ... m:
      m := A[i, k] / A[k, k]
     '' Do for all remaining elements in current row:''
      '''for''' j = k + 1 ... n:
        A[i, j]  := A[i, j] - A[k, j] * m
      ''Fill lower triangular matrix with zeros:''
      A[i, k]  := 0

==== Brahmagupa ====
[[Brahmagupta]] was born in 598 A.D. and lived in the northwest of [[India]] until he died in 668 A.D. He was an astronomer and mathematician. He wrote method of solving the [[indeterminate equation]] ax + by = c  Brahmagupta has developed a method of solving indeterminate equations of the second degree and rules of solving simple [[quadratic equations]] of various types.

&lt;math display=&quot;block&quot;&gt;x = { \sqrt {4ac + b^2} - b \over {2a} }&lt;/math&gt;

The major divergence is that Brahmagupta attempted to define [[division by zero]].

==== Muhammad al-Khwarizmi ====
[[Muhammad ibn Musa al-Khwarizmi|Muhammad al-Khwarizmi]] was a Persian scientist, mathematician, [[astronomer]], and [[astrologer]]. He was born in 780 A.D. and died around 840 A.D. He is often cited as &quot;the father of algebra&quot;,  which was named after a part of the title of his book.  He made major contributions to the fields of [[algebra]], [[trigonometry]], and [[geography]].  With his publication about the calculation with Hindu Numerals he promotes the use of the Indian system of numeration in the [[Middle-East|Middle East]] followed by Europe. This book was translated into Latin in the 12th century with the name “''Algoritmi de numero Indorum''”, because his name was rendered in Latin as “Algoritmi” he is indirect responsible for the term [[algorithm]]. His books made a significant contribution to the advancement of mathematics (solving linear and quadratic equations including geometric principles for completing the square) in Europe. He made contributions in tables of trigonometric functions, refinements in the geometric representation of [[conic sections]], and aspects of the calculus of two errors as well as publications on mechanical devices like the clock, [[astrolabe]], and [[sundial]].

==== Ibn Al-Haitham  ====
Abu Ali Hasan Ibn al-Haitham was one of the most eminent [[physicists]], whose contributions to [[optics]] and the scientific methods are outstanding. He went to Egypt, where he was asked to find ways of controlling the flood of the [[Nile]].  Being unsuccessful in this, he feigned madness until the death of [[Caliph]] al-Hakim. He also travelled to Spain and, during this period, he had ample time for his scientific pursuits, which included [[optics]], [[mathematics]], [[physics]], [[medicine]] and development of scientific methods on each of which he has left several outstanding books.

He made a thorough examination of the passage of [[light]] through various media and discovered the laws of refraction. He also carried out the first experiments on the dispersion of light into its constituent colours.  He dealt at length with the theory of various physical phenomena like shadows, eclipses, the rainbow, and speculated on the physical nature of light. He is the first to describe accurately the various parts of the [[eye]] and give a scientific explanation of the process of vision. He also attempted to explain binocular vision, and gave a correct explanation of the apparent increase in size of the sun and the moon when near the horizon. Through these extensive researches on optics, he has been considered as the father of modern optics. Ibn Al-Haitham is a mathematician who first brought the formula for the sum of the fourth degree, and later developed an algorithm for determining the general formula for the sum of any degree, which is the basis of development of integral calculus. His contributions to mathematics and physics is extensive. In mathematics, he developed analytical geometry by establishing connections between algebra and geometry. He studied the mechanics of body movement and was the first claims that the body moves constantly unless the external force does not stop or change direction of movement, which is equivalent to Newton's first law of motion.

== First computers ==
[[Charles Babbage]] was an English mathematician, analytical philosopher, mechanical engineer, scientist, inventor of the first [[computer]] that could be [[Computer programming|programmed]], as well as professor of mathematics at [[Cambridge]]. Because of the impact on the subsequent development of science, called the &quot;father&quot; of computing. Babbage's machines were the first mechanical computers. Babbage realized that machines can work better and more reliably than man. He started the construction of a machine that was more or less is finishing work and proposed that the calculation can be mechanized to the extreme. Although Babbage's [[machines]] were huge, their structure was similar to today's computer. The [[data]] and program [[memory]] were separated, operations were based on instructions from the man's hand. In 1822. he developed mechanical machine called a [[differential machine]]. Babbage's machine was created that automatically calculates more mathematical operations. The first differential machine had 25,000 parts, was 8 feet tall, and 15 tons weight. Although he had a lot of sponsors, failed to complete it.

[[Ada Lovelace]] was one of the most picturesque characters in computer history. At the age of 17 Ada met Mary Somerville, a remarkable woman who translated [[Laplace]]'s works into English, and whose texts were used at Cambridge. Though Mrs. Somerville encouraged Ada in her mathematical studies, Ada has attempted to put mathematics and technology into an appropriate human context. At the dinner at Mrs. Somerville, Ada heard the ideas of Charles Babbage calculating machines, [[analytical engine]]. He guessed: what if calculating machine could not only foresee but also to act on that foresight.

After unsuccessful attempts differential machine, Babbage started working on the project differently, much more complex machine called the analytical engine. It was not simply a physical machine, but a combination of multiple designs of machines devised by the end of his life (1871). The main difference between these two machines is that the analytical engine could be programmed using [[punched cards]], which was an idea ahead of his time. He realized that there could not be more [[Computer program|programs]] to fit on a single card, and also had to be present a person who would create other programs. This machine was the first [[Turing-complete]] computer mechanical device. Analytical engine was the forerunner of the modern computer. About throug plans while working on a new machine Babbage reported on developments at a seminar in Turin in the autumn of 1841. The Italian, Menabrea, wrote a summary of what Babbage described and published an article in French. Ada has translated the article and showed to Babbage, and he suggested that she add her own notes, which turned out to be three times longer than the original article.  Letters between Babbage and Ada flew back and forth filled with fact and fantasy.In her article, published in 1843, Lady Lovelace's prescient comments included her predictions that such a machine might be used to compose complex music, to produce graphics, and would be used for both practical and scientific use. She was correct. Ada has proposed Babbage writing algorithms that a machine might calculate [[Bernoulli numbers]]. This algorithm, is now considered the first computer program. Programming language developed by the US Department of Defense was named &quot;[[Ada (programming language)|Ada]]&quot; in her honor in 1979.

== Symbols, rules, formalization ==

=== Symbols and rules ===
[[George Boole]] has invented the [[Binary Alphabet|binary algebra]], the basis of computers. [[Boolean algebra]] is part of the [[Logic|logic of mathematics]] - algebraic structure that summarizes the basis of operations AND, OR and NOT as a [[Set (mathematics)|set]] of theoretical operations such as union, intersection and complement. Unlike elementary algebra, where variables for value have numbers in Boolean algebra values of variables can only be true and false, which is usually denoted by 1 and 0, where 1 is true and 0 false. Bull is actually united logic and calculations with common symbols.

[[Gottlob Frege|Friedrich Ludwig Gottlob Frege]] was a German mathematician, logician and philosopher. One of the founders of modern mathematical logic and analytic philosophy. It is considered one of the greatest logicians of all time. In 1879, Frege constructed a first variant of [[predicate calculus]]. It is very similar to that used today, although Frege uses a different notation. Frege's discovery quantifiers that bind variables, considered one of the greatest discoveries of the nineteenth century. Frege wanted to show that the whole mathematics can be reduced to logic, but he failed. He developed a specific philosophy of language, which today many philosophers considered very significant. Formula language's, that is a lingua characterica, a language written with special symbols, &quot;for pure thought&quot;, that is free from rhetorical embellishments ... It is made from special symbols that are set in accordance with certain rules. His work continued to [[Alfred North Whitehead]] and [[Bertrand Russell]] in their book ,, Principles of Mathematics &quot;

=== First formalization ===
The concept of algorithm was formalized in 1936 through [[Alan Turing|Alan Turing's]] Turing machines and [[Alonzo Church|Alonzo Church's]] lambda calculus, which in turn formed the foundation of computer science.

[[Alonzo Church]] - his work is of great importance in [[mathematical logic]], [[recursion]] theory, and theoretical computer science. He created the lambda calculus in 1930 which today is an invaluable tool for computer scientists.

Church is probably best remembered for 'Church's Theorem' and 'Church's Thesis'.

Church's Theorem, showing the undecidability of first order logic, appeared in A note on the [[Entscheidungsproblem]] published in the first issue of the Journal of Symbolic Logic. This, of course, is in contrast with the propositional calculus which has a decision procedure based on truth tables.

Church's Thesis appears in An unsolvable problem in elementary number theory. In the paper he defines the notion of effective calculability and identifies it with the notion of a recursive function.

Another area of interest to Church was axiomatic set theory. He published A formulation of the simple theory of types in which he attempted to give a system related to that of Whitehead and Russell's Principia Mathematica which was designed to avoid the paradoxes of naive set theory. Church bases his form of the theory of types on his -calculus.

Although most of Church's contributions are directed towards mathematical logic, he did write a few mathematical papers of other topics. For example he published Remarks on the elementary theory of differential equations as area of research and A generalization of Laplace's transformation. The first examines ideas and results in the elementary theory of ordinary and partial [[differential equation]]s. The paper includes a discussion of a generalization the Laplace transform which he extends to non-linear partial differential equations.

== Turing and Turing machine ==
[[Alan Turing|Alan Mathison Turing]], the British mathematician and cryptographer who is considered the father of the modern computer. During [[World War II]] he worked in Bletchley Park and built a machine with which the Allies could read German messages encrypted through [[Enigma machine|Enigma]] machine that had 15x10&lt;sup&gt;19&lt;/sup&gt; combination.

After the war, he built the first computer and dealing with the problems of [[artificial intelligence]]. Known for his eccentric lifestyle, was arrested in 1952 for violating public morality and sentenced. Two years later he committed suicide. He is also known for having defined the test for machine intelligence. The purpose of this test is to determine whether the machine is actually  intelligent, or is merely a simulation of intelligence. So far, no machine failed to pass the [[Turing test]], while people walk by.

=== Turing machine ===
The idea was born in the first half of the twentieth century from the so-called &quot;irrelevant&quot; mathematics, who have tried to give replies that if some still unproven mathematical theorems have evidence. On that occasion, David Hilbert posed three questions:
# whether mathematics is complete
# whether mathematics is consistent
# whether mathematics is decided (whether there is an algorithm that shows that a formula is correct)
The mathematician [[Kurt Gödel]] in 1930 answered the first two questions and proved that mathematics is not complete, and will always be unproven proposition. This evidence was thirty years have caused great disappointment among mathematicians.

[[Alan Turing]] in 1936 gave the answer to the third question in an unusual way. After he has noticed certain regularities in everyday arithmetic, conceived the so-called [[Turing machine]] which on the bar with symbols simulates the computation. This hypothetical (imaginary) machine is not really done. It was used to show whether every mathematical problem can be solved using the algorithm. Soon after this machine has improved and created the Universal Turing machine, with which he gave a negative answer to the third question Hilbert. In this way, he sets the basis of [[software]].

== Artificial Intelligence ==
Unlike ordinary algorithm, in which the computer follows the orders of his tasks, step by step, some computational algorithms are designed to enable the computer to learn by themselves ([[machine learning]]). Using machine learning involves understanding the data and pattern recognition. [[Artificial intelligence]] is reduced to the use of algorithms to identify and handle different forms and presenting them in a form suitable for humans. As mathematical equations, algorithms are neither good nor bad. There are only people with good and bad intentions who use algorithms. As technology develops, there will be a lot of mistakes ,, &quot;but it is important to remember that the algorithms are just tools. We should not blame the tools. The algorithms make the system better, but no small use common sense to the equation, they can produce some pretty bizarre results.

John McCarthy is a prominent computer scientist who received the [[Turing]] Award in 1971 due to large contributions to the field of artificial intelligence. In fact, he is responsible for the term &quot;artificial intelligence&quot;. McCarthy wrote the Lisp programming language. In 1961, the first publicly proposed that computer technology of the time sharing may lead to a future in which computing power and even specific applications could be sold through a dedicated business model (such as water or electricity). This idea of computer or information utility was very popular in the late 1960s, but slowly fade from the mid-1970s when it became clear that the then [[Computer hardware|hardware]], [[software]] and [[Telecommunication|telecommunication technologies]] were not ready. However, in 2000, the idea reappeared in new forms.

Dr Taher Elgamal is an American-Egyptian [[cryptographer]]. In 1985, Elgamal published a paper entitled ,, The public key cryptosystem and signature scheme based on discrete logarithms &quot;in which the proposed design Elgamalovih discrete logarithm cryptosystems. This scheme has become the basis for the [[digital signature]] algorithm (DSA) which was adopted by the National Institute of Standards and Technology (NIST) as a standard for the digital signature (DSS). He also participated in the protocol for payment with [[credit cards]] &quot;SET&quot; - the security of electronic transactions, but also in many other tasks related to internet payments.

== External links ==
* [http://www.scriptol.com/programming/algorithm-history.php History of Algorithms and Algorithmics]

{{uncategorized|date=November 2015}}</text>
      <sha1>rpmmnudikh71uenxvnpwaxvxqhtt5zt</sha1>
    </revision>
  </page>
  <page>
    <title>US transition to electric cars</title>
    <ns>0</ns>
    <id>48694890</id>
    <revision>
      <id>693393261</id>
      <parentid>693334005</parentid>
      <timestamp>2015-12-02T07:35:19Z</timestamp>
      <contributor>
        <username>BG19bot</username>
        <id>14508071</id>
      </contributor>
      <minor />
      <comment>[[WP:CHECKWIKI]] error fix for #61.  Punctuation goes before References. Do [[Wikipedia:GENFIXES|general fixes]] if a problem exists. - using [[Project:AWB|AWB]] (11751)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve">{{Orphan|date=December 2015}}

[[File:2013 Tesla Model S (11322176214) cropped.jpg|thumb|The 2013 Tesla Model S, a fully electric car built by the American company Tesla Motors]]
Fully electric cars such as the [[Tesla Model S]] and the [[Nissan Leaf]] are quickly becoming affordable alternatives to cars that operate on [[internal combustion engine]]s. This page explores the consequences that would accompany a national transition from internal combustion cars to fully electric cars.

== Efficiency ==
Thermal efficiency, &lt;math&gt;E&lt;/math&gt;, of a heat engine is determined by the ratio between the useful output of work, &lt;math&gt;Wout&lt;/math&gt;, and the input of energy, &lt;math&gt;Qin&lt;/math&gt;.

&lt;math&gt;E=Wout/Qin&lt;/math&gt;
[[File:P-V Otto cycle.svg|alt=|thumb|215x215px|A pressure vs. volume graph representing the Otto Cycle
1-2 Compression

2-3 Ignition

3-4 Expansion

4-1 Expulsion
]]
Some heat engines' efficiency is determined by specific equations. For instance, internal combustion engines follow the [[Otto cycle]]. The Otto cycle considers the compression ratio of the engine, &lt;math&gt;r&lt;/math&gt;, and the specific heat ratio of the gas in the combustion chamber, &lt;math&gt;\gamma&lt;/math&gt;.

&lt;math&gt;E=1-1/r^{\gamma-1}&lt;/math&gt;

In modern internal combustion cars, the compression ratio is between 6 and 10, and the specific heat of the air and gas mixture is 1.28.&lt;ref name=&quot;:0&quot;&gt;{{Cite web|title = A Comparison of Diesel and Gasoline in Consumer Automobiles|url = http://large.stanford.edu/courses/2012/ph240/ramos1/|website = large.stanford.edu|accessdate = 2015-12-01}}&lt;/ref&gt; Given these values, the maximum efficiency of an internal combustion engine is around 44%. An additional 20% is lost to various contributing factors, but mostly friction. As such, the effective efficiency of an internal combustion engine in ideal conditions is around 25%.&lt;ref name=&quot;:0&quot; /&gt;

An electric car is powered by an electric motor. The motor converts electrical work to mechanical work. Due to the fact the motor operates by converting one type of work to another, the motor has a theoretical efficiency of 100%. In practice, an electric car suffers the same friction losses as an internal combustion engine, so the effective efficiency is around 80%.

Well-to-wheel efficiency considers the ratio between the output of useful work and the energy content of the fuel, before it has been processed. For internal combustion engines, the well-to-wheel efficiency follows the equation for the efficiency of a heat engine. In this case, &lt;math&gt;Qin&lt;/math&gt; is the original energy content of the fuel. A barrel (42 gallons) of crude oil contains, on average, 1694.44 kilowatt-hours of energy.&lt;ref name=&quot;:1&quot;&gt;{{Cite web|url = http://www.ocean.washington.edu/courses/envir215/energynumbers.pdf|title = Energy in natural processes and human consumption|date = |accessdate = |website = |publisher = |last = |first = }}&lt;/ref&gt; This barrel will produce 19 gallons of gasoline,&lt;ref&gt;{{Cite web|title = How many gallons of diesel fuel and gasoline are made from one barrel of oil? - FAQ - U.S. Energy Information Administration (EIA)|url = https://www.eia.gov/tools/faqs/faq.cfm?id=327&amp;t=6|website = www.eia.gov|accessdate = 2015-12-01}}&lt;/ref&gt; giving an unrefined original energy content of 40.13 kilowatt-hours per gallon. In 2014, The United States consumed 136.78 billion gallons of gasoline,&lt;ref name=&quot;:2&quot; /&gt; giving a total &lt;math&gt;Qin&lt;/math&gt; of 5.48 trillion kilowatt-hours. The total work done by all of the cars in The United States in 2014 (see Energy Requirements) is 1.15 trillion kilowatt-hours. This results in a well-to-wheel efficiency of 20.9%.

To determine the well-to-wheel efficiency for electric cars we must consider the same efficiency equation above. We will use natural gas as the original fuel source. It takes 10.1 cubic feet of natural gas to produce 1 kilowatt-hour of electricity at a power plant.&lt;ref&gt;{{Cite web|title = How much coal, natural gas, or petroleum is used to generate a kilowatthour of electricity? - FAQ - U.S. Energy Information Administration (EIA)|url = https://www.eia.gov/tools/faqs/faq.cfm?id=667&amp;t=3|website = www.eia.gov|accessdate = 2015-12-01}}&lt;/ref&gt; The energy requirement to perform the mechanical work of all US cars, assuming 80% efficient electric cars, is 1.437 trillion kilowatt-hours. The product of the two gives 14.5 trillion cubic feet of natural gas required to produce enough electricity to power a total transition to electric cars. The original energy content of natural gas is 0.305 kilowatt-hours per cubic foot.&lt;ref name=&quot;:1&quot; /&gt; The product of the energy content and required volume of natural gas gives a total &lt;math&gt;Qin&lt;/math&gt; of 4.428 trillion kilowatt-hours. Finally, to determine the overall well-to-wheel efficiency we divide the total work, 1.15 trillion kilowatt-hours by the total &lt;math&gt;Qin&lt;/math&gt; of 4.428 trillion kilowatt-hours. The result is an overall well-to-wheel efficiency of 26%.

As shown above, a total transition to electric cars in The United States would raise the overall efficiency of the system.

== Energy Requirements ==
The following calculations present an examination of a total transition from internal combustion cars to electric cars in The United States. As such, the sum of all cars is treated as a single system.

In 2014 the US car system consumed 136.78 billion gallons of gasoline.&lt;ref name=&quot;:2&quot;&gt;{{Cite web|title = How much gasoline does the United States consume?  - FAQ - U.S. Energy Information Administration (EIA)|url = https://www.eia.gov/tools/faqs/faq.cfm?id=23&amp;t=10|website = www.eia.gov|accessdate = 2015-12-01}}&lt;/ref&gt; The EPA lists the energy content in 1 gallon of gasoline as 33.7 kilowatt-hours.&lt;ref&gt;{{Cite web|url = http://www3.epa.gov/carlabel/documents/420f13009.pdf|title = Understanding the New Fuel Economy and Environment Labels|date = |accessdate = |website = |publisher = |last = |first = }}&lt;/ref&gt; The product of the energy content and amount consumed results in 4.609 trillion kilowatt-hours, this value represents the input energy into the system. Given that the average efficiency of an internal combustion engine is 25%, we can use the equation for efficiency of a heat engine to determine the total work done by the system. The equation can be rearranged to solve for output of work, &lt;math&gt;Wout&lt;/math&gt;, as the product of energy input, &lt;math&gt;Qin&lt;/math&gt;, and efficiency, &lt;math&gt;E&lt;/math&gt;.

&lt;math&gt;Wout=Qin*E&lt;/math&gt;

The total work output of the system is determined to be 1.15 trillion kilowatt-hours.

For a total transition to electric cars to be effective, the system would require the same work output as the current system. Assuming an average efficiency of 80% for the electric car system, another rearrangement of the efficiency equation can solve for required energy input.

&lt;math&gt;Qin=Wout/E&lt;/math&gt;

The required energy input is determined to be 1.437 trillion kilowatt-hours. The current electricity consumption of The United States is 3.862 trillion kilowatt-hours per year.&lt;ref&gt;{{Cite web|title = U.S. Energy Information Administration (EIA) - Data|url = http://www.eia.gov/electricity/data.cfm#sales|website = www.eia.gov|accessdate = 2015-12-01}}&lt;/ref&gt; A total switch to electric cars would push that number to 5.01 trillion kilowatt-hours per year, a 37% increase.

== Cost ==
In 2014 the US consumed 136.78 billion gallons of gasoline.&lt;ref name=&quot;:2&quot;/&gt; At the current price of about $2.00 per gallon of gas,&lt;ref&gt;{{Cite web|title = AAA's Daily Fuel Gauge Report|url = http://www.fuelgaugereport.com/|website = www.fuelgaugereport.com|accessdate = 2015-12-01}}&lt;/ref&gt; the US spends about $273 billion per year on the volatile product. Each gallon of gasoline yields 13.6 kWh,&lt;ref&gt;{{Cite web|title = How much coal, natural gas, or petroleum is used to generate a kilowatthour of electricity? - FAQ - U.S. Energy Information Administration (EIA)|url = https://www.eia.gov/tools/faqs/faq.cfm?id=667&amp;t=6|website = www.eia.gov|accessdate = 2015-12-01}}&lt;/ref&gt; resulting in the national consumption of 1.8 trillion kWh per year. One kWh costs an average of $0.12 across the US,&lt;ref&gt;{{Cite web|title = The Price Of Electricity In Your State|url = http://www.npr.org/sections/money/2011/10/27/141766341/the-price-of-electricity-in-your-state|website = NPR.org|accessdate = 2015-12-01}}&lt;/ref&gt; so to generate 1.8 trillion kWh through sources other than oil would cost the US about $216 billion, resulting in a savings of $57 billion per year.

==References==
{{Reflist}}

[[Category:Electric cars]]</text>
      <sha1>fs13cg3xz146h668ur5sj1redg9682h</sha1>
    </revision>
  </page>
  <page>
    <title>Promoter activity</title>
    <ns>0</ns>
    <id>48695290</id>
    <revision>
      <id>693291099</id>
      <parentid>693291054</parentid>
      <timestamp>2015-12-01T16:58:16Z</timestamp>
      <contributor>
        <username>Eliaspty</username>
        <id>26861869</id>
      </contributor>
      <comment>/* Eukaryotes promoter structure */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve">[[File:Promoter_Activity.png|thumb|Promoter activity of the P-RM and P-R promoters vs RNA polymerase concentration in the enterobacteriophage lamda&lt;ref&gt;{{Cite journal|url = |title = The 0, Control System of Bacteriophage Lambda
A Physical-Chemical Model for Gene Regulation|last = Shea|first = M|date = 1985|journal = Journal of Molecular Biology|doi = |pmid = |access-date = |last2 = Akers|first2 = G|volume = 181|pages = 211–230}}&lt;/ref&gt;]]

'''Promoter activity ''' is a term that encompasses several meanings around the process of [[gene expression]] from regulatory sequences —[[promoter]]s and [[Enhancer (genetics)|enhancers]].&lt;ref name=&quot;:0&quot;&gt;{{Cite book|title = Developmental Biology|last = Gilbert|first = S.F.|publisher = Sinauer Associates|year = 2000|isbn = |location = http://www.ncbi.nlm.nih.gov/books/NBK10023/|pages = }}&lt;/ref&gt; Gene expression has been commonly characterized as a measure of how much, how fast, when and where this process happens.&lt;ref name=&quot;:1&quot;&gt;{{Cite journal|url = |title = Transcriptional regulation by the numbers: models|last = Bintu|first = L.|date = 2005|journal = Current Opinion in Genetics &amp; Development|doi = |pmid = |access-date = |volume = 15|pages = 116–124|last2 = Buchler|first2 = N|first3 = H|last3 = Garcia|last4 = Gerland|first4 = U|last5 = Hwa|first5 = T|last6 = Kondev|first6 = J|last7 = Phillips|first7 = R}}&lt;/ref&gt; Promoters and enhancers are required for controlling where and when an specific genes is transcribed.&lt;ref name=&quot;:0&quot; /&gt;

Traditionally the measure of gene products (i.e. mRNA, proteins, etc.) has been the major approach of measure promoter activity. However, this method confront with two issues: the stochastic nature of the gene expression&lt;ref&gt;{{Cite journal|url = |title = Stochastic Gene Expression in a Single Cell|last = Elowitz|first = M|date = 2002|journal = Science|doi = |pmid = |access-date = |first2 = A.J.|last2 = Levine|last3 = Siggia|first3 = E.|last4 = Swain|first4 = P.|volume = 297|pages = 1183–1186}}&lt;/ref&gt; and the lack of mechanistic interpretation of the thermodynamical process involve in the promoter activation.&lt;ref name=&quot;:1&quot; /&gt;

The actual developments in [[metabolomics]] product of developments of [[next-generation sequencing]] technologies and molecular structural analysis have enable the development of more accurate models of the process of promoter activation (e.g. the sigma structure of the polymerase holoenzyme domains&lt;ref&gt;{{Cite journal|url = |title = RNA polymerase holoenzyme: structure, function and
biological implications|last = Borukhov|first = S.|date = 2003|journal = Current Opinion in Microbiology|doi = |pmid = |access-date = |last2 = Nudlery|first2 = E|volume = 6|pages = 93–100}}&lt;/ref&gt;) and a better understanding of the complexities of the regulatory factors involved.

== Promoter binding ==
[[File:Probability of Binding new.png|thumb|Probability of binding for the T7 (green) and Lac  Ecoli (blue) promoters by RNA polymerase concentrations.&lt;ref name=&quot;:1&quot; /&gt;]]The process of binding is central in determining the &quot;strength&quot; of promoters, that is the relative estimation of how &quot;well&quot; a promoter perform the expression of a gene under specific circumstances. Brewster et al.,&lt;ref name=&quot;:2&quot;&gt;{{Cite journal|url = |title = Tuning Promoter Strength through RNA Polymerase Binding Site Design in Escherichia coli|last = Brewster|first = R.|date = 2012|journal = PLOS Computational Biology|doi = 10.1371/journal.pcbi.1002811|pmid = |access-date = |volume = 8|issue = 12|pages = 1–10|last2 = Jones|first2 = D.|last3 = Phillips|first3 = R.}}&lt;/ref&gt; using a simple thermodynamical model based on the postulate that transcriptional activity is proportional to the probability of finding the RNA polymerase bound at the promoter, obtained predictions of the scaling of the RNA polymerase binding energy. This models support the relationship between the probability of binding and the output of gene expression&lt;ref name=&quot;:2&quot; /&gt;

===Mathematical representation of promoter binding ===
The problem of gene regulation could be represented mathematically as the probability of n molecules   — RNAP, activators, repressors and inducers  — are bound to a target regions.&lt;ref name=&quot;:1&quot; /&gt;

To compute the probability of bound, it is needed to sum the Boltzman weights over all possible states of &lt;math&gt;P&lt;/math&gt; polymerase molecules

on DNA.&lt;ref&gt;{{Cite journal|url = |title = On schemes of combinatorial transcription logic|last = Buchler|first = N.E.|date = 2003|journal = Proc. National Academy of Sciences, USA|doi = |pmid = |access-date = |last2 = Gerland|first2 = U.|last3 = Hwa|first3 = T.|volume = 100|issue = 5|pages = 5136–5141}}&lt;/ref&gt;  Here in this deduction  &lt;math&gt;P&lt;/math&gt;  is the effective number of RNAP molecules available for binding to the promoter.

This approach is based in statistical thermodynamics of two possible microscopic outcomes:&lt;ref name=&quot;:1&quot; /&gt;  
# one state where all P polymerases molecules are distributed among all the non-specific sites (sites not participating in gene expression)  
# a promoter occupied and the remaining P-1 polymerases distributed among the non-specific sites.

The statistical weigh of promoter unoccupied ''Z(P)'' is defined:

&lt;math id=&quot;1&quot;&gt;Z(P) = ~\frac{N_{NS}!}{P!*(N_{NS}-P)!}* {e}^{- ~\frac{E_{NS}}{KbT} }&lt;/math&gt;

Where the first term is the combinatorial result of taken  &lt;math&gt;P&lt;/math&gt; polymerase of &lt;math&gt;N_{NS}&lt;/math&gt;  non-specific sites available, and the second term are the  [[Boltzmann distribution|boltzmann]] weights, where &lt;math id=&quot;1&quot;&gt;E_{NS}&lt;/math&gt; is the energy that represents the average binding energy of RNA polymerase to the genomic background (non-specific sites).

Then, the total statistical weight &lt;math&gt;Z(Ptotal)&lt;/math&gt;, can be written as the sum of the &lt;math&gt;Z(P)&lt;/math&gt; state and the RNA polymerase on promoter state:

&lt;math id=&quot;1&quot;&gt;Z(Ptotal) = Z(P)+ Z(P-1)* {e}^{- ~\frac{E_{S}}{KbT} }&lt;/math&gt;

Where &lt;math id=&quot;1&quot;&gt;E_{S}&lt;/math&gt; in the &lt;math&gt;Z(P-1)&lt;/math&gt; state is the binding energy for RNA polymerase on the promoter (where the s stands for specific site).

Finally, to find the probability of a RNA polymerase to binding (&lt;math&gt;Prob_{bound}&lt;/math&gt; )  to an specific promoter, we divide &lt;math id=&quot;1&quot;&gt;Z(P)&lt;/math&gt; by &lt;math id=&quot;1&quot;&gt;Z(Ptotal)&lt;/math&gt; which produces:

&lt;math&gt;Prob_{bound} = \frac{1}{1+\frac{N_NS}{P}*{e}^{-\frac{\Delta E}{KbT}}}&lt;/math&gt;

Where,  &lt;math&gt;\Delta E = E_{S}-E_{NS}&lt;/math&gt;

An important result of this model is that any transcription factor, regulator or perturbation could be introduced as a term multiplying &lt;math&gt;P&lt;/math&gt; in the probability of binding equation. This term for any transcriptional factor (here called factor regulators) modify the probability of binding to:

&lt;math&gt;Prob_{bound} = \frac{1}{1+\frac{N_NS}{P*F_{R}}*{e}^{-\frac{\Delta E}{KbT}}}&lt;/math&gt;

Where &lt;math&gt;F_{R}&lt;/math&gt; is the term for transcriptional factors, and it has the value of &lt;math&gt;F_{R} &gt;1&lt;/math&gt; for increase of &lt;math&gt;F_{R}&lt;1&lt;/math&gt; for decrease of the number of RNA polymerase available to bind.

This result has an important significance to represent mathematically all the possible configurations of transcriptional factor by derive different models to estimate &lt;math&gt;F_{R}&lt;/math&gt; (for further developments, see also &lt;ref name=&quot;:1&quot; /&gt;).

===Eukaryotes promoter structure===
[[File:CorepromoterK.png|thumb|Core Promoter elemets.]]
The process of activation and binding in eukaryotes is different from bacteria in the way that specific DNA elements bind the factors for a functional pre-initiation complex. In bacteria there is a single polymerase, that contain catalytic subunits and a single regulatory  subunits known as [[Sigma factor|sigma]], which transcribe for different type of genes.&lt;ref name=&quot;:4&quot;&gt;{{Cite journal|url = |title = Structure and mechanism of the RNA polymerase II transcription machinery|last = Hahn|first = S.|date = 2004|journal = Nature Structural and Molecular Biology|doi = |pmid = |access-date = |volume = 11|pages = 394–403}}&lt;/ref&gt;

In eukaryotes, the transcription is performed by three different RNA polymerase, RNA pol I for ribosomal RNAs (rRNAs), RNA polymerase II for messenger RNAs (mRNAs) and some small regulatory RNAs, and the RNA polymeerase III for small RNAs such as transfer RNAs (tRNAs). The process of positioning of the RNA polymerase II and the transcriptional machinery require the recognition of a region know as &quot;core promoter&quot;.&lt;ref name=&quot;:4&quot; /&gt; The elements that could be found in the core promoter include the TATA element, the TFIIB recognition element (BRE), the initiatior (Inr), and  the downstream core promoter element (DPE).&lt;ref name=&quot;:5&quot;&gt;{{Cite journal|url = |title = The RNA polymerase II core promoter: a key component in the regulation of gene expression|last = Butler|first = J.|date = 2015|journal = Genes and Development|doi = |pmid = |access-date = |last2 = Kodonaga|first2 = J.|volume = 16|pages = 2583–2592}}&lt;/ref&gt; Promoters in eukaryotes contain one or more of these core promotes elements (but any of them are absolutely essential for promoter function),&lt;ref name=&quot;:4&quot; /&gt; these elements are binding sites for subunits of the transcriptional machinery and are involve in the initiation of the transcription, but also they have some specific enhancer functions.&lt;ref name=&quot;:5&quot; /&gt; In addition, the promoter activity in eukaryotes include some complexities in the way of how they integrate signals from distal factors  with the core promoter.&lt;ref&gt;{{Cite journal|url = |title = The RNA Polymerase II Core Promoter|last = Smale|first = S.|date = 2003|journal = Ann. Review in Biochemistry|doi = |pmid = |access-date = |last2 = Kadonaga|first2 = T.|volume = 79|pages = 449–479}}&lt;/ref&gt;

== Evolutionary processes ==
Unlike in protein coding regions, where the assumption of  sequence conservation of functionally homologous genes have been frequently proved, there is no a clear relationship of conservation between sequences and their functions for regulatory regions.&lt;ref name=&quot;:6&quot;&gt;{{Cite journal|url = |title = Phylogenetic simulation of promoter evolution: estimation and
modeling of binding site turnover events and assessment of their impact on alignment tools|last = Huang|first = W.|date = 2007|journal = Genome Biology|doi = |pmid = |access-date = |last2 = Nevins|first2 = J.|last3 = Ohler|first3 = U.|volume = 8|issue = 10|article = }}&lt;/ref&gt; The transcriptional promoters regions are under less stringent selection, then have a higher substitutions rates, allowing transcription factor binding sites to be replaced easily be new ones arising from random mutations.&lt;ref name=&quot;:6&quot; /&gt; Notwithstanding the sequence changes, mainly  the functions of regulatory sequences remain conserved.&lt;ref name=&quot;:6&quot; /&gt;

In recents years with the increase of availability of genome sequences, [[phylogenetic footprinting]] open the possibitlity to identify cis-elements, and then study their evolution processes. In this sense, Raijman et al.,&lt;ref&gt;{{Cite journal|url = |title = Evolution and Selection in Yeast Promoters: Analyzing the Combined Effect of Diverse Transcription Factor Binding Sites|last = Raijman|first = D.|date = 2008|journal = PLOS Computational Biology|doi = 10.1371/journal.pcbi.0040007|pmid = |access-date = |last2 = Shamir|first2 = R.|last3 = Tanay|first3 = A.|volume = 4|issue = 1|pages = e7}}&lt;/ref&gt; Dermitzakis et al.&lt;ref name=&quot;:7&quot;&gt;{{Cite journal|url = |title = Evolution of Transcription Factor Binding Sites in Mammalian Gene Regulatory Regions: Conservation and Turnover|last = Dermitzakis|first = E.|date = 2002|journal = Molecular Biology Evolution|doi = |pmid = |access-date = |last2 = Clark|first2 = A.|volume = 19|issue = 7|pages = 1114–1121}}&lt;/ref&gt; have developed techniques for analyzing evolutionary processes in transcription factor regions in Saccharomyces species promoters and mammalian regualatory networks respectively.

The basis for many of these evolutionary changes in nature are probably related with events within the cis-regulatory regions involve in gene expression.&lt;ref name=&quot;:3&quot;&gt;{{Cite journal|url = |title = Rapid Evolution of cis-Regulatory Sequences via local Point Mutations|last = Stone|first = J.|date = 2001|journal = Molecular Biology Evolution|doi = |pmid = |access-date = |volume = 18|issue = 9|pages = 1754–1770|last2 = Wray|first2 = G.A.}}&lt;/ref&gt; The impact of variation in regulatory regions is important for disease risk&lt;ref name=&quot;:7&quot; /&gt; due their impact in the gene expression level. Furthermore, perturbations in the binding properties of proteins encoded by regulatory genes have been linked with phenotypes effects such as, duplicated structures, homeotic transformations and novel morphologies.&lt;ref name=&quot;:3&quot; /&gt;

== Measure of promoter activity ==
The measure of the promoter activity has a broad meaning. The promoter activity could be measured for different situations or research questions,&lt;ref name=&quot;:1&quot; /&gt; such as:
* estimation of the level of expression in comparison (relative) to some know value 
* how fast a gene is expressed after induction 
* the timing of expression relative to others genes 
* the specific spatial location of expression 
Methods to study promoter activity commonly are based in the expression of a reporter gene from the promoter of the gene of interest.&lt;ref&gt;{{Cite journal|url = |title = Real-time detection of gene promoter activity: quantitation of toxin gene transcription|last = Jeyaseelan|first = K.|date = 2001|journal = Nucleic Acid Research|doi = |pmid = |access-date = |last2 = Ma|first2 = D.|last3 = Armugan|first3 = A.|volume = 29|issue = 12}}&lt;/ref&gt; [[Mutation]]s and [[deletion]]s are made in a promoter region, and their changes on couple expression of the reporter gene are measured.&lt;ref&gt;{{Cite journal|url = |title = LUCIFERASE REPORTER ASSAYS: POWERFUL, ADAPTABLE TOOLS FOR CELL BIOLOGY RESEARCH|last = ALLARD|first = S.T.|date = 2008|journal = Cell Notes|doi = |pmid = |access-date = |last2 = KOPISH|first2 = K.|volume = |issue = 21}}&lt;/ref&gt;

The most important [[reporter gene]]s  are the fluorescence proteins as [[Green fluorescent protein|GFP]]. These reporters allow to measure promoter activation by increasing fluorescent signals, and deactivation by decrease in the rate of fluorescence.&lt;ref&gt;{{Cite journal|url = |title = A comprehensive library of fluorescent transcriptional reporters for Escherichia Coli|last = Zaslaver|first = A.|date = 2006|journal = Nature Methods|doi = |pmid = |access-date = |volume = 3|issue = 8|pages = 623–628|last2 = Bren|first2 = A.|publisher = |last3 = Ronen|first3 = M.|last4 = Itzkoviz|first4 = S.|last5 = Kikoin|last6 = Shavit|first5 = I.|first6 = S.|last7 = Leibesmeister|first7 = W.|last8 = Surette|first8 = M.|last9 = Alon|first9 = U.}}&lt;/ref&gt;

== See also ==
{{colbegin|2}}
* [[Enhancer (genetics)]]
* [[Glossary of gene expression terms]]
* [[cis-Regulatory element]]
* [[Transcription factor]]
* [[RNA polymerase]]
{{colend}}

== References ==
{{reflist|35em}}

{{Transcription}}

{{DEFAULTSORT:Promoter Activity}}
[[Category:Gene expression]]
[[Category:Evolutionary biology]]</text>
      <sha1>ncwfk727kkq6l5nkmfahisnim8l4vrz</sha1>
    </revision>
  </page>
</mediawiki>